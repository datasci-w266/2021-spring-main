{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Word Embeddings\n",
        "\n",
        "In this part of the assignment, we'll explore a few properties of word embeddings. We'll use pre-trained GloVe ([Pennington et al. 2013](https://nlp.stanford.edu/pubs/glove.pdf)) embeddings, and evaluate on the analogy task described in ([Mikolov et al. 2013](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)).\n",
        "\n",
        "If you haven't seen the [embeddings.ipynb](../../materials/embeddings/embeddings.ipynb) demo notebook, we recommend you look through it; this part of the assignment will build on that material."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install a few python packages using pip\n",
        "from w266_common import utils\n",
        "utils.require_package(\"wget\")      # for fetching dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard python helper libraries.\n",
        "import os, sys, re, json, time\n",
        "import itertools, collections\n",
        "from importlib import reload\n",
        "from IPython.display import display\n",
        "\n",
        "# NumPy and SciPy for matrix ops\n",
        "import numpy as np\n",
        "import scipy.sparse\n",
        "\n",
        "# NLTK for NLP utils\n",
        "import nltk\n",
        "\n",
        "# Helper libraries\n",
        "from w266_common import utils, vocabulary, tf_embed_viz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fits like a GloVe\n",
        "\n",
        "Word embeddings take a long time to train - since the goal is to provide a good representation for as many words as possible, generating good embeddings often requires making several passes over a very large corpus. \n",
        "\n",
        "Fortunately, it's possible to learn fairly general embeddings from large corpora that are useful for many downstream tasks. We'll use the GloVe vectors available at https://nlp.stanford.edu/projects/glove/ - specifically, a set trained with a vocabulary of 400,000 on a corpus of 6B tokens from Wikipedia and Gigaword.\n",
        "\n",
        "The vectors are distributed as a (very) large text file, with one word per line followed by its vector:\n",
        "```\n",
        "the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459\n",
        "```\n",
        "\n",
        "We've implemented a helper class, `Hands` in `glove_helper.py`, that will parse these files in a memory efficient manner and provide a wrapper object over a NumPy array containing the actual vectors. \n",
        "\n",
        "Run the cell below; the first time, it will download an ~800 MB file to the `data/` directory. **_PLEASE DO NOT CHECK THIS DATA DIRECTORY IN TO GIT!_**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glove_helper; reload(glove_helper)\n",
        "\n",
        "hands = glove_helper.Hands(ndim=100)  # 50, 100, 200, 300 dim are available"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`hands` has a few properties and methods that might be useful:\n",
        "- `hands.vocab` is a `vocabulary.Vocabulary` object that manages the set of available words\n",
        "- `hands.W` is a matrix of shape $|V| \\times d$ containing the actual vectors, one per row. Row indices are as given by `hands.vocab.word_to_id[word]`.\n",
        "- `hands.get_vector(word)` returns the vector for a word (passed as a string).\n",
        "\n",
        "Note that we let $|V| = $`hands.W.shape[0]`, which in addition to the actual words includes three special tokens: `<s>` (begin sentence), `</s>` (end sentence), and `<unk>` (unknown word)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part (a): Nearest Neighbors\n",
        "\n",
        "### Cosine Similarity\n",
        "\n",
        "To measure the similarity of two words, we'll use the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) between their representation vectors:\n",
        "\n",
        "$$ D^{cos}_{ij} = \\frac{v_i^T v_j}{||v_i||\\ ||v_j||}$$\n",
        "\n",
        "*Note that this is called cosine similarity because $D^{cos}_{ij} = \\cos(\\theta_{ij})$, where $\\theta_{ij}$ is the angle between the two vectors.*\n",
        "\n",
        "## Part (a) Questions\n",
        "\n",
        "1. In `vector_math.py`, implement the `find_nn_cos(...)` function. Read the docstring _carefully_ - it describes what you should return. *Hint: use NumPy functions instead of a `for` loop.*\n",
        "<p>\n",
        "2. Use the `show_nns(...)` function below to find the nearest neighbors for the words `\"bank\"`, `\"plane\"`, and `\"flies\"`. Put the closest word (other than *the exact word* itself) in the answers file.  If you inspect `\"flies\"`, for instance, are the neighbors dominated by one sense of the word or is there evidence that the vector encodes meaning of the other senses as well?\n",
        "<p>\n",
        "3. Like `word2vec`, GloVe constructs representations by summarizing word-word coocurrence statistics. Use `show_nns(...)` to find the neighbors of `\"green\"` and `\"celadon\"`, and `\"orange\"`. Explain what you find in terms of the distributional hypothesis and the grounding problem. Does the vector for `\"celadon\"` appear to encode a notion of color? What do they represent, instead?\n",
        "\n",
        "_(The Distributional Hypothesis is the idea that \"you shall know a word by the company it keeps\" (Firth, 1957) - that meaning is derived from the context in which a word is used. Grounding refers to the meaning of language in terms of external concepts, such as real-world entities or physical characteristics.  Take a look at the readings from the first week if you're not confident.)_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import vector_math; reload(vector_math)\n",
        "\n",
        "def show_nns(hands, word, k=10):\n",
        "    \"\"\"Helper function to print neighbors of a given word.\"\"\"\n",
        "    word = word.lower()\n",
        "    print(\"Nearest neighbors for '{:s}'\".format(word))\n",
        "    v = hands.get_vector(word)\n",
        "    for i, sim in zip(*vector_math.find_nn_cos(v, hands.W, k)):\n",
        "        target_word = hands.vocab.id_to_word[i]\n",
        "        print(\"{:.03f} : '{:s}'\".format(sim, target_word))\n",
        "    print(\"\")\n",
        "    \n",
        "show_nns(hands, \"the\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### YOUR CODE HERE ####\n",
        "# Code for Part (a).2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### END(YOUR CODE) ####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### YOUR CODE HERE ####\n",
        "# Code for Part (a).3\n",
        "\n",
        "\n",
        "#### END(YOUR CODE) ####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part (b): Linear Analogies\n",
        "\n",
        "In this part, you'll implement the word analogy task described in Section 4 of ([Mikolov et al. 2013](https://arxiv.org/pdf/1301.3781.pdf)), and discussed in section 4.8 and 4.11 of the async.\n",
        "\n",
        "1. In `vector_math.py`, implement the `analogy(...)` function. (*Hint: this should be a very short function, given what you've already written above.*)\n",
        "<p>\n",
        "\n",
        "2. Evaluate the following analogies:\n",
        "<ul>\n",
        "<li> `\"mouse\" is to \"mice\" as \"dog\" is to ____`\n",
        "<li> `\"fast\" is to  \"fastest\"   as \"slow\" is to ____`\n",
        "<li> `\"work\" is to  \"works\"   as \"speak\" is to ____`\n",
        "<li> `\"russia\" is to  \"moscow\"   as \"greece\" is to ____`\n",
        "</ul>\n",
        "\n",
        "<p>\n",
        "\n",
        "3. (Ungraded) Play with a few analogies using the `show_analogy(...)` function below. Concretely, see if you can find at least one more analogy that tests each of the following relationships, and that the model gets right:<ul>\n",
        "<li> Singular / plural\n",
        "<li> Superlatives\n",
        "<li> Verb tense\n",
        "<li> Country / capital\n",
        "</ul>\n",
        "\n",
        "(See Table 1 of [Mikolov et al. 2013](https://arxiv.org/pdf/1301.3781.pdf) for a few ideas)\n",
        "\n",
        "<p>\n",
        "\n",
        "4. Evaluate the following analogies:\n",
        "<ul>\n",
        "<li> `\"lizard\" is to \"reptile\" as \"dog\" is to ____`\n",
        "<li> `\"finger\" is to  \"hand\"   as \"toe\" is to ____`\n",
        "</ul>\n",
        "\n",
        "\n",
        "Similar to before, report the closest word (highest scoring) and exclude any exact match to any of the three words in the analogy query.\n",
        "\n",
        "Think about what types of relations do these test? Does our approach of linear analogies work well here? What assumption is violated by these sorts of relationships? (_Hint: what if we reversed the order, and tested \"reptile\" is to \"lizard\", and so on?_ See [Meronymy](https://en.wikipedia.org/wiki/Meronymy).)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import vector_math; reload(vector_math)\n",
        "\n",
        "def show_analogy(hands, a, b, c, k=5):\n",
        "    \"\"\"Compute and print a vector analogy.\"\"\"\n",
        "    a, b, c = a.lower(), b.lower(), c.lower()\n",
        "    va = hands.get_vector(a)\n",
        "    vb = hands.get_vector(b)\n",
        "    vc = hands.get_vector(c)\n",
        "    print(\"'{a:s}' is to '{b:s}' as '{c:s}' is to ___\".format(**locals()))\n",
        "    for i, sim in zip(*vector_math.analogy(va, vb, vc, hands.W, k)):\n",
        "        target_word = hands.vocab.id_to_word[i]\n",
        "        print(\"{:.03f} : '{:s}'\".format(sim, target_word))\n",
        "    print(\"\")\n",
        "    \n",
        "show_analogy(hands, \"king\", \"queen\", \"man\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### YOUR CODE HERE ####\n",
        "# Code for Part (b).2\n",
        "\n",
        "\n",
        "#### END(YOUR CODE) ####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### YOUR CODE HERE ####\n",
        "# Code for Part (b).3\n",
        "\n",
        "\n",
        "#### END(YOUR CODE) ####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
