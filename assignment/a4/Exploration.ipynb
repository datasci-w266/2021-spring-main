{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploration and a Simple Baseline\n",
        "\n",
        "In this part of the assignment, we'll introduce the [Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/index.html) (SST) dataset, and train a Naive Bayes model as a simple baseline. The SST, introduced by [(Socher et al. 2013)](http://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf) consists of approximately 10,000 sentences from movie reviews associated with fine-grained sentiment labels on a five-point scale, and is a popular benchmark for text classification.\n",
        "\n",
        "## Outline\n",
        "\n",
        "- **Part (a):** The Stanford Sentiment Treebank\n",
        "- **Part (b):** Naive Bayes\n",
        "- **Part (c):** Exploring Negation\n",
        "\n",
        "Exercises are interspersed throughout the notebook. Be sure you catch all of them! There are 4 questions for Part (a), 2 for Part (b), and 3 for Part (c). Some questions have multiple parts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install a few python packages using pip\n",
        "from w266_common import utils\n",
        "utils.require_package(\"wget\")      # for fetching dataset\n",
        "utils.require_package(\"bokeh\")     # for plotting histograms\n",
        "utils.require_package(\"graphviz\")  # for rendering trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preliminaries: GraphViz\n",
        "\n",
        "This notebook uses [GraphViz](https://www.graphviz.org/) to render tree structures. On Ubuntu / Debian (including Google Cloud), you can install it by running on the command line:\n",
        "```\n",
        "sudo apt-get install graphviz\n",
        "```\n",
        "\n",
        "For Mac OSX, you can install using Homebrew:\n",
        "```\n",
        "brew install graphviz\n",
        "```\n",
        "or see https://www.graphviz.org/download/ for more options. Run the cell below to set up rendering and show a sample tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from w266_common import treeviz\n",
        "import sst\n",
        "# Monkey-patch NLTK with better Tree display that works on Cloud or other display-less server.\n",
        "print(\"Overriding nltk.tree.Tree pretty-printing to use custom GraphViz.\")\n",
        "treeviz.monkey_patch(nltk.tree.Tree, node_style_fn=sst.sst_node_style, format='svg')\n",
        "\n",
        "# Test rendering\n",
        "print(\"Sample tree to test rendering:\")\n",
        "nltk.tree.Tree.fromstring(\"(4 (4 (2 I) (3 love) (3 W266)) (3 ðŸ˜„))\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "import os, sys, re, json, time, datetime, shutil\n",
        "import itertools, collections\n",
        "from importlib import reload\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# NLTK, NumPy, and Pandas.\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Helper libraries.\n",
        "from w266_common import utils, vocabulary, tf_embed_viz, treeviz\n",
        "from w266_common import patched_numpy_io\n",
        "# Code for this assignment\n",
        "import sst\n",
        "\n",
        "# Bokeh for plotting.\n",
        "import bokeh.plotting as bp\n",
        "from bokeh.models import HoverTool\n",
        "bp.output_notebook()\n",
        "\n",
        "# Helper code for plotting histograms\n",
        "def plot_length_histogram(lengths, x_range=[0,100], bins=40, normed=True):\n",
        "    hist, bin_edges = np.histogram(a=lengths, bins=bins, normed=normed, range=x_range)\n",
        "    bin_centers = (bin_edges[1:] + bin_edges[:-1])/2\n",
        "    bin_widths =  (bin_edges[1:] - bin_edges[:-1])\n",
        "\n",
        "    hover = HoverTool(tooltips=[(\"bucket\", \"@x\"), (\"count\", \"@top\")], mode=\"vline\")\n",
        "    fig = bp.figure(plot_width=800, plot_height=400, tools=[hover])\n",
        "    fig.vbar(x=bin_centers, width=bin_widths, top=hist, hover_fill_color=\"firebrick\")\n",
        "    fig.y_range.start = 0\n",
        "    fig.x_range.start = 0\n",
        "    fig.xaxis.axis_label = \"Example length (number of tokens)\"\n",
        "    fig.yaxis.axis_label = \"Frequency\"\n",
        "    bp.show(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part (a): The Stanford Sentiment Treebank\n",
        "\n",
        "_If you haven't yet, be sure to familiarize yourself with the [Prelude notebook](Prelude.ipynb), as this assignment will assume familiarity with the text pre-processing steps described there._\n",
        "\n",
        "The [Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/index.html) (SST) is one of the most widely used datasets as a benchmark for text classification. It consists of 11,855 sentences drawn from a corpus of movie reviews (originally from Rotten Tomatoes), each labeled with sentiment on a five-point scale.\n",
        "\n",
        "For example:\n",
        "```\n",
        "sentence: [A warm , funny , engaging film .]\n",
        "label:    4 (very positive)\n",
        "```\n",
        "\n",
        "For this assignment, we'll work with the binarized form of the dataset, where the lowest two classes are mapped to a single \"negative\" label, the highest two are mapped to a single \"positive\" label, and neutral examples are omitted.\n",
        "\n",
        "**Side note:**\n",
        "Unlike most classification datasets, SST is also a _treebank_, which means each sentence is associated with a tree structure that decomposes it into subphrases. So for the example above, we'd also have sentiment labels for `[warm , funny]` and `[engaging film .]` and so on. The trees are created by running the [Stanford Parser](https://nlp.stanford.edu/software/lex-parser.shtml) over the original sentences, then crowdsourcing sentiment labels on each sub-phrase. We'll talk more about treebanks later in the course, and in Assignment 5 you'll implement a simple version of the Stanford Parser.\n",
        "\n",
        "For the purposes of this assignment, we'll mostly concern ourselves with the sentence-level (\"root\") labels, but the tree structure will come in handy in two places:\n",
        "- As a way of analyzing the examples to find instances of negation\n",
        "- (optionally) As a source of additional training data, by including phrase labels\n",
        "\n",
        "### Obtaining the Data\n",
        "The data is distributed as serialized trees in [S-expression](https://en.wikipedia.org/wiki/S-expression) form, like this:\n",
        "```\n",
        "(4 (4 (2 A) (4 (3 (3 warm) (2 ,)) (3 funny))) (3 (2 ,) (3 (4 (4 engaging) (2 film)) (2 .))))\n",
        "```\n",
        "\n",
        "We've provided an `SSTDataset` class (in `sst.py`) which will download the dataset and parse the S-expressions into [`nltk.tree.Tree`](http://www.nltk.org/api/nltk.html?highlight=tree#nltk.tree.Tree) objects that you can easily view in the notebook.\n",
        "\n",
        "`SSTDataset` also implements the text-processing pipeline described in the [Prelude notebook](Prelude.ipynb), and provides methods (`as_sparse_bow` and `as_padded_array`) to convert the data to matrix form.\n",
        "\n",
        "Run the cell below; it will download a ~6MB .zip file to the local directory the first time you run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sst\n",
        "ds = sst.SSTDataset(V=20000).process(label_scheme=\"binary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A few members of the `SSTDataset()` class that you might find useful (click through and skim read):\n",
        "- **`ds.vocab`**: a [`vocabulary.Vocabulary`](https://github.com/datasci-w266/2021-spring-main/blob/master/common/vocabulary.py#L8) object managing the model vocabulary\n",
        "- **`ds.{train,dev,test}_trees`**: a list of [`nltk.tree.Tree`](http://www.nltk.org/api/nltk.html?highlight=tree#nltk.tree.Tree) objects representing each sentence\n",
        "- **`ds.{train,dev,test}`**: a Pandas DataFrame containing the _processed_ examples, including all subphrases. `label` is the target label, `is_root` denotes whether this example is a root node (full sentence), and `root_id` is the index of the tree that the example was derived from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Look at a tree for a positive review\n",
        "ds.dev_trees[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Look at a tree for a negative review\n",
        "ds.dev_trees[361]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Labels.\n",
        "ds.train.label[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part (a) questions: Exploring the Data\n",
        "\n",
        "Let's explore the data a bit, to get a sense of what we're working with. If you're not familiar with DataFrames, you may wish to review the Pandas documentation: https://pandas.pydata.org/pandas-docs/stable/dsintro.html \n",
        "\n",
        "Answer the following questions in the cells below:\n",
        "\n",
        "1. Looking at only the root examples in the training set (*Hint: use `ds.train[ds.train.is_root]`*), what is the fraction of positive labels? Is the classification task balanced, or close to it? If we used most-common-class as a baseline classifier, what would the accuracy be?\n",
        "2. What are the five most common tokens (excluding punctuation) in the entire dataset? (*Hint: there are several ways to get at this - you might use `collections.Counter`, or poke around in the `ds.vocab` object.*)\n",
        "3. Use the `plot_length_histogram` function (defined above) to plot a histogram of the sentence lengths in the training set. What is the 95% percentile length? (i.e. 95% of examples in the training set should be shorter than this)\n",
        "4. Repeat 3., but this time including all subphrases. Notice the difference in distributions. Think about how predicting subphrases might be problematic if they're too short."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use the cells below for your code solutions. Each part shouldn't require more than a couple lines, but you're welcome to explore more!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### YOUR CODE HERE ####\n",
        "# Code for Part (a).1\n",
        "\n",
        "#### END(YOUR CODE) ####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### YOUR CODE HERE ####\n",
        "# Code for Part (a).2\n",
        "\n",
        "#### END(YOUR CODE) ####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### YOUR CODE HERE ####\n",
        "# Code for Part (a).3\n",
        "\n",
        "\n",
        "#### END(YOUR CODE) ####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### YOUR CODE HERE ####\n",
        "# Code for Part (a).4\n",
        "\n",
        "\n",
        "#### END(YOUR CODE) ####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part (b): Naive Bayes\n",
        "\n",
        "In this section, we'll build and explore a Naive Bayes model as a baseline classifier for our dataset.\n",
        "\n",
        "Naive Bayes is perhaps the simplest possible classification algorithm, but it's one that still surprisingly effective for many text classification problems. Recall from your Machine Learning course:\n",
        "\n",
        "$$ P(y = k) = \\hat{\\theta}_k = \\frac{1}{N}\\sum_{i = 1}^N \\mathbf{1}[y_i = k] $$\n",
        "\n",
        "$$ P(x_j | y = 1) = \\hat{\\theta}_{k,j} = \n",
        "\\frac{ \n",
        "\\sum_{i = 1}^N  \\sum_{j' = 1}^{n_i} \\mathbf{1}[y_i = 1 \\wedge x_{j'} = j]\n",
        "}{\n",
        "\\sum_{i = 1}^N  \\mathbf{1}[y_i = 1] \\cdot n_i\n",
        "}\n",
        "$$\n",
        "\n",
        "where $N$ is the size of the dataset, and $n_i$ is the length (number of tokens of the $i^{th}$ example. Prediction is done by computing the score:\n",
        "\n",
        "$$ \\mathrm{score}(x) = \\log \\left(\\frac{P(y = 1) \\prod_{j=1}^n P(x_j | y = 1)}{P(y = 0) \\prod_{j=1}^n P(x_j | y = 0)}\\right) $$\n",
        "\n",
        "We'll just use the [implementation from scikit-learn](http://scikit-learn.org/stable/modules/naive_bayes.html). Like other scikit-learn classifiers, this expects the input as a `scipy.sparse` matrix. Run the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 'csr' stands for \"Compressed Sparse Row\", which is one format\n",
        "# for representing sparse matricies.\n",
        "train_x_csr, train_y = ds.as_sparse_bow(\"train\", root_only=True)\n",
        "test_x_csr,  test_y  = ds.as_sparse_bow(\"test\", root_only=True)\n",
        "print(\"Training set: x = {:s} sparse, y = {:s}\".format(str(train_x_csr.shape), \n",
        "                                                str(train_y.shape)))\n",
        "print(\"Test set:     x = {:s} sparse, y = {:s}\".format(str(test_x_csr.shape), \n",
        "                                                str(test_y.shape)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note the `root_only=True` parameter - this will return only examples corresponding to whole sentences. If you set this to false, you can get examples for all phrases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part (b) Questions\n",
        "\n",
        "Answer the following questions in the **answer file found in this directory** and code cells below.\n",
        "\n",
        "**Question 1.)** Implement Naive Bayes using `sklearn.naive_bayes.MultinomialNB`. Train on the training set and evaluate accuracy on the test set using `.predict(...)`. \n",
        "\n",
        "Your model should train almost instantly, and score between 82% and 83% - not bad! On SST, this is actually a very strong baseline.\n",
        "\n",
        "Recall that Naive Bayes can be interpreted as a linear model, where score is given by:\n",
        "\n",
        "$$ \\mathrm{score}(x) = \\log \\left(\\frac{P(y = 1) \\prod_{j=1}^n P(x_j | y = 1)}{P(y = 0) \\prod_{j=1}^n P(x_j | y = 0)}\\right) \n",
        "= \\left( \\log\\hat{\\theta}_1 - \\log\\hat{\\theta}_0 \\right) + \\sum_{j=1}^n \\left( \\log\\hat{\\theta}_{1,j} - \\log\\hat{\\theta}_{0,j} \\right)$$\n",
        "\n",
        "You can access the values $\\log\\hat{\\theta}_{k,j}$ from the trained model using `nb.feature_log_prob_[k,j]`.\n",
        "\n",
        "**Question 2.)** In the cell below, compute the weights $w_j = \\left( \\log\\hat{\\theta}_{1,j} - \\log\\hat{\\theta}_{0,j} \\right)$ of the linear model, and find the top 10 most negative and most positive weights. _(Hint: use `np.argsort` to get the indices of the most extreme elements.)_ Put the most negative and most positive word in the answers file.  Do the features you found make sense for this domain?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "#### YOUR CODE HERE ####\n",
        "# Code for part (b).1\n",
        "nb = MultinomialNB()\n",
        "\n",
        "\n",
        "#### END(YOUR CODE) ####\n",
        "acc = accuracy_score(test_y, y_pred)\n",
        "print(\"Accuracy on test set: {:.02%}\".format(acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### YOUR CODE HERE ####\n",
        "# Code for part (b).2\n",
        "linear_weights = None  # populate this with actual values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### END(YOUR CODE) ####\n",
        "\n",
        "print(\"Most negative features:\")\n",
        "for idx in top_negative_features:\n",
        "    print(\"  {:s} ({:.02f})\".format(ds.vocab.id_to_word[idx], \n",
        "                                    linear_weights[idx]))\n",
        "print(\"\")\n",
        "print(\"Most positive features:\")\n",
        "for idx in top_positive_features:\n",
        "    print(\"  {:s} ({:.02f})\".format(ds.vocab.id_to_word[idx], \n",
        "                                    linear_weights[idx]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part (c): Examining Negation\n",
        "\n",
        "While Naive Bayes performs well in the aggregate, as a linear model it's still limited in its ability to model complex phenomena in the data. Each feature - in this case, each word - contributes a weight to the total, and if the sum is $\\ge 0$ we predict the example is positive. But what happens when we have an example with both positive and negative words? For instance:\n",
        "\n",
        "```\n",
        "[Brando 's performance fell short of the high standards set by his earlier work .]\n",
        "[A thoughtful look at a painful incident that made headlines in 1995 .]\n",
        "```\n",
        "\n",
        "Run the cell below to evaluate your model on these examples. It should predict both as positive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "examples = [\"Brando 's performance fell short of the high standards set by his earlier work .\",\n",
        "            \"A thoughtful look at a painful incident that made headlines in 1995 .\"]\n",
        "canonicalized_examples = [ds.canonicalize(s.split()) for s in examples]\n",
        "id_lists = [ds.vocab.words_to_ids(s) for s in canonicalized_examples]\n",
        "x = utils.id_lists_to_sparse_bow(id_lists, ds.vocab.size)\n",
        "nb.predict(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part (c).1\n",
        "<a id=\"answers_c1\"></a>\n",
        "\n",
        "**Question 1.)** Why does the model get the first example wrong?  Think about what other kinds of sentence structure might fail to be properly understood by a linear model.  *Hint: See the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "[x for x in zip(canonicalized_examples[0], linear_weights[np.array(id_lists[0])])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It's always important to look at individual examples, but let's try to do this a bit more systematically. Recall that SST gives us labels not only at the whole-sentence (root) level, but for individual phrases as well. We can use this to look for examples where polarity changes between different parts of the sentence. Here's one of the examples above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds.test_trees[210]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following cell will comb through the test set, looking for examples where there's some degree of ambiguity. We'll use a fairly crude heuristic for now: count up all the non-neutral phrases for a given sentence, and look at ones where there's a mix of both positive and negative labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = ds.test\n",
        "\n",
        "gb = df.groupby(by=['root_id'])\n",
        "interesting_ids = []   # root ids, index into ds.test_trees\n",
        "interesting_idxs = []  # DataFrame indices, index into ds.test\n",
        "# This groups the DataFrame by sentence\n",
        "for root_id, idxs in gb.groups.items():\n",
        "    # Get the average score of all the phrases for this sentence\n",
        "    mean = df.loc[idxs].label.mean()\n",
        "    if (mean > 0.4 and mean < 0.6):\n",
        "        interesting_ids.append(root_id)\n",
        "        interesting_idxs.extend(idxs)\n",
        "        \n",
        "print(\"Found {:,} interesting examples\".format(len(interesting_ids)))\n",
        "# This will extract only the \"interesting\" sentences we found above\n",
        "test_x_interesting, test_y_interesting = ds.as_sparse_bow(\"test\", root_only=True, \n",
        "                                                          df_idxs=interesting_idxs)\n",
        "print(\"Interesting ids (into ds.test_trees): \", interesting_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part (c) continued\n",
        "\n",
        "Answer the following in the cells below.\n",
        "\n",
        "**Question 2.)** Examine a few of the \"interesting\" trees. What kinds of patterns do you see? What is the relation of the polarity of the sub-phrases to that of the whole sentence? Is this well-captured by a linear model?\n",
        "\n",
        "**Question 3.)** Evaluate your model on `test_x_interesting` and compute accuracy. Does your model do better or worse on this slice of the data than on the whole test set (interesting + uninteresting examples)? What is the _relative_ change in the error rate? _(For example, if you go from 90% accuracy to 85%, that's a 50% increase in error!)_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### YOUR CODE HERE ####\n",
        "# Code for part (c).2\n",
        "# Example: display(ds.test_trees[idx])\n",
        "\n",
        "\n",
        "#### END(YOUR CODE) ####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### YOUR CODE HERE ####\n",
        "# Code for part (c).3\n",
        "acc = 0.0  # replace this with a real value for accuracy\n",
        "\n",
        "\n",
        "#### END(YOUR CODE) ####\n",
        "print(\"Accuracy on selected examples: {:.02f}%\".format(100*acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
