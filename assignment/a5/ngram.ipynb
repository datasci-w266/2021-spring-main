{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# n-gram Language Modeling\n",
        "\n",
        "In this part of the assignment, we'll expand on the `SimpleTrigramLM` from the live session demo. We'll add smoothing to improve performance on unseen data, and explore some of the properties of the smoothed model.\n",
        "\n",
        "If you haven't looked over the simple trigram LM in [lm1.ipynb](../../materials/simple_lm/lm1.ipynb), we recommend familiarizing yourself with it. This assignment will use a very similar set-up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A note on notation\n",
        "\n",
        "\"Primed\" variables, e.g. $c′$. This is just a dummy variable, not necessarily equal to $c$ but usually assumed to be from the same domain. For example: $P(c)$ is the probability of a particular word $c$, while $\\sum_{c′} P(c′)$ is the sum of the probabilities of all possible words $c′$. If not explicitly stated, the domain is usually the domain of the expression inside the sum - so in this case we'd sum for all words $c′ \\in V$ in the vocabulary.\n",
        " \n",
        "- Set notation: $\\{x : f(x) > 0\\}$ means the set of all $x$ where $f(x)>0$. Strictly speaking, we should write $\\{x∈S:f(x)>0\\}$ where $S$ is some other set, but often we omit this when $S$ is implied (such as words (types) in the vocabulary). So $\\{ b′:C_{b′c} > 0 \\}$ is the set of all words (types) $b′$ where the counts $C_{b′c}$ (for some particular word $c$) are greater than zero.\n",
        " \n",
        "- Similar to the above, $\\left| x:f(x)>0 \\right|$ means the number of elements (size) of that set. So $\\left| b′:C_{b′c} > 0 \\right|$ is the number of words $b′$ where the counts $C_{b′c}$ (for some particular word $c$) are greater than zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, re, json, time, unittest\n",
        "import itertools, collections\n",
        "from importlib import reload\n",
        "\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "import nltk\n",
        "\n",
        "# Helper libraries for this notebook\n",
        "from w266_common import utils, vocabulary\n",
        "import ngram_lm, ngram_lm_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Add-k Smoothing\n",
        "\n",
        "Recall our unsmoothed maximum likelihood estimate of $ P(w_i\\ |\\ w_{i-1}, w_{i-2})$ where we use the raw distribution over words seen in a context in the training data:\n",
        "\n",
        "$$  \\hat{P}(w_i = c\\ |\\ w_{i-1} = b, w_{i-2} = a) = \\frac{C_{abc}}{\\sum_{c'} C_{abc'}} $$\n",
        "\n",
        "Add-k smoothing is the simple refinement where we add $k > 0$ to each count $C_{abc}$, pretending we've seen every vocabulary word $k$ extra times in each context. So we have:\n",
        "\n",
        "$$ \\hat{P}_k(w_i = c\\ |\\ w_{i-1} = b, w_{i-2} = a) = \\frac{C_{abc} + k}{\\sum_{c'} (C_{abc'} + k)} = \\frac{C_{abc} + k}{C_{ab} + k\\cdot|V|} $$\n",
        "\n",
        "where $|V|$ is the size of our vocabulary.\n",
        "\n",
        "In the questions below and in the code, we'll refer to $(w_{i-2}, w_{i-1})$ as the *context*, and $w_i$ as the current *word*. By convention, we'll somewhat interchangeably refer to the sequence $(w_{i-2}, w_{i-1}, w)$ as $abc$.\n",
        "\n",
        "### Part (a): Short answer questions\n",
        "\n",
        "Give brief answers to the following, in the cell below.\n",
        "\n",
        "1. If we encounter a new context `(foo, bar)` unseen in the training data, what will the predicted *distribution* $\\hat{P}_k(w\\ |\\ \\text{foo}, \\text{bar})$ be? How does your answer depend on $k$?\n",
        "<p>\n",
        "2. If we encounter a new word in a familiar context (i.e. `ab` is in the corpus, but `abq` is not), what will our predicted probability $\\hat{P}_k(q\\ |\\ b, a)$ be? Assume $C_{abq} = 0$, $C_{ab}=10$, $k=2$, and $|V|=2000$.\n",
        "<p>\n",
        "3. Based on your answer to question 2, in which context will your model predict a higher probability of *any* unknown word?  \n",
        "Context (a): `<s> the ___`  \n",
        "Context (b): `Mister Rogers ___`  \n",
        "Assume $C_{\\text{<s>, the}} = 10000$ and $C_{\\text{Mister, Rogers}} = 47$.\n",
        "<p>\n",
        "4. Based on your knowledge of language, which of the contexts from question 3 *should* have a higher probability of an unknown word?  _(Hint: recall that most unknown tokens are nouns)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part (b): Implementing the Add-k Model\n",
        "\n",
        "Despite its shortcomings, it's worth implementing an add-k model as a baseline. Unlike the unsmoothed model, we'll be able to get some reasonable (or at least, finite) perplexity numbers which we can compare to the Kneser-Ney model below.\n",
        "\n",
        "We've provided some skeleton code (similar to [lm1.ipynb](../../materials/simple_lm/lm1.ipynb)) in the `ngram_lm.py` file. In the `AddKTrigramLM` class, implement the following:\n",
        "- `__init__(self, words)`, which computes the necessary corpus statistics $C_{abc}$ and $C_{ab}$.\n",
        "- `next_word_proba(self, word, seq, k)`, which computes $\\hat{P}_k(w\\ |\\ w_{i-1}, w_{i-2})$\n",
        "\n",
        "See the function docstrings and in-line comments for more details. In particular, you may want to use `collections.defaultdict` and `dict.get()` to simplify handling of unknown keys. See [dict_notes.md](dict_notes.md) for a brief overview of how to use these.\n",
        "\n",
        "**Note on keys and word-order:** Convention in the math is to write context in reverse order, as in $P(w\\ |\\ w_{i-1}, w_{i-2})$, but in the code it'll be much easier to write things left-to-right as in normal English: so for the context \"`foo bar ___`\", you'll want to use `(\"foo\", \"bar\")` as a dict key.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "reload(ngram_lm)\n",
        "utils.run_tests(ngram_lm_test, [\"TestAddKTrigramLM\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training your Model\n",
        "\n",
        "The same code below can be used with either model; in the cell where it says \"Select your Model\", you can choose the add-k model or the KN model.\n",
        "\n",
        "## Loading & Preprocessing\n",
        "Once again, we'll build our model on the Brown corpus. We'll do an 80/20 train/test split, and preprocess words by lowercasing and replacing digits with `DG` (so `2016` becomes `DGDGDGDG`).\n",
        "\n",
        "In a slight departure from the `lm1.ipynb` demo, we'll restrict the vocabulary to 30000 words. This way, a small fraction of the *training* data will be mapped to `<unk>` tokens, and the model can learn n-gram probabilities that include `<unk>` for prediction on the test set. (If we interpret `<unk>` as meaning \"rare word\", then this is somewhat plausible as a way to infer things about the class of rare words.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert(nltk.download('brown'))  # Make sure we have the data.\n",
        "corpus = nltk.corpus.brown\n",
        "V = 30000\n",
        "train_sents, test_sents = utils.get_train_test_sents(corpus, split=0.8, shuffle=False)\n",
        "# Build vocabulary only on the training set.\n",
        "vocab = vocabulary.Vocabulary((utils.canonicalize_word(w) for w in utils.flatten(train_sents)), size=V)\n",
        "print(\"Train set vocabulary: {:,} words\".format(vocab.size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our smoothed models will also be trigram models, so for convenience we'll also prepend *two* `<s>` markers. (We could avoid this, but then we'd need special handling for the first token of each sentence.)\n",
        "\n",
        "To make it easier to work with, we'll take the list of tokens as a NumPy array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sents_to_tokens(sents):\n",
        "    \"\"\"Returns an flattened list of the words in the sentences, with padding for a trigram model.\"\"\"\n",
        "    padded_sentences = ([u\"<s>\", u\"<s>\"] + s + [u\"</s>\"] for s in sents)\n",
        "    # This will canonicalize words, and replace anything not in vocab with <unk>\n",
        "    return np.array([utils.canonicalize_word(w, wordset=vocab.wordset) \n",
        "                     for w in utils.flatten(padded_sentences)], dtype=object)\n",
        "\n",
        "train_tokens = sents_to_tokens(train_sents)\n",
        "test_tokens = sents_to_tokens(test_sents)\n",
        "print(\"Sample data: \\n\", repr(train_tokens[:20]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Select your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "reload(ngram_lm)\n",
        "\n",
        "Model = ngram_lm.AddKTrigramLM\n",
        "\n",
        "t0 = time.time()\n",
        "print(\"Building trigram LM... \", end=\"\")\n",
        "lm = Model(train_tokens)\n",
        "print(\"done in {:.02f} s\".format(time.time() - t0))\n",
        "lm.print_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "lm.set_live_params(k = 0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sampling Sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_length = 20\n",
        "num_sentences = 5\n",
        "\n",
        "for _ in range(num_sentences):\n",
        "    seq = [\"<s>\", \"<s>\"]\n",
        "    for i in range(max_length):\n",
        "        seq.append(lm.sample_next(seq))\n",
        "        # Stop at end-of-sentence.\n",
        "        if seq[-1] == \"</s>\": break\n",
        "    print(\" \".join(seq))\n",
        "    print(\"[{1:d} tokens; log P(seq): {0:.02f}]\\n\".format(*lm.score_seq(seq)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scoring on Held-Out Data\n",
        "\n",
        "Try playing with the \"k\" parameter above to see how it impacts the result. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_p_data, num_real_tokens = lm.score_seq(train_tokens)\n",
        "print(\"Train perplexity: {:.02f}\".format(2**(-1*log_p_data/num_real_tokens)))\n",
        "\n",
        "log_p_data, num_real_tokens = lm.score_seq(test_tokens)\n",
        "print(\"Test perplexity: {:.02f}\".format(2**(-1*log_p_data/num_real_tokens)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part (e): Additional Questions\n",
        "\n",
        "Answer the following in the cell below.\n",
        "\n",
        "1. What is the average number of times our model sees any particular trigram in the training set, averaged across the trigrams we observed at least once (i.e. ignoring the zeros for trigrams we never observed)? How about averaged across *all possible* trigrams (i.e. including hypothetical ones we never observed)? (*Hint:* you don't need to write any code for this - it should be a quick calculation.) (*Second Hint:* you can factor in the start and end sentence tags or ignore them, we'll accept either answer.)\n",
        "<p>\n",
        "2. Based on your answer above, do you think that a 5-gram model would perform better than the trigram model on the 1 million token Brown corpus?  \n",
        "<p><p>\n",
        "3. Which model generates more \"realistic\" sentences - `AddKTrigramLM` or the unsmoothed `SimpleTrigramLM` from the demo notebook?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Just for fun: Linguistic Curiosities\n",
        "\n",
        "You might have seen this floating around the internet:\n",
        "![Adjective Order](adjective_order.jpg)\n",
        "*source: https://twitter.com/MattAndersonBBC/status/772002757222002688?lang=en*\n",
        "\n",
        "Let's see if it holds true, statistically at least. Note that log probabilities are always negative, so the smaller magnitude is better. And remember the log scale: a difference of score of 8 units means one utterance is $2^8 = 256$ times more likely!\n",
        "\n",
        "For this part, we'll use a trigram model (but with a more sophisticated smoothing technique, **Knesser-Ney**, which we learned about in the async)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import kn_lm\n",
        "reload(kn_lm)\n",
        "\n",
        "Model = kn_lm.KNTrigramLM\n",
        "t0 = time.time()\n",
        "print(\"Building KN trigram LM... \", end=\"\")\n",
        "lm = Model(train_tokens)\n",
        "print(\"done in {:.02f} s\".format(time.time() - t0))\n",
        "lm.print_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_for_scoring(sentence, vocab):\n",
        "    # Pre-process words, replace anything the model doesn't know with <unk>\n",
        "    words = [utils.canonicalize_word(w, wordset=vocab.wordset)\n",
        "             for w in sentence]\n",
        "    # Pad sequence with start and end markers\n",
        "    return [u\"<s>\", u\"<s>\"] + words + [u\"</s>\"]\n",
        "\n",
        "s0 = preprocess_for_scoring(\"square green plastic toys\".split(), vocab)\n",
        "s1 = preprocess_for_scoring(\"plastic green square toys\".split(), vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"s0 score: {:.02f}\".format(lm.score_seq(s0)[0]))\n",
        "print(\"s1 score: {:.02f}\".format(lm.score_seq(s1)[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "noun = \"toys\"\n",
        "adjectives = [\"square\", \"green\", \"plastic\"]\n",
        "results = []\n",
        "for adjs in itertools.permutations(adjectives):\n",
        "    words = list(adjs) + [noun]\n",
        "    seq = preprocess_for_scoring(words, vocab)\n",
        "    score, _ = lm.score_seq(seq)\n",
        "    results.append((score, words))\n",
        "\n",
        "# Sort results\n",
        "for score, words in sorted(results, reverse=True):\n",
        "    print(\"\\\"{:s}\\\" : {:.02f}\".format(\" \".join(words), score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
