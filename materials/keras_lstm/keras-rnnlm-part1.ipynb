{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'returnToTop'></a>\n",
    "\n",
    "# Table of contents\n",
    "  * A. [Introduction to RNN](#introToRnn) \n",
    "    * 1. [Model Structure](#modelStructure)\n",
    "    * 2. [Multi-Layer Cells](#multiLayerCells)\n",
    "    * 3. [Batching and Truncated Backpropagation Through Time (BPTT)](#backProp)\n",
    "    * 4. [Implementing a very simple RNN](#implementRnn)  \n",
    "  * B. [Implementing a simple RNNLM and training on a trivial corpus](#runOnToyInput)  \n",
    "  * C. [Introducing the Dataset input utility](#tfDataset)  \n",
    "  * D. [Run on larger data and use Tensorboard to visualize](#runOnLargerInput) \n",
    "  * E. [Refining results (using sampled softmax loss) -- go to part II](#goToPartII)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'introToRnn'></a>\n",
    "# A. Introduction to Recurrent Neural Network Language Model\n",
    "\n",
    "In this part, we'll learn about building a recurrent neural network language model (RNNLM) using TensorFlow Keras. This class of models represented the cutting edge in language modeling about 5 years ago.  Even though nowadays transformers (like BERT) represent the state-of-the-art for overall accuracy, LSTMs tend to take much less time to train and so with a limited amount of training time and compute resources they can produce surpprisingly good results.  Analyzing and building them is also useful for understanding fundamental concepts of all neural network architectures (states, input / output dimensions, batching, setting up loss and metrics.)\n",
    "\n",
    "As a reference, you may want to review the following:\n",
    "\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) (Andrej Karpathy, 2015)\n",
    "- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) (Chris Olah, 2015)\n",
    "- [A Tensorflow / Keras tutorial on using RNNs for text generation](https://www.tensorflow.org/tutorials/text/text_generation) (updated Oct. 2020)\n",
    "\n",
    "The specific model we'll build is based on the following papers. You should skim these (particularly the first one), but you don't need to read them in detail:\n",
    "\n",
    "- [Recurrent neural network based language model](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf) (Mikolov, et al. 2010)\n",
    "- [Exploring the Limits of Language Modeling](http://arxiv.org/pdf/1602.02410.pdf) (Jozefowicz, et al. 2016)\n",
    "\n",
    "We'll build our model entirely in TensorFlow Keras, so you may want to review the [TensorFlow section of assignment 1](../a1/tensorflow/tensorflow.ipynb).\n",
    "\n",
    "Finally, you'll possibly want to consult the [TensorFlow Keras API reference](https://www.tensorflow.org/api_docs/python/tf/keras), and pay special attention to the types, dimensions and order of arguments for each function.  As we suggested you do in Assignment 1, you'll want to **draw the shape of any matrices you work with on a scrap paper** or you may have trouble keeping track of your forward path!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"2.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "\n",
    "Notebook I consists of 7 parts:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'modelStructure'></a>\n",
    "## RNNLM Model Structure\n",
    "\n",
    "![RNNLM](images/rnnlm_layers.png)\n",
    "\n",
    "Here's the basic spec for our model. We'll use the following notation:\n",
    "\n",
    "- $w^{(i)}$ for the $i^{th}$ word of the sequence (as an integer index)\n",
    "- $x^{(i)}$ for the vector representation of $w^{(i)}$\n",
    "- $h^{(i)}$ for the $i^{th}$ hidden state, with indices as in Section 5.8 of the async\n",
    "- $o^{(i)}$ for the $i^{th}$ output state, which may or may not be the same as the hidden state\n",
    "- $y^{(i)}$ for the $i^{th}$ target word, which for a language model is always equal to $w^{(i+1)}$\n",
    "\n",
    "Let $ h^{(-1)} = h^{init} $ be an initial state. For an input sequence of $n$ words and $i = 0, ..., n-1$, we have:\n",
    "\n",
    "- **Embedding layer:** $ x^{(i)} = W_{in}[w^{(i)}] $\n",
    "- **Recurrent layer:** $ (o^{(i)}, h^{(i)}) = \\text{CellFunc}(x^{(i)}, h^{(i-1)}) $\n",
    "- **Output layer:** $\\hat{P}(y^{(i)}) = \\hat{P}(w^{(i+1)}) = \\text{softmax}(o^{(i)}W_{out} + b_{out}) $\n",
    " \n",
    "$\\text{CellFunc}$ can be an arbitrary function representing our recurrent cell - it can be a simple RNN cell, or something more complicated like an LSTM, or even a stacked multi-layer cell. *Note that the cell has its own internal, trainable parameters.*\n",
    "\n",
    "It may be convenient to deal with the logits of the output layer, which are the un-normalized inputs to the softmax:\n",
    "\n",
    "$$ \\text{logits}^{(i)} = o^{(i)}W_{out} + b_{out} $$\n",
    "\n",
    "We'll use these as shorthand for important dimensions:\n",
    "- `V` : vocabulary size\n",
    "- `H` : hidden state size = embedding size = per-cell output size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'multiLayerCells'></a>\n",
    "### Multi-Layer Cells\n",
    "\n",
    "One popular technique for improving the performance of RNNs is to stack multiple layers. Conceptually, this is similar to an ordinary multi-layer network, such as those you implemented on Assignment 1.\n",
    "\n",
    "![RNNLM - multicell](images/rnnlm_multicell.png)\n",
    "\n",
    "**Recurent layer 1** will take embeddings $ x^{(i)} $ as inputs and produce outputs $o^{(i)}_0$. We can feed these in to **Recurrent layer 2**, and get another set of outputs $o^{(i)}_1$, and so on. Note that because the input dimension of an RNN cell is typically the same as the output, all of these layers will have the same shape.\n",
    "\n",
    "In TensorFlow, a single RNN layer is composed of an [LSTM cell](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell), which represents one unit of time in our model (one word in our diagram above.  If you want to stack multiple layers at each time-step you can use [tf.keras.layers.StackedRNNCells](https://www.tensorflow.org/api_docs/python/tf/keras/layers/StackedRNNCells). The `StackedRNNCells` object provides a vertically-stacked cell, as shown by the dashed green lines above.\n",
    "\n",
    "Effectively the concept of time is an abstraction and ultimately at the end of training the model parameters of `cell` above are identical at each time position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'backProp'></a>\n",
    "## Batching and Truncated Backpropagation Through Time (BPTT)\n",
    "\n",
    "Batching for an RNN works the same as for any neural network: we'll run several copies of the RNN simultaneously, each with their own hidden state and outputs. Most TensorFlow functions are batch-aware, and expect `batch_size` as the first dimension.\n",
    "\n",
    "With RNNs, however, we also need to consider the sequence length. In theory, we model our RNN as operating on sequences of arbitary length, but in practice it's much more efficient to work with batches where all the sequences have the same (maximum) length. TensorFlow calls this dimension `max_time`.  _Note: since LSTMs model sequences, a lot of the nomenclature around them mentions \"time\".  Whenever you see a reference to \"time\" in documentation, just read it as \"word sequence position(s)\"._\n",
    "\n",
    "Put together, it looks like this, where our inputs $w$ and targets $y$ will both be 2D arrays of shape `[batch_size, max_time]`.\n",
    "\n",
    "![RNNLM - batching](images/rnnlm_batching.png)\n",
    "\n",
    "Note that along the batch dimension, sequences are independent. Along the time dimension, the output of one timestep is fed into the next. \n",
    "\n",
    "In the common case of processing sequences longer than `max_time`, we can chop the input up into smaller chunks, and carry the final hidden state from one batch as the input to the next. For example, given the input `[a b c d e f g h]` and `max_time = 4`, we would run twice:\n",
    "```\n",
    "h_init    -> RNN on [a b c d] -> h_final_0\n",
    "h_final_0 -> RNN on [e f g h] -> h_final_1\n",
    "```\n",
    "We can also do this with batches, taking care to construct our batches in such a way that each batch lines up with it's predecessor. For example, with inputs `[a b c d e f g h]` and `[s t u v w x y z]`, we would do:\n",
    "```\n",
    "h_init    -> RNN on [a b c d] -> h_final_0\n",
    "                    [s t u v]\n",
    "\n",
    "h_final_0 -> RNN on [e f g h] -> h_final_1\n",
    "                    [w x y z]\n",
    "```\n",
    "where our hidden states `h_init`, etc. have shape `[batch_size, state_size]`. (*Note that `state_size = H` for a simple RNN, but is larger for LSTMs or stacked cells.*)\n",
    "\n",
    "Training in this setting is known as *truncated backpropagation through time*, or truncated BPTT. We can backpropagate errors within a batch for up to `max_time` timesteps, but not any further past the batch boundary. In practice with `max_time` greater than 20 or so, this doesn't significantly hurt the performance of our language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'implementRnn'></a>\n",
    "## Choosing an optimizer\n",
    "\n",
    "For training steps, you can use any optimizer, but we recommend `tf.train.AdamOptimizer` with gradient clipping (`tf.clip_by_global_norm`).  Adam adjusts the learning rate on a per-variable basis, and also adds a \"momentum\" term that improves the speed of convergence. See [An overview of gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/) for more.\n",
    "\n",
    "For training with AdamOptimizer, you want to use the `learning_rate = 0.01` as defined under \"Training Parameters\" (next to batch size, num epochs, etc.). If you use `learning_rate = 0.1` with Adam, the model will likely overfit or training may be unstable. (However, 0.1 works well with Adagrad and vanilla SGD.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'runOnToyInput'></a>\n",
    "# (B) Let's implement a simple RNNLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# NumPy and TensorFlow\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"2.\"))\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend\n",
    "assert(tf.__version__.startswith(\"2.\"))\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras import Model, layers\n",
    "\n",
    "# Tensorboard\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "\n",
    "# Tensorboard\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary, tf_embed_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a very simple one-layer RNN\n",
    "# We'll stick with using Keras functional format models as these give you more flexibility.\n",
    "\n",
    "# Use a simple toy corpus\n",
    "toy_corpus = \"<s> Mary had a little lamb . <s> The lamb was white as snow . <s>\"\n",
    "sequence = np.array(toy_corpus.split())\n",
    "labels = np.array(\"Mary had a little lamb . <s> The lamb was white as snow . <s> And\".split())\n",
    "vocab = vocabulary.Vocabulary(sequence)\n",
    "\n",
    "# Model parameters\n",
    "modelParams = dict(\n",
    "    max_time = 8,   # length of words per batch\n",
    "    batch_size = 2, \n",
    "    learning_rate = 0.01,\n",
    "    V = vocab.size,\n",
    "    H = 10,\n",
    "    num_layers = 1,\n",
    "    dropout_rate = 0.1, \n",
    "    metrics = ['accuracy'],\n",
    "    # metrics = ['categorical_accuracy']\n",
    ")\n",
    "\n",
    "# Define a function to create our LSTM model\n",
    "def create_lstm_model(**kwargs):\n",
    "    max_time = kwargs.get('max_time', 8)   # length of words per batch\n",
    "    batch_size = kwargs.get('batch_size', 2) \n",
    "    learning_rate = kwargs.get('learning_rate', 0.01)\n",
    "    V = kwargs.get('V', vocab.size)\n",
    "    H = kwargs.get('H', 10)\n",
    "    num_layers = kwargs.get('num_layers', 1)\n",
    "    dropout_rate = kwargs.get('dropout_rate', 0.1)\n",
    "    metrics = kwargs.get('metrics', ['accuracy'])\n",
    "    # Create an input layer for our model\n",
    "    # input_ = Input(batch_shape = [batch_size, max_time], name=\"x\")\n",
    "    input_ = Input(shape = [max_time], name=\"x\")\n",
    "    # Create an embedding layer of dimension V x H\n",
    "    embedding_layer = Embedding(input_dim=V, output_dim=H, \n",
    "                                embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-1, maxval=1), \n",
    "                                trainable=True)\n",
    "    x = embedding_layer(input_)\n",
    "    \n",
    "    # Create hidden layers\n",
    "    for i in range(num_layers):\n",
    "        # x, state, memory = keras.layers.LSTM(H, return_sequences = True, \n",
    "        #                   return_state=True, stateful=False)(x)\n",
    "        x = keras.layers.LSTM(H, return_sequences = True, \n",
    "                          return_state=False, stateful=False)(x)\n",
    "        x = keras.layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Create an output layer with softmax output type\n",
    "    outlayer = keras.layers.Dense(V, \n",
    "                                  activation = \"softmax\", \n",
    "                                  kernel_initializer = \n",
    "                                  tf.keras.initializers.RandomUniform(minval=-1.0, maxval=1.0), \n",
    "                                  bias_initializer = tf.keras.initializers.Zeros())\n",
    "    \n",
    "    # Predicted outputs from output layer\n",
    "    yhat = outlayer(x)\n",
    "    # Build / compile the model\n",
    "    ## Use Adam optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, \n",
    "                                         beta_2=0.999, epsilon=1e-07, amsgrad=False, \n",
    "                                         name='Adam')       \n",
    "    ## Use accuracy metrics for evaluation\n",
    "    # metrics = \"accuracy\"\n",
    "    \n",
    "    ## Use sparse categorical crossentropy since we are outputting a single word \n",
    "    ## ...rather than a one-hot vector.\n",
    "    loss_ = \"sparse_categorical_crossentropy\"\n",
    "    \n",
    "    ## Build / Compile model\n",
    "    model = Model(inputs=input_, outputs=yhat, name=\"rnn_prediction_model\")\n",
    "    model.compile(optimizer = optimizer, loss = \"sparse_categorical_crossentropy\",\n",
    "                       metrics = metrics)\n",
    "    return model\n",
    "\n",
    "modelSimple = create_lstm_model(**modelParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output layer weights are dimension: (10, 15) and biases are dimension: (15,).\n",
      "\n",
      "\n",
      "Hidden layer weights are dimension:\n",
      "   input weights: (10, 40),\n",
      "   state-input weights: (10, 40),\n",
      "  and biases: (40,)\n",
      "Note that input and state-input weights have H (10) each for:\n",
      "  --input,\n",
      "  --forget,\n",
      "  --memory, and\n",
      "  --output gates\n",
      "Shapes of embedding layer weights: (15, 10)\n"
     ]
    }
   ],
   "source": [
    "# Let's inspect our model to get a feel for dimensions! \n",
    "\n",
    "# You can refer to model layers and weights by using the self.layers and self.weights objects\n",
    "output_layer = modelSimple.layers[-1]\n",
    "print('Output layer weights are dimension: {} and biases are dimension: {}.\\n\\n'.format(\n",
    "    output_layer.weights[0].shape, output_layer.weights[1].shape))\n",
    "\n",
    "hidden_layer = modelSimple.layers[-3]\n",
    "print('Hidden layer weights are dimension:\\n   input weights: {},\\n   state-input weights: {},\\n  and biases: {}'.format(\n",
    "    hidden_layer.weights[0].shape, hidden_layer.weights[1].shape, \n",
    "    hidden_layer.weights[2].shape))\n",
    "print('Note that input and state-input weights have H (10) each for:\\n  --input,\\n  --forget,\\n  --memory, and\\n  --output gates')\n",
    "\n",
    "embedding_layer = modelSimple.layers[-4]\n",
    "print('Shapes of embedding layer weights: {}'.format(\n",
    "    embedding_layer.weights[0].shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 8) (200, 8)\n",
      "Train on 200 samples\n",
      "Epoch 1/20\n",
      "200/200 [==============================] - 1s 6ms/sample - loss: 2.4601 - accuracy: 0.3187\n",
      "Epoch 2/20\n",
      "200/200 [==============================] - 0s 197us/sample - loss: 2.0784 - accuracy: 0.3856\n",
      "Epoch 3/20\n",
      "200/200 [==============================] - 0s 159us/sample - loss: 1.7168 - accuracy: 0.4819\n",
      "Epoch 4/20\n",
      "200/200 [==============================] - 0s 159us/sample - loss: 1.3715 - accuracy: 0.6212\n",
      "Epoch 5/20\n",
      "200/200 [==============================] - 0s 159us/sample - loss: 1.0612 - accuracy: 0.8163\n",
      "Epoch 6/20\n",
      "200/200 [==============================] - 0s 162us/sample - loss: 0.7874 - accuracy: 0.9350\n",
      "Epoch 7/20\n",
      "200/200 [==============================] - 0s 168us/sample - loss: 0.5683 - accuracy: 0.9619\n",
      "Epoch 8/20\n",
      "200/200 [==============================] - 0s 168us/sample - loss: 0.4116 - accuracy: 0.9762\n",
      "Epoch 9/20\n",
      "200/200 [==============================] - 0s 171us/sample - loss: 0.3015 - accuracy: 0.9781\n",
      "Epoch 10/20\n",
      "200/200 [==============================] - 0s 166us/sample - loss: 0.2370 - accuracy: 0.9900\n",
      "Epoch 11/20\n",
      "200/200 [==============================] - 0s 166us/sample - loss: 0.1798 - accuracy: 0.9969\n",
      "Epoch 12/20\n",
      "200/200 [==============================] - 0s 164us/sample - loss: 0.1530 - accuracy: 0.9950\n",
      "Epoch 13/20\n",
      "200/200 [==============================] - 0s 165us/sample - loss: 0.1290 - accuracy: 0.9975\n",
      "Epoch 14/20\n",
      "200/200 [==============================] - 0s 165us/sample - loss: 0.1094 - accuracy: 0.9975\n",
      "Epoch 15/20\n",
      "200/200 [==============================] - 0s 166us/sample - loss: 0.0990 - accuracy: 0.9937\n",
      "Epoch 16/20\n",
      "200/200 [==============================] - 0s 162us/sample - loss: 0.0850 - accuracy: 0.9981\n",
      "Epoch 17/20\n",
      "200/200 [==============================] - 0s 153us/sample - loss: 0.0747 - accuracy: 0.9981\n",
      "Epoch 18/20\n",
      "200/200 [==============================] - 0s 161us/sample - loss: 0.0710 - accuracy: 0.9975\n",
      "Epoch 19/20\n",
      "200/200 [==============================] - 0s 167us/sample - loss: 0.0632 - accuracy: 0.9981\n",
      "Epoch 20/20\n",
      "200/200 [==============================] - 0s 162us/sample - loss: 0.0589 - accuracy: 0.9987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x149a10650>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now train the model\n",
    "ids = vocab.words_to_ids(sequence)\n",
    "ids = np.vstack([ids]*100)\n",
    "ids = ids.reshape([-1, 8])\n",
    "y = vocab.words_to_ids(labels)\n",
    "y = np.vstack([y]*100).reshape([-1, 8])\n",
    "print(ids.shape, y.shape)\n",
    "modelSimple.fit(ids, y, epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'tfDataset'></a>\n",
    "[Return to Top](#top)\n",
    "# C. Introducing the Dataset input utility\n",
    "\n",
    "To make life easier, Keras offers a [Dataset](https://www.tensorflow.org/guide/data) interface to easily wrangle input and label data for feeding and training models (not just for an LSTM model but any Keras model.)\n",
    "\n",
    "Let's use this to create an input dataset for a larger corpus for our next RNN model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/drewplant/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: 10,000 types\n",
      "Loaded 57,340 sentences (1.16119e+06 tokens)\n",
      "Training set: 45,872 sentences (924,077 tokens)\n",
      "Test set: 11,468 sentences (237,115 tokens)\n",
      "Raw shapes:\n",
      "  train_ids: (969950,),\n",
      "  test_ids: (248584,)\n",
      "\n",
      "\n",
      "Training input (x) shape: (969925,),  training labels (y) shape: (969925,)\n",
      "<TensorSliceDataset shapes: ((), ()), types: (tf.int32, tf.int32)>\n",
      "\n",
      "\n",
      "Demo-ing using batch to create sequences:\n",
      "Input array: [   0  304  657  434    0    7   42  213   42   36  977  391    5    0\n",
      " 5970 1097    3  250   34    3    2    8    3 1196    5]\n",
      "Label array: [ 304  657  434    0    7   42  213   42   36  977  391    5    0 5970\n",
      " 1097    3  250   34    3    2    8    3 1196    5    0]\n",
      "\n",
      "\n",
      "Input array: [   0   28   58 1398   42    8  140    9 2684 2806    6  126  113    3\n",
      " 1114   15    3 4526   58  422    5    0    3    2   16]\n",
      "Label array: [  28   58 1398   42    8  140    9 2684 2806    6  126  113    3 1114\n",
      "   15    3 4526   58  422    5    0    3    2   16 5080]\n",
      "\n",
      "\n",
      "Input array: [5080 8564   17    4   10   39  331   12    2   26 4391   24   24    0\n",
      "   38   32   10  190  122 2336   11  189  290    8   36]\n",
      "Label array: [8564   17    4   10   39  331   12    2   26 4391   24   24    0   38\n",
      "   32   10  190  122 2336   11  189  290    8   36   52]\n",
      "\n",
      "\n",
      "Input array: [  52    2    8 3779   35 4118    5    0  220   38   32  662   24   24\n",
      "    0    3 5171   13 2338   10 2108    4   66    4    7]\n",
      "Label array: [   2    8 3779   35 4118    5    0  220   38   32  662   24   24    0\n",
      "    3 5171   13 2338   10 2108    4   66    4    7 4018]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "V = 10000\n",
    "vocab, train_ids, test_ids = utils.load_corpus(\"brown\", split=0.8, V=V, shuffle=42)\n",
    "\n",
    "max_time = 25   # length of words per batch\n",
    "batch_size = 100\n",
    "\n",
    "# Look at the output shapes for train_ids, test_ids\n",
    "print('Raw shapes:\\n  train_ids: {},\\n  test_ids: {}\\n\\n'.format(train_ids.shape, test_ids.shape))\n",
    "\n",
    "\n",
    "# Truncate training_set to have complete rows only (considering our max_time parameter.)\n",
    "trainlen_round = int((train_ids.shape[0] - max_time)/ max_time) * max_time\n",
    "# trainlen_round = int((1000 - max_time)/ max_time) * max_time\n",
    "trainroundx = train_ids[:trainlen_round]\n",
    "trainroundy = train_ids[1:trainlen_round+1] # input words one position ahead\n",
    "# trainx_shaped = np.reshape(train_ids[:trainlen_round],[-1,max_time])\n",
    "# trainy_shaped = np.reshape(train_ids[1:trainlen_round + 1],[-1,max_time])\n",
    "print('Training input (x) shape: {},  training labels (y) shape: {}'.format(trainroundx.shape, trainroundy.shape))\n",
    "\n",
    "# A Dataset object packages input numpy objects into tensorflow objects of the same dims\n",
    "\n",
    "\n",
    "# Package trainx, trainy into a train input dataset\n",
    "# trainds = tf.data.Dataset.from_tensor_slices((trainx_shaped, trainy_shaped)).shuffle(buffer_size = 3).batch(batch_size)\n",
    "trainds = tf.data.Dataset.from_tensor_slices((trainroundx, trainroundy))\n",
    "\n",
    "print(trainds)\n",
    "\n",
    "# Dataset has a batch method.  Use this first to create sequences of max_time:\n",
    "trainds = trainds.batch(max_time)\n",
    "\n",
    "# Show that dataset batching did the right thing:\n",
    "print('\\n\\nDemo-ing using batch to create sequences:')\n",
    "for batch in trainds.take(4):\n",
    "  print('Input array: {}\\nLabel array: {}\\n\\n'.format(batch[0].numpy(), batch[1].numpy()))\n",
    " \n",
    "# You can and should shuffle the rows of a dataset.  \n",
    "# In fact, TF will warn you if you use an un-shuffled dataset.\n",
    "# You can read about shuffle here:  ()\n",
    "trainds = trainds.shuffle(max_time * batch_size_)  # Shuffle rows from each of 100-row buffers\n",
    "\n",
    "# Finally, create a batch dimension for input data\n",
    "trainds = trainds.batch(batch_size_)\n",
    "\n",
    "\n",
    "# Repeat for test set\n",
    "testlen_round = int((test_ids.shape[0] - max_time) / max_time) * max_time\n",
    "testroundx = test_ids[:testlen_round]\n",
    "testroundy = test_ids[1:testlen_round + 1]\n",
    "# Bundle testx, testy into a validation input dataset\n",
    "testds = tf.data.Dataset.from_tensor_slices((testroundx, testroundy)).batch(max_time).batch(batch_size_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'runOnLargerInput'></a> \n",
    "[Top](#top)\n",
    "# D. An industrial-strength RNN on a larger data\n",
    "Let's build a new RNN model and fit to our larger Brown corpus dataset\n",
    "\n",
    "You can use [Tensorboard](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard) for visualizing your model's structure, as well as viewing training and validation accuracy as your training fit progresses.\n",
    "\n",
    "To create tensorboard data, simply create a callback and include the callback inside the call to fit() as shown below.  Then run the tensorboard command in a terminal window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To open tensorboard, execute the following command:\n",
      "  tensorboard --logdir=./logs\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trainds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-df02fe803ef8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Train our model using training and verification datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m modelindustrial.fit(trainds, validation_data = testds, epochs = 10, \n\u001b[0m\u001b[1;32m     22\u001b[0m                     callbacks = [tb_callback])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainds' is not defined"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "modelParams = dict(\n",
    "    max_time = 25,   # length of words per batch\n",
    "    batch_size = 100, \n",
    "    learning_rate = 0.01,\n",
    "    V = vocab.size,\n",
    "    H = 200,\n",
    "    num_layers = 2,\n",
    "    dropout_rate = 0.1,\n",
    "    metrics = ['sparse_categorical_accuracy']\n",
    ")\n",
    "\n",
    "modelindustrial = create_lstm_model(**modelParams)\n",
    "\n",
    "# Create a tensorboard callback\n",
    "tb_logpath = './logs'\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(tb_logpath, update_freq=1)\n",
    "print('To open tensorboard, execute the following command:\\n  tensorboard --logdir={}\\n\\n'.format(tb_logpath))\n",
    "\n",
    "# Train our model using training and verification datasets\n",
    "modelindustrial.fit(trainds, validation_data = testds, epochs = 10, \n",
    "                    callbacks = [tb_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'goToPartII'></a> \n",
    "[Top](#top)\n",
    "# E. Intro to part II -- Advanced training using sampled softmax loss  \n",
    "\n",
    "This notebook has shown how to build and train a basic LSTM model using Keras.  In part II (coming) you will be introduced to customizing your model, loss and metric functions in order to provide greater flexibility into how training and inference are carried out with your LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
