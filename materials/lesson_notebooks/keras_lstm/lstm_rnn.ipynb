{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'top'></a>\n",
    "\n",
    "# Notebook Contents\n",
    "  * A. [Introduction to RNN](#introToRnn) \n",
    "    * 1. [Model Structure](#modelStructure)\n",
    "    * 2. [Multi-Layer Cells](#multiLayerCells)\n",
    "    * 3. [Batching and Truncated Backpropagation Through Time (BPTT)](#backProp)\n",
    "  * B. [Introducing the StringLookup and Dataset utilities](#kerasUtilities)  \n",
    "  * C. [Create model / setup Tensorboard / train](#createTensorboardTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'introToRnn'></a>\n",
    "# A. Introduction to Recurrent Neural Network Language Model\n",
    "\n",
    "In this part, we'll learn about building a recurrent neural network language model (RNNLM) using TensorFlow Keras. This class of models represented the cutting edge in language modeling about 5 years ago.  Even though nowadays transformers (like BERT) represent the state-of-the-art for overall accuracy, LSTMs tend to take much less time to train and so with a limited amount of training time and compute resources they can produce surpprisingly good results.  Analyzing and building them is also useful for understanding fundamental concepts of all neural network architectures (states, input / output dimensions, batching, setting up loss and metrics.)\n",
    "\n",
    "As a reference, you may want to review the following:\n",
    "\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) (Andrej Karpathy, 2015)\n",
    "- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) (Chris Olah, 2015)\n",
    "- [A Tensorflow / Keras tutorial on using RNNs for text generation](https://www.tensorflow.org/tutorials/text/text_generation) (updated Oct. 2020)\n",
    "\n",
    "The specific model we'll build is based on the following papers. You should skim these (particularly the first one), but you don't need to read them in detail:\n",
    "\n",
    "- [Recurrent neural network based language model](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf) (Mikolov, et al. 2010)\n",
    "- [Exploring the Limits of Language Modeling](http://arxiv.org/pdf/1602.02410.pdf) (Jozefowicz, et al. 2016)\n",
    "\n",
    "We'll build our model entirely in TensorFlow Keras, so you may want to review the [TensorFlow section of assignment 1](../a1/tensorflow/tensorflow.ipynb).\n",
    "\n",
    "Finally, you'll possibly want to consult the [TensorFlow Keras API reference](https://www.tensorflow.org/api_docs/python/tf/keras), and pay special attention to the types, dimensions and order of arguments for each function.  As we suggested you do in Assignment 1, you'll want to **draw the shape of any matrices you work with on a scrap paper** or you may have trouble keeping track of your forward path!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "\n",
    "Notebook I consists of 7 parts:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'modelStructure'></a>\n",
    "## RNNLM Model Structure\n",
    "\n",
    "![RNNLM](images/rnnlm_layers.png)\n",
    "\n",
    "Here's the basic spec for our model. We'll use the following notation:\n",
    "\n",
    "- $w^{(i)}$ for the $i^{th}$ word of the sequence (as an integer index)\n",
    "- $x^{(i)}$ for the vector representation of $w^{(i)}$\n",
    "- $h^{(i)}$ for the $i^{th}$ hidden state, with indices as in Section 5.8 of the async\n",
    "- $o^{(i)}$ for the $i^{th}$ output state, which may or may not be the same as the hidden state\n",
    "- $y^{(i)}$ for the $i^{th}$ target word, which for a language model is always equal to $w^{(i+1)}$\n",
    "\n",
    "Let $ h^{(-1)} = h^{init} $ be an initial state. For an input sequence of $n$ words and $i = 0, ..., n-1$, we have:\n",
    "\n",
    "- **Embedding layer:** $ x^{(i)} = W_{in}[w^{(i)}] $\n",
    "- **Recurrent layer:** $ (o^{(i)}, h^{(i)}) = \\text{CellFunc}(x^{(i)}, h^{(i-1)}) $\n",
    "- **Output layer:** $\\hat{P}(y^{(i)}) = \\hat{P}(w^{(i+1)}) = \\text{softmax}(o^{(i)}W_{out} + b_{out}) $\n",
    " \n",
    "$\\text{CellFunc}$ can be an arbitrary function representing our recurrent cell - it can be a simple RNN cell, or something more complicated like an LSTM, or even a stacked multi-layer cell. *Note that the cell has its own internal, trainable parameters.*\n",
    "\n",
    "It may be convenient to deal with the logits of the output layer, which are the un-normalized inputs to the softmax:\n",
    "\n",
    "$$ \\text{logits}^{(i)} = o^{(i)}W_{out} + b_{out} $$\n",
    "\n",
    "We'll use these as shorthand for important dimensions:\n",
    "- `V` : vocabulary size\n",
    "- `H` : hidden state size = embedding size = per-cell output size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'multiLayerCells'></a>\n",
    "### Multi-Layer Cells\n",
    "\n",
    "One popular technique for improving the performance of RNNs is to stack multiple layers. Conceptually, this is similar to an ordinary multi-layer network, such as those you implemented on Assignment 1.\n",
    "\n",
    "![RNNLM - multicell](images/rnnlm_multicell.png)\n",
    "\n",
    "**Recurrent layer 1** will take embeddings $ x^{(i)} $ as inputs and produce outputs $o^{(i)}_0$. We can feed these in to **Recurrent layer 2**, and get another set of outputs $o^{(i)}_1$, and so on. Note that because the input dimension of an RNN cell is typically the same as the output, all of these layers will have the same shape.\n",
    "\n",
    "In TensorFlow, a single RNN layer is composed of an [LSTM cell](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell), which represents one unit of time in our model (one word in our diagram above.  If you want to stack multiple layers at each time-step you can use [tf.keras.layers.StackedRNNCells](https://www.tensorflow.org/api_docs/python/tf/keras/layers/StackedRNNCells). The `StackedRNNCells` object provides a vertically-stacked cell, as shown by the dashed green lines above.\n",
    "\n",
    "Effectively the concept of time is an abstraction and ultimately at the end of training the model parameters of `cell` above are identical at each time position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'backProp'></a>\n",
    "## Batching and Truncated Backpropagation Through Time (BPTT)\n",
    "\n",
    "Batching for an RNN works the same as for any neural network: we'll run several copies of the RNN simultaneously, each with their own hidden state and outputs. Most TensorFlow functions are batch-aware, and expect `batch_size` as the first dimension.\n",
    "\n",
    "With RNNs, however, we also need to consider the sequence length. In theory, we model our RNN as operating on sequences of arbitary length, but in practice it's much more efficient to work with batches where all the sequences have the same (maximum) length. TensorFlow calls this dimension `max_time`.  _Note: since LSTMs model sequences, a lot of the nomenclature around them mentions \"time\".  Whenever you see a reference to \"time\" in documentation, just read it as \"word sequence position(s)\"._\n",
    "\n",
    "Put together, it looks like this, where our inputs $w$ and targets $y$ will both be 2D arrays of shape `[batch_size, max_time]`.\n",
    "\n",
    "![RNNLM - batching](images/rnnlm_batching.png)\n",
    "\n",
    "Note that along the batch dimension, sequences are independent. Along the time dimension, the output of one timestep is fed into the next. \n",
    "\n",
    "In the common case of processing sequences longer than `max_time`, we can chop the input up into smaller chunks, and carry the final hidden state from one batch as the input to the next. For example, given the input `[a b c d e f g h]` and `max_time = 4`, we would run twice:\n",
    "```\n",
    "h_init    -> RNN on [a b c d] -> h_final_0\n",
    "h_final_0 -> RNN on [e f g h] -> h_final_1\n",
    "```\n",
    "We can also do this with batches, taking care to construct our batches in such a way that each batch lines up with it's predecessor. For example, with inputs `[a b c d e f g h]` and `[s t u v w x y z]`, we would do:\n",
    "```\n",
    "h_init    -> RNN on [a b c d] -> h_final_0\n",
    "                    [s t u v]\n",
    "\n",
    "h_final_0 -> RNN on [e f g h] -> h_final_1\n",
    "                    [w x y z]\n",
    "```\n",
    "where our hidden states `h_init`, etc. have shape `[batch_size, state_size]`. (*Note that `state_size = H` for a simple RNN, but is larger for LSTMs or stacked cells.*)\n",
    "\n",
    "Training in this setting is known as *truncated backpropagation through time*, or truncated BPTT. We can backpropagate errors within a batch for up to `max_time` timesteps, but not any further past the batch boundary. In practice with `max_time` greater than 20 or so, this doesn't significantly hurt the performance of our language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'implementRnn'></a>\n",
    "## Choosing an optimizer\n",
    "\n",
    "For training steps, you can use any optimizer, but we recommend `tf.train.AdamOptimizer` with gradient clipping (`tf.clip_by_global_norm`).  Adam adjusts the learning rate on a per-variable basis, and also adds a \"momentum\" term that improves the speed of convergence. See [An overview of gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/) for more.\n",
    "\n",
    "For training with AdamOptimizer, you want to use the `learning_rate = 0.01` as defined under \"Training Parameters\" (next to batch size, num epochs, etc.). If you use `learning_rate = 0.1` with Adam, the model will likely overfit or training may be unstable. (However, 0.1 works well with Adagrad and vanilla SGD.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'runOnToyInput'></a>\n",
    "# (B) Let's implement a simple RNNLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "# NumPy and TensorFlow\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "assert(tf.__version__.startswith(\"2.\"))\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend\n",
    "assert(tf.__version__.startswith(\"2.\"))\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras import Model, layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "\n",
    "# Tensorboard\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary, tf_embed_viz\n",
    "\n",
    "# From sklearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Let's inspect our model to get a feel for dimensions! \n",
    "\n",
    "# You can refer to model layers and weights by using the self.layers and self.weights objects\n",
    "output_layer = modelSimple.layers[-1]\n",
    "print('Output layer weights are dimension: {} and biases are dimension: {}.\\n\\n'.format(\n",
    "    output_layer.weights[0].shape, output_layer.weights[1].shape))\n",
    "\n",
    "hidden_layer = modelSimple.layers[-4]   # Note that layers[-2] is our dropout and layers[-3] is our batchnorm\n",
    "print('Hidden layer weights are dimension:\\n   input weights: {},\\n   state-input weights: {},\\n  and biases: {}'.format(\n",
    "    hidden_layer.weights[0].shape, hidden_layer.weights[1].shape, \n",
    "    hidden_layer.weights[2].shape))\n",
    "print('Note that input and state-input weights have H (10) each for:\\n  --input,\\n  --forget,\\n  --memory, and\\n  --output gates')\n",
    "\n",
    "embedding_layer = modelSimple.layers[-5]\n",
    "print('Shapes of embedding layer weights: {}'.format(\n",
    "    embedding_layer.weights[0].shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'kerasUtilities'></a>\n",
    "[Return to Top](#top)\n",
    "# B. Keras utilities \n",
    "We'll introduce __StringLookup__ for vocabulary creation and __Dataset__ for training / label pre-processing.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/drewplant/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example of pre-standardized sentence:\n",
      "  ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n",
      "\n",
      "\n",
      "and after standardization:\n",
      "  ['<s>', 'the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of', \"atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.', '<s>']\n"
     ]
    }
   ],
   "source": [
    "# Load \"brown\" dataset\n",
    "from importlib import reload\n",
    "reload(utils)\n",
    "name = \"brown\"\n",
    "import nltk\n",
    "assert(nltk.download(name))\n",
    "corpus = nltk.corpus.__getattr__(name)\n",
    "sentences = list(corpus.sents())\n",
    "\n",
    "# Standardize words -- lower-case characters, convert numbers to a standard code, etc.\n",
    "# Also insert a '<s> character at the beginning and end of every sentence.'\n",
    "canonsentences = np.array([['<s>'] + [utils.canonicalize_word(word) for word in sentence] + ['<s>'] for sentence in sentences ])\n",
    "print('An example of pre-standardized sentence:\\n  {}'.format(sentences[0]))\n",
    "print('\\n\\nand after standardization:\\n  {}'.format(canonsentences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras PreProcessing:  StringLookup\n",
    "\n",
    "If you're using your own input data-set in order to train a custom embedding layer, you will need a way to convert words to ids for the embedding-layer lookup operation.  For this purpose, Keras provides a convenient utility to create an input vocabulary as well as a lookup dictionary, using the StringLookup object.  Refer to Keras documentation [here.](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/StringLookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of brown corpus is 57340 sentences\n",
      "Length of words in brown corpus is 1275872\n"
     ]
    }
   ],
   "source": [
    "# Size of corpus\n",
    "print('Length of brown corpus is {} sentences'.format(len(canonsentences)))\n",
    "\n",
    "# Convert to single dimension of words\n",
    "canonwords = [ word for sentence in canonsentences for word in sentence]\n",
    "print('Length of words in brown corpus is {}'.format(len(canonwords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted vocabulary length is 10000\n"
     ]
    }
   ],
   "source": [
    "# Create the string lookup object using the 10000 most-popular words\n",
    "words_to_ids = StringLookup(max_tokens = 10000)\n",
    "\n",
    "# Process the input corpus words, creating a vocabulary / id lookup:\n",
    "words_to_ids.adapt(canonwords)\n",
    "\n",
    "# Get vocabulary size\n",
    "V = len(words_to_ids.get_vocabulary())\n",
    "print('Extracted vocabulary length is {}'.format(V))\n",
    "\n",
    "# Also create an object to convert from ids back to words from the same vocabulary:\n",
    "ids_to_words = StringLookup(vocabulary=words_to_ids.get_vocabulary(), invert=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Dataset input utility\n",
    "\n",
    "To make life easier Keras offers a [Dataset](https://www.tensorflow.org/guide/data) interface to easily wrangle input and label data for feeding and training models (not just for an LSTM model but any Keras model.)\n",
    "\n",
    "Let's use this to create an input dataset for training and evaluating our RNN model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training / test sets of word ids \n",
    "corpus_ids = words_to_ids(canonwords).numpy()\n",
    "\n",
    "# Split into train (80%) dev (10%) test (10%)\n",
    "train_ids, dev_test_ids = train_test_split(corpus_ids, train_size=0.8, random_state=42, shuffle=False)\n",
    "\n",
    "dev_ids, test_ids = train_test_split(dev_test_ids, train_size=0.5, random_state=42, shuffle=False)\n",
    "\n",
    "x_ids_train = train_ids[:-1]\n",
    "y_ids_train = train_ids[1:]\n",
    "\n",
    "# inputs of length max_time words\n",
    "max_time = 25   # length of words per sequence\n",
    "buffer_size = 100\n",
    "batch_size = 100\n",
    "\n",
    "ids_labels_dataset = tf.data.Dataset.from_tensor_slices((x_ids_train, y_ids_train))\n",
    "# examples_per_epoch = len(corpus_ids)//(max_time+1)\n",
    "\n",
    "# Create a train sequence dimension for words.  \n",
    "sequences_train = ids_labels_dataset.batch(max_time, drop_remainder=True).shuffle(buffer_size).batch(\n",
    "    batch_size, drop_remainder=True)\n",
    "\n",
    "# Create a dataset for validating during fit\n",
    "x_dev = dev_ids[:-1]\n",
    "y_dev = dev_ids[1:]\n",
    "ids_labels_validation = tf.data.Dataset.from_tensor_slices((x_dev, y_dev))\n",
    "sequences_val = ids_labels_validation.batch(max_time, drop_remainder=True).shuffle(buffer_size).batch(\n",
    "    batch_size, drop_remainder=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs of length max_time words\n",
    "max_time = 25   # length of words per sequence\n",
    "batch_size = 100\n",
    "\n",
    "# Create training / test sets of word ids without using dataset\n",
    "corpus_ids = words_to_ids(canonwords).numpy()\n",
    "\n",
    "# Truncate to have even number of sequences\n",
    "trunc_id = (( len(corpus_ids)-1 ) // (max_time * batch_size)) * (max_time * batch_size)\n",
    "corpus_x_ids = corpus_ids[:trunc_id]\n",
    "corpus_y_ids = corpus_ids[1:trunc_id+1]\n",
    "\n",
    "# Add in word_length dimension\n",
    "corpus_x_ids = corpus_x_ids.reshape([-1, max_time])\n",
    "corpus_y_ids = corpus_y_ids.reshape([-1, max_time])\n",
    "\n",
    "# Split into train (80%) dev (10%) test (10%)\n",
    "train_x, test_x, train_y, test_y = train_test_split(corpus_x_ids, corpus_y_ids,  train_size=0.8, random_state=42, shuffle=True)\n",
    "\n",
    "# dev_ids, test_ids = train_test_split(dev_test_ids, train_size=0.5, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'createTensorboardTrain'></a> \n",
    "[Top](#top)\n",
    "# C. Create the model.  Setup tensorboard.  Train the model.\n",
    "Let's build a new RNN model and fit to our larger Brown corpus dataset\n",
    "\n",
    "You can use [Tensorboard](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard) for visualizing your model's structure, as well as viewing training and validation accuracy as your training fit progresses.\n",
    "\n",
    "To create tensorboard data, simply create a callback and include the callback inside the call to fit() as shown below.  Then run the tensorboard command in a terminal window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. This one works\n",
    "# The LSTM layer provides two arguments:\n",
    "#   return_state (which returns lstm_state, lstm_last_time_state, cell_state)\n",
    "#   return_sequence (which ensures that the 'lstm_state' returned object is the vector output\n",
    "#   for all time positions in the sequence.)\n",
    "#\n",
    "#   Note that for the case (return_sequence = False, return_state = True) lstm_state and lstm_last_time_state\n",
    "#   are the same tensor.\n",
    "#\n",
    "# Here is a good article illustrating the two options:\n",
    "#    https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/\n",
    "\n",
    "\n",
    "# Let's build a model class to instantiate our model \n",
    "# ...and more closely control training / inference behavior.\n",
    "class MyModel(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units, hidden_activation, hidden_initializer,\n",
    "                batchnorm = True):\n",
    "        super().__init__(self)\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        # self.rnn = keras.layers.GRU(rnn_units, return_sequences = True, return_state=True, \n",
    "        #                              activation = hidden_activation, kernel_initializer = hidden_initializer, \n",
    "        #                              stateful=False)\n",
    "        \n",
    "        self.rnn = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True,\n",
    "                                        activation = hidden_activation, \n",
    "                                        kernel_initializer = hidden_initializer)\n",
    "        \n",
    "        self.norm = tf.keras.layers.BatchNormalization()\n",
    "        self.do_batchnorm = batchnorm\n",
    "    \n",
    "        # tf.keras.layers.GRU(rnn_units,\n",
    "        #                                return_sequences=True, \n",
    "        #                                return_state=True)\n",
    "        self.dense = keras.layers.Dense(vocab_size)\n",
    " \n",
    "    # You must set return_sequences=True when stacking LSTM layers so that the second LSTM layer has a three-dimensional sequence input.\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.rnn.get_initial_state(x)\n",
    "            \n",
    "        # In the following expression the return values are:  \n",
    "        #        x = the sequence of outputs from the layer, \n",
    "        #   states = the final state vector (at the last time-step)\n",
    "        #        _ = the cell memory at the final step\n",
    "        x, state_h, state_c = self.rnn(x, initial_state=states, training=training)\n",
    "        if self.do_batchnorm:\n",
    "            x = self.norm(x)\n",
    "        \n",
    "        # Output layer outputs logits rather than softmax as we didn't specify any activation\n",
    "        x = self.dense(x, training=training)\n",
    "        \n",
    "        if return_state:\n",
    "            return x, [state_h, state_c]\n",
    "        else: \n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Trying this one with multiple hidden layers\n",
    "# The LSTM layer provides two arguments:\n",
    "#   return_state (which returns lstm_state, lstm_last_time_state, cell_state)\n",
    "#   return_sequence (which ensures that the 'lstm_state' returned object is the vector output\n",
    "#   for all time positions in the sequence.)\n",
    "#\n",
    "#   Note that for the case (return_sequence = False, return_state = True) lstm_state and lstm_last_time_state\n",
    "#   are the same tensor.\n",
    "#\n",
    "# Here is a good article illustrating the two options:\n",
    "#    https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/\n",
    "\n",
    "\n",
    "# Let's build a model class to instantiate our model \n",
    "# ...and more closely control training / inference behavior.\n",
    "class MyModel(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_layers, rnn_units, hidden_activation, dropout_rate,\n",
    "                 hidden_initializer, batchnorm = True):\n",
    "        super().__init__(self)\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        # self.rnn = keras.layers.GRU(rnn_units, return_sequences = True, return_state=True, \n",
    "        #                              activation = hidden_activation, kernel_initializer = hidden_initializer, \n",
    "        #                              stateful=False)\n",
    "        self.n_layers = n_layers\n",
    "        self.rnn = []\n",
    "        self.norm = []\n",
    "        self.dropout = []\n",
    "        for i in range(n_layers):\n",
    "            self.rnn.append(tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True, \n",
    "                                                 activation = hidden_activation,\n",
    "                                                 kernel_initializer = hidden_initializer))\n",
    "            self.norm.append(tf.keras.layers.BatchNormalization())\n",
    "            self.dropout.append(tf.keras.layers.Dropout(dropout_rate))\n",
    "            \n",
    "        # self.rnn = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True,\n",
    "        #                                 activation = hidden_activation, \n",
    "        #                                 kernel_initializer = hidden_initializer)\n",
    "        \n",
    "        self.do_batchnorm = batchnorm\n",
    "    \n",
    "        # tf.keras.layers.GRU(rnn_units,\n",
    "        #                                return_sequences=True, \n",
    "        #                                return_state=True)\n",
    "        self.dense = keras.layers.Dense(vocab_size)\n",
    " \n",
    "    # You must set return_sequences=True when stacking LSTM layers so that the second LSTM layer has a three-dimensional sequence input.\n",
    "\n",
    "    def call(self, inputs, passin_states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "            \n",
    "        # In the following expression the return values are:  \n",
    "        #         x = the sequence of outputs from the layer, \n",
    "        #   state_h = the final state vector (at the last time-step)\n",
    "        #   state_c = the cell memory at the final step\n",
    "        states = []\n",
    "        for i in range(self.n_layers):\n",
    "            if passin_states is None:\n",
    "                statesi = self.rnn[i].get_initial_state(x)\n",
    "            else:\n",
    "                statesi = passin_states[i]\n",
    "            x, state_h, state_c = self.rnn[i](x, initial_state=statesi, training=training)\n",
    "            # x, state_h, state_c = self.rnn(x, initial_state=states, training=training)\n",
    "            x = self.dropout[i](x)\n",
    "            if self.do_batchnorm:\n",
    "                x = self.norm[i](x)\n",
    "            states.append((state_h, state_c))\n",
    "        \n",
    "        # Output layer outputs logits rather than softmax as we didn't specify any activation\n",
    "        x = self.dense(x, training=training)\n",
    "        \n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else: \n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = 10000\n",
    "\n",
    "# The embedding dimension\n",
    "# embedding_dim = 256\n",
    "embedding_dim = 50\n",
    "\n",
    "# Number of hidden layers\n",
    "n_layers = 2\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 100\n",
    "\n",
    "hidden_activation = 'relu'\n",
    "\n",
    "hidden_initializer = 'he_uniform'\n",
    "\n",
    "# Dropout\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Create model instance\n",
    "model = MyModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    n_layers = n_layers,\n",
    "    rnn_units=rnn_units,\n",
    "    hidden_activation = hidden_activation, \n",
    "    hidden_initializer = hidden_initializer,\n",
    "    dropout_rate = dropout_rate,\n",
    "    batchnorm = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 25, 10000) # (batch_size, sequence_length, vocab_size)\n",
      "(100, 25) (100, 25)\n",
      "Model: \"my_model_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     multiple                  500000    \n",
      "_________________________________________________________________\n",
      "lstm_24 (LSTM)               multiple                  60400     \n",
      "_________________________________________________________________\n",
      "lstm_25 (LSTM)               multiple                  80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc multiple                  400       \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc multiple                  400       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             multiple                  1010000   \n",
      "=================================================================\n",
      "Total params: 1,651,600\n",
      "Trainable params: 1,651,200\n",
      "Non-trainable params: 400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get a feel for looking at training samples in our input Dataset\n",
    "for input_example_batch, target_example_batch in sequences_train.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "    print(input_example_batch.shape, target_example_batch.shape)\n",
    "\n",
    "# Print out a model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of example batch loss: ()\n",
      "Prediction shape:  (100, 25, 10000)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         9.210451\n",
      "20.0\n",
      "10001.107\n"
     ]
    }
   ],
   "source": [
    "# See the behavior of loss function, how to take mean loss over batch\n",
    "# We will use \"from_logits\" = True since our outputs are logits rather than softmax (ie, [batch,seq_len,V])\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# We can calculate an example loss using eager execution.\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print('Shape of example batch loss: {}'.format(example_batch_loss.numpy().shape))\n",
    "mean_loss = example_batch_loss.numpy().mean()\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", mean_loss)\n",
    "\n",
    "# We know that tensorflow uses natural log (base e) for crossentropy calculation \n",
    "checkbase = loss(np.array([[[0]]]), np.array([[[0., 20, 0.]]]))\n",
    "print(checkbase.numpy())\n",
    "# Confirm that exp(mean loss ~ V)  (why?)\n",
    "# If initialization is good, each q ~ 1 / V => p ln(V) = ln(V) -> exp(ln(V)) = V !!\n",
    "# ln(x) = ln(2^log2(x)) = log2(x) * ln(2) => \n",
    "print(tf.exp(mean_loss).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(optimizer='adam', loss=loss, metrics = ['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "408/408 [==============================] - 115s 281ms/step - loss: 6.0542 - sparse_categorical_accuracy: 0.1977 - val_loss: 5.5943 - val_sparse_categorical_accuracy: 0.2192\n",
      "Epoch 2/20\n",
      "408/408 [==============================] - 116s 283ms/step - loss: 5.0654 - sparse_categorical_accuracy: 0.2446 - val_loss: 4.6434 - val_sparse_categorical_accuracy: 0.2700\n",
      "Epoch 3/20\n",
      "408/408 [==============================] - 115s 282ms/step - loss: 4.9117 - sparse_categorical_accuracy: 0.2536 - val_loss: 4.5738 - val_sparse_categorical_accuracy: 0.2741\n",
      "Epoch 4/20\n",
      "408/408 [==============================] - 115s 282ms/step - loss: 4.8125 - sparse_categorical_accuracy: 0.2597 - val_loss: 4.5449 - val_sparse_categorical_accuracy: 0.2758\n",
      "Epoch 5/20\n",
      "408/408 [==============================] - 115s 282ms/step - loss: 4.7388 - sparse_categorical_accuracy: 0.2638 - val_loss: 4.5297 - val_sparse_categorical_accuracy: 0.2771\n",
      "Epoch 6/20\n",
      "408/408 [==============================] - 115s 281ms/step - loss: 4.6801 - sparse_categorical_accuracy: 0.2670 - val_loss: 4.5322 - val_sparse_categorical_accuracy: 0.2778\n",
      "Epoch 7/20\n",
      "408/408 [==============================] - 115s 281ms/step - loss: 4.6313 - sparse_categorical_accuracy: 0.2694 - val_loss: 4.5453 - val_sparse_categorical_accuracy: 0.2773\n",
      "Epoch 8/20\n",
      "408/408 [==============================] - 116s 284ms/step - loss: 4.5889 - sparse_categorical_accuracy: 0.2710 - val_loss: 4.5492 - val_sparse_categorical_accuracy: 0.2783\n",
      "Epoch 9/20\n",
      "408/408 [==============================] - 115s 281ms/step - loss: 4.5524 - sparse_categorical_accuracy: 0.2728 - val_loss: 4.5631 - val_sparse_categorical_accuracy: 0.2786\n",
      "Epoch 10/20\n",
      "408/408 [==============================] - 115s 282ms/step - loss: 4.5199 - sparse_categorical_accuracy: 0.2746 - val_loss: 4.5724 - val_sparse_categorical_accuracy: 0.2784\n",
      "Epoch 11/20\n",
      "408/408 [==============================] - 115s 283ms/step - loss: 4.4905 - sparse_categorical_accuracy: 0.2756 - val_loss: 4.5905 - val_sparse_categorical_accuracy: 0.2784\n",
      "Epoch 12/20\n",
      "408/408 [==============================] - 115s 282ms/step - loss: 4.4634 - sparse_categorical_accuracy: 0.2767 - val_loss: 4.6057 - val_sparse_categorical_accuracy: 0.2789\n",
      "Epoch 13/20\n",
      "408/408 [==============================] - 115s 282ms/step - loss: 4.4387 - sparse_categorical_accuracy: 0.2777 - val_loss: 4.6174 - val_sparse_categorical_accuracy: 0.2783\n",
      "Epoch 14/20\n",
      "408/408 [==============================] - 115s 281ms/step - loss: 4.4171 - sparse_categorical_accuracy: 0.2788 - val_loss: 4.6373 - val_sparse_categorical_accuracy: 0.2777\n",
      "Epoch 15/20\n",
      "408/408 [==============================] - 2513s 6s/step - loss: 4.3969 - sparse_categorical_accuracy: 0.2797 - val_loss: 4.6572 - val_sparse_categorical_accuracy: 0.2779\n",
      "Epoch 16/20\n",
      "408/408 [==============================] - 20540s 50s/step - loss: 4.3784 - sparse_categorical_accuracy: 0.2806 - val_loss: 4.6669 - val_sparse_categorical_accuracy: 0.2783\n",
      "Epoch 17/20\n",
      "408/408 [==============================] - 309s 757ms/step - loss: 4.3606 - sparse_categorical_accuracy: 0.2811 - val_loss: 4.6981 - val_sparse_categorical_accuracy: 0.2775\n",
      "Epoch 18/20\n",
      "408/408 [==============================] - 188s 462ms/step - loss: 4.3437 - sparse_categorical_accuracy: 0.2819 - val_loss: 4.7074 - val_sparse_categorical_accuracy: 0.2773\n",
      "Epoch 19/20\n",
      "408/408 [==============================] - 117s 287ms/step - loss: 4.3295 - sparse_categorical_accuracy: 0.2824 - val_loss: 4.7199 - val_sparse_categorical_accuracy: 0.2776\n",
      "Epoch 20/20\n",
      "408/408 [==============================] - 117s 287ms/step - loss: 4.3156 - sparse_categorical_accuracy: 0.2827 - val_loss: 4.7380 - val_sparse_categorical_accuracy: 0.2776\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "EPOCHS = 20\n",
    "history = model.fit(sequences_train, \n",
    "                    validation_data = sequences_val, epochs=EPOCHS, \n",
    "                    callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Generate text\n",
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, ids_to_words, words_to_ids, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature=temperature\n",
    "    self.model = model\n",
    "    self.ids_to_words = ids_to_words\n",
    "    self.words_to_ids = words_to_ids\n",
    "\n",
    "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
    "    skip_ids = self.words_to_ids(['','[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices = skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(words_to_ids.get_vocabulary())]) \n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  # @tf.function\n",
    "  def generate_one_step(self, input_words, passin_states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    # input_words = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    # input_ids = self.words_to_ids(input_words).to_tensor()\n",
    "    input_words = tf.strings.split(input_words)\n",
    "    input_ids = self.words_to_ids(input_words.to_tensor())\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, word, next_word_logits] \n",
    "    predicted_logits, states =  self.model(inputs=input_ids, passin_states=passin_states, \n",
    "                                          return_state=True)\n",
    "    # Only use the prediction in the final time-position.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to words\n",
    "    predicted_words = self.ids_to_words(predicted_ids)\n",
    "\n",
    "    # Return the words and model state.\n",
    "    return predicted_words, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated language:\n",
      "hello, my name is shown only and what you had imagine those '' . <s> <s> mr. brown was on the state owned , the rows of scanned out out on the porch , and laughed what he had been talking comparable some while he moved back back to the halt . <s> <s> he had better at the moment , to the playing one day show in daytime and they'll repeat him ) down the ball . <s> <s> `` don't have before his own enough '' ? ? <s> <s> another deputies and his women , colonel van miller at a relatively \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 0.6813898086547852\n"
     ]
    }
   ],
   "source": [
    "# Generate an inference instance\n",
    "one_step_model = OneStep(model, ids_to_words, words_to_ids)\n",
    "\n",
    "# \n",
    "start = time.time()\n",
    "states = None\n",
    "# next_word = tf.constant(['hello, my name is'])\n",
    "next_word = np.array(['hello, my name is'])\n",
    "# next_word = tf.constant([['hello, my name is'],['hello', 'my', 'name', 'is']])\n",
    "result = [next_word]\n",
    "\n",
    "for n in range(100):\n",
    "    next_word, states = one_step_model.generate_one_step(next_word, passin_states=states)\n",
    "    result.append(next_word)\n",
    "\n",
    "result = tf.strings.join(result, separator=' ')\n",
    "end = time.time()\n",
    "\n",
    "print('Generated language:')\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "\n",
    "print(f\"\\nRun time: {end - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'endOfNotebook'></a>\n",
    "_End of Notebook_  \n",
    "[Back to top](#top)  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
