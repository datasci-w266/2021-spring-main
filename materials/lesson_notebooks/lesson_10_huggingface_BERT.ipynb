{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Notebook - Lesson 10\n",
    "\n",
    "Today, we look more at the Bert implementation from Hugging Face (https://huggingface.co/). It is at this point probably the most convenient way to obtain pre-trained transformer models, and they are available in both TensorFlow and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the sample problem of identifying a context-based embedding of the word **'bank'** in the sentence **\"I deposited 12342 dollars in the bank.\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"I deposited 12342 dollars in the bank.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by tokenizing the sentence. Bert has its own **tokenizer** and it should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'deposited', '123', '##42', 'dollars', 'in', 'the', 'bank', '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(test_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the long number got split. The prefix '##' indicates that the token is a continuation of the previous one. One needs to have an eye on that for many reasons, particularly when one wants to identify the proper Bert output vector for a token.\n",
    "\n",
    "Now, let's do a short excercise to get familiar with BERT. BERT is supposed to get us *context-based embeddings*, i.e. embeddings for the same word in different contexts should be different. Let's give that a try!\n",
    "\n",
    "\n",
    "**Exercise:** compare the context-based embedding vectors for '*bank*' in the following 4 sentences:\n",
    "\n",
    "* \"I need to bring my money to the bank today\" \n",
    "* \"I will need to bring my money to the bank tomorrow\" \n",
    "* \"I had to bank into a turn\"\n",
    "* \"The bank teller was very nice\" \n",
    "\n",
    "\n",
    "\n",
    "We first need to tokenize the input, which is very easy with the latest Huggingface tokenizers (note the easy padding option!): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(4, 13), dtype=int32, numpy=\n",
       "array([[ 101,  146, 1444, 1106, 2498, 1139, 1948, 1106, 1103, 3085, 2052,\n",
       "         102,    0],\n",
       "       [ 101,  146, 1209, 1444, 1106, 2498, 1139, 1948, 1106, 1103, 3085,\n",
       "        4911,  102],\n",
       "       [ 101,  146, 1125, 1106, 3085, 1154,  170, 1885,  102,    0,    0,\n",
       "           0,    0],\n",
       "       [ 101, 1109, 3085, 1587, 1200, 1108, 1304, 3505,  102,    0,    0,\n",
       "           0,    0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(4, 13), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(4, 13), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_inputs = tokenizer([\"I need to bring my money to the bank today\",\n",
    "                    \"I will need to bring my money to the bank tomorrow\",\n",
    "                    \"I had to bank into a turn\",\n",
    "                    \"The bank teller was very nice\" ],\n",
    "                  padding=True,\n",
    "                  return_tensors='tf')\n",
    "\n",
    "bert_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are actually three outputs: the token ids (starting with '101' for the '[CLS]' token), the token_type_ids which are usefull when one has distinct segments, and the attention masks which are used to mask out padding tokens.\n",
    "\n",
    "Next, define a **Keras layer from the pre-trained BERT model** from Hugging Face. It's this simple!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert_layer = TFBertModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. So now let's get the BERT encoding of our test sentences. We just follow the Functional API approach: \n",
    "\n",
    "layer_output = layer(layer_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of first output: \t\t (4, 13, 768)\n",
      "shape of second output: \t (4, 768)\n"
     ]
    }
   ],
   "source": [
    "bert_outputs = bert_layer(bert_inputs)\n",
    "\n",
    "print('shape of first output: \\t\\t', bert_outputs[0].shape)\n",
    "print('shape of second output: \\t', bert_outputs[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**: \n",
    "\n",
    "1) Hmm... there are two outputs. One with dims [4, 13, 768], and one with [4, 768]. What are these? Which one do we need? And what do these dimensions correspond to?\n",
    "\n",
    "2) How do we get the context-based embedding for the word 'bank'?\n",
    "\n",
    "Let's start by defining a function that shows the respective cosine distances between a list of vectors, We'll use this in a bit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distances(vecs):\n",
    "    for v_1 in vecs:\n",
    "        distances = ''\n",
    "        for v_2 in vecs:\n",
    "            distances += ('\\t' + str(np.dot(v_1, v_2)/np.sqrt(np.dot(v_1, v_1) * np.dot(v_2, v_2)))[:4])\n",
    "        print(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's the first BERT output that we need, as that one gets us the token-level embeddings. \n",
    "\n",
    "Now, we get the vectors in the most pedestrian way by simply finding the 'bank'-token positions in the *encoded* input and extract the proper components: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_1 = bert_outputs[0][0, 9]\n",
    "bank_2 = bert_outputs[0][1, 10]\n",
    "bank_3 = bert_outputs[0][2, 4]\n",
    "bank_4 = bert_outputs[0][3, 2]\n",
    "\n",
    "banks = [bank_1, bank_2, bank_3, bank_4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Let's now get the pair-wise cosine distances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0\t0.99\t0.59\t0.86\n",
      "\t0.99\t1.0\t0.59\t0.87\n",
      "\t0.59\t0.59\t1.0\t0.62\n",
      "\t0.86\t0.87\t0.62\t1.0\n"
     ]
    }
   ],
   "source": [
    "cosine_distances(banks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks rights! The 'bank' in 'I had to bank into a turn' is the one that's most different from the others.\n",
    "\n",
    "Also, note that 'bank' has a slightly different embedding in the two sentences \"*I need to bring my money to the bank today*\" and \"*I will need to bring my money to the bank tomorrow*\". Maybe a bit surprising, but the sentences are slightly different, so the self-attention calculations will be slightly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_nb",
   "language": "python",
   "name": "bert_nb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
