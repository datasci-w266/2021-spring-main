{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions in TensorFlow\n",
    "\n",
    "Training neural networks requires the specification of a loss function. The gradient descent algorithm attempts to minimize the loss during training and the choice of loss function is a key part of the training of a deep learning model.\n",
    "\n",
    "In this notebook we demonstrate how to use different loss functions for training. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we use the Keras API for TensorFlow and import the keras module. The submodule `keras.losses` contains many loss functions that can be used in training the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the notebook is to focus on the loss function (and not really the problem at hand) and so we pick what might be considered the \"Hello World\" problem of machine learning, the MNIST handwritten digit classification problem.\n",
    "\n",
    "To get the dataset you can follow the guides [here](https://www.tensorflow.org/datasets/overview) and also find the overview of MNIST [here](https://www.tensorflow.org/datasets/catalog/mnist)\n",
    "\n",
    "Recall that every datapoint in the MNIST dataset is a 28x28 image of a hand written digit. The array that represents this image is flattened into a 784 length array and is the input to the neural network (given below as the Input layer in Keras). We create two fully-connected (dense) hidden layers and we have a final output layer of size 10 (one for each digit) using softmax to make predictions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling the model is the next step and at this point we provide both the optimizer and the loss function to optimize. the loss function in this case is SparseCategoricalCrossentropy. Let's break this down:\n",
    "- Crossentropy means that we are using the cross entropy between predicted values (probabilities in the softmax layer) and the true labels.\n",
    "- The categorical tells us that this is a categorical data as output (classificaction problem with more than 2 labels)\n",
    "- The Sparse tells us that the class labels are provided as integer labels (as opposed to being 1-hot encoded).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.adam,  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also provide the loss function as a string. Typically by replacing the CamelCase class with the more traditional python like (snake case) lowercase and underscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of built-in loss functions in the Keras API in Tensorflow, some of these are listed below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many built-in loss function are available\n",
    "\n",
    "- 'MeanSquaredError()'\n",
    "- 'MeanSquaredLogarithmicError()'\n",
    "- `MeanAbsoluteError()`\n",
    "- `BinaryCrossentropy()`\n",
    "- `Hinge()`\n",
    "- `SquaredHinge()`\n",
    "- `MeanAbsoluteError()`\n",
    "- `KLDivergence()`\n",
    "- `CosineSimilarity()`\n",
    "\n",
    "The full list is [here](https://keras.io/api/losses/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More example of loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error loss function\n",
    "Computes the mean of squares of errors between labels and predictions.\n",
    " \n",
    "$$loss = \\frac{1}{n}\\sum_{i=1}^n (y_{true,i} - y_{pred,i})^2$$\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.adam,  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.MeanSquaredError()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Error loss function\n",
    "\n",
    "Computes the mean of absolute difference between labels and predictions.\n",
    "\n",
    "$$loss = \\frac{1}{n}\\sum_{i=1}^n |y_{true,i} - y_{pred,i}|$$\n",
    "\n",
    "Usage with the compile() API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.adam,  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.MeanAbsoluteError()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Custom Loss functions\n",
    "\n",
    "You can also invent or create your own loss function. A loss function takes two arguments, the true values (y_true) and the predicted values (y_pred) and returns a number. For example the loss function below replicates the behavior of the `MeanSquaredError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_mean_squared_error(y_true, y_pred):\n",
    "    return tf.math.reduce_mean(tf.square(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_uncompiled_model()\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=custom_mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to one-hot encode the labels to use MSE\n",
    "y_train_one_hot = tf.one_hot(y_train, depth=10)\n",
    "model.fit(x_train, y_train_one_hot, batch_size=64, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional parameters to loss functions\n",
    "\n",
    "You may have wondered why in the above loss functions you are initializing a class, e.g., `MeanSquareError()`. The answer is that this class can accept additional arguments and allow you to build more sophisticated loss functions, or more flexible loss functions, without duplication of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need a loss function that takes in parameters beside `y_true` and `y_pred`, you\n",
    "can subclass the `tf.keras.losses.Loss` class and implement the following two methods:\n",
    "\n",
    "- `__init__(self)`: accept parameters to pass during the call of your loss function\n",
    "- `call(self, y_true, y_pred)`: use the targets (y_true) and the model predictions\n",
    "(y_pred) to compute the model's loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the example we will add a term to the loss function that will de-incentivize prediction values far from 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMSE(keras.losses.Loss):\n",
    "    def __init__(self, regularization_factor=0.1, name=\"custom_mse\"):\n",
    "        super().__init__(name=name)\n",
    "        self.regularization_factor = regularization_factor\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        mse = tf.math.reduce_mean(tf.square(y_true - y_pred))\n",
    "        reg = tf.math.reduce_mean(tf.square(0.5 - y_pred))\n",
    "        return mse + reg * self.regularization_factor\n",
    "\n",
    "\n",
    "model = get_uncompiled_model()\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=CustomMSE())\n",
    "\n",
    "y_train_one_hot = tf.one_hot(y_train, depth=10)\n",
    "model.fit(x_train, y_train_one_hot, batch_size=64, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. [Training and evaluation with the built-in methods](https://www.tensorflow.org/guide/keras/train_and_evaluate)\n",
    "1. [Keras loss functions list and API](https://keras.io/api/losses/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
