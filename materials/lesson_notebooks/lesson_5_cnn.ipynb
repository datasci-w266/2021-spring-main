{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live Notes for Lesson 5: CNNs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.data import find\n",
    "\n",
    "import gensim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /home/mhbutler/anaconda3/lib/python3.8/site-packages (3.8.3)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/mhbutler/anaconda3/lib/python3.8/site-packages (from gensim) (1.18.5)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/mhbutler/anaconda3/lib/python3.8/site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/mhbutler/anaconda3/lib/python3.8/site-packages (from gensim) (4.1.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/mhbutler/anaconda3/lib/python3.8/site-packages (from gensim) (1.4.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/mhbutler/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simple CNNs Math with numpy \n",
    "\n",
    "Let's start by defining simple 1D convolution operation, realizing the formulas from the slides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution(w, x):\n",
    "    \"\"\"\n",
    "    Most quick and basic operation, valuing clarity over compactness.   \n",
    "    \"\"\"\n",
    "\n",
    "    if len(w.shape) == 1:                              # if input is 1d ...\n",
    "        return w.dot(x)\n",
    "    \n",
    "    elif len(w.shape) == 2:                            # if input is 2d ...\n",
    "        return np.trace(np.matmul(w, np.transpose(x)))\n",
    "    \n",
    "    else:\n",
    "        raise ValueError('you have to define the proper formula')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, choose weights and bias for our (one) convolution layer to mimic the edge detector in the slides: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolution_weights = np.array([-1, 2, -1])\n",
    "convolution_bias = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following input and then calculate the convolution. Do we locater the edges/transitions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vals = np.array([0,0,0,1,1,1,0,0,0,1,1,1,1,1,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 1 0 0 0 1 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "convolution_shape = convolution_weights.shape \n",
    "input_format = input_vals.shape \n",
    "\n",
    "convolution_results = []\n",
    "\n",
    "# slide window across input\n",
    "for i in range(1 + input_format[-1] - convolution_shape[-1]):\n",
    "    \n",
    "    # convolution results at position i\n",
    "    conv_val_i = convolution(convolution_weights,input_vals[i:i + convolution_shape[0]]) + convolution_bias\n",
    "    \n",
    "    # apply poor-man's relu\n",
    "    if conv_val_i < 0:\n",
    "        conv_val_i = 0\n",
    "    convolution_results.append(conv_val_i)\n",
    "    \n",
    "print(np.array(convolution_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks right! We don't use any padding ('valid' padding) hence we miss the first and last positions. \n",
    "\n",
    "**Question:** how would the dimension change with larger stride size? \n",
    "\n",
    "Next, imagine that our input has 2 dimensions. (This could for example represent an array of word embeddings or the output of a previous CNN layer with multiple filters.)\n",
    "\n",
    "So we define 2-d input and choose arbitrary weights to illustrate the point. (These parameters represent the values of our example in the last part of the slides.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolution_weights = np.array([[1, 3, -1], [2,1,-1]])\n",
    "convolution_bias = 0\n",
    "\n",
    "input_vals = np.transpose(np.array([[-3,-5], [-2,2],[3,1],[4,3], [-1,-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3, -2,  3,  4, -1],\n",
       "       [-5,  2,  1,  3, -1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  3, -1],\n",
       "       [ 2,  1, -1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolution_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.         0.9999092  1.       ]\n"
     ]
    }
   ],
   "source": [
    "convolution_results = []\n",
    "\n",
    "input_format = input_vals.shape \n",
    "convolution_shape = convolution_weights.shape \n",
    "\n",
    "# slide window across input\n",
    "for i in range(1 + input_format[-1] - convolution_shape[-1]):\n",
    "    \n",
    "    # convolution results at position i\n",
    "    conv_val_i = convolution(convolution_weights,\n",
    "                                   input_vals[:,i:i + convolution_shape[-1]]) + convolution_bias\n",
    "    \n",
    "    # let's  choose tanh for the activation \n",
    "    conv_val_i = np.tanh(conv_val_i)\n",
    "    convolution_results.append(conv_val_i)\n",
    "    \n",
    "print(np.array(convolution_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the dimensions look right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Simple CNN Classification using Word Embeddings in Keras\n",
    "\n",
    "Here we apply CNN classification to our **toy** example of the lesson notebook for the previous session. I.e., we use the same gensim/word-2-vec model, define the embedding matrix, and then create the embedding layer for our Keras model:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package word2vec_sample to\n",
      "[nltk_data]     /home/mhbutler/nltk_data...\n",
      "[nltk_data]   Package word2vec_sample is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('word2vec_sample')\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we know the number of words that have an embedding. Let's build the embedding matrix from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = len(model['university'])      # we know... it's 300\n",
    "\n",
    "# initialize embedding matrix and word-to-id map:\n",
    "embedding_matrix = np.zeros((len(model.vocab.keys()) + 1, EMBEDDING_DIM))       \n",
    "vocab_dict = {}\n",
    "\n",
    "# build the embedding matrix and the word-to-id map:\n",
    "for i, word in enumerate(model.vocab.keys()):\n",
    "    embedding_vector = model[word]\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        vocab_dict[word] = i\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 5  # Keras' embedding layer expects a specific input length. Padding is often needed here.\n",
    "\n",
    "embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                            embedding_matrix.shape[1],\n",
    "                            embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del tf_model\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the model (again as a **Sequential Model**). Now, we replace the concatination with a 1D CNN layer and a max-pooling operation. Let's choose 10 filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = tf.keras.Sequential()\n",
    "\n",
    "tf_model.add(embedding_layer)                                        # embedding layer\n",
    "\n",
    "tf_model.add(tf.keras.layers.Conv1D(\n",
    "    filters=10, \n",
    "    kernel_size=3, \n",
    "    strides=1, \n",
    "    padding='same', \n",
    "    activation='relu', \n",
    "    use_bias=True,\n",
    "    kernel_initializer='glorot_uniform', \n",
    "    bias_initializer='zeros')\n",
    "            )    \n",
    "\n",
    "tf_model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "\n",
    "\n",
    "tf_model.add(Dense(100, activation='relu'))                          # hidden layer\n",
    "tf_model.add(Dense(1, activation='sigmoid'))                         # classification layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at dimensions and parameters of the model.\n",
    "\n",
    "**Question**: does this make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 5, 300)            13194600  \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 5, 10)             9010      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               1100      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 13,204,811\n",
      "Trainable params: 10,211\n",
      "Non-trainable params: 13,194,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like last week... let's compile the model. I.e, define optimizer, loss function, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost there... let's the same crazy-simple fake training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = ['this is really absolutely great', 'this is really absolutely terrible']\n",
    "train_labels = [[1], [0]]\n",
    "\n",
    "test_sentences = ['never seen anything this stupid', 'never seen anything this fantastic']\n",
    "test_labels = [[0], [1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and then do some formatting gymnastics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sents_to_ids(sentences):\n",
    "    \"\"\"\n",
    "    converting a list of strings to a list of lists of word ids\n",
    "    \"\"\"\n",
    "    text_ids = []\n",
    "    for sentence in sentences:\n",
    "        example = []\n",
    "        for word in sentence.split(' '):\n",
    "            example.append(vocab_dict[word])\n",
    "        text_ids.append(example)\n",
    "\n",
    "    return  text_ids   \n",
    "\n",
    "\n",
    "train_input = np.array(sents_to_ids(train_sentences))\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "test_input = np.array(sents_to_ids(test_sentences))\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the model input are word ids in the vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[35029, 16908, 34554,  7427, 35058],\n",
       "       [35029, 16908, 34554,  7427, 37254]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next: let's get the start predictions. Should be random-ish. Are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5088843]\n",
      " [0.5097769]]\n",
      "[[0.5144767 ]\n",
      " [0.51471424]]\n"
     ]
    }
   ],
   "source": [
    "print(tf_model.predict(train_input))\n",
    "print(tf_model.predict(test_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, looks quite random.\n",
    "\n",
    "Finally... let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 166ms/step - loss: 0.6942 - val_loss: 0.6932\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0122 - val_loss: 0.1753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd3681073a0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_model.fit(train_input, train_labels, validation_data=(test_input, test_labels), epochs=1)\n",
    "tf_model.fit(train_input, train_labels, validation_data=(test_input, test_labels), epochs=150, verbose=0)\n",
    "tf_model.fit(train_input, train_labels, validation_data=(test_input, test_labels), epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look's good! Actually better than last week... but don't make much of that, given this crazy simple data set. \n",
    "\n",
    "What are train & test predictions now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1841006 ],\n",
       "       [0.86312157]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_model.predict(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yey! But we obviously cheated here with the choice of sentences. Nevertheless, the idea should be clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions for the class for joint live in-class exercises**:\n",
    "\n",
    "1) Can you relate the value for the validation loss to the prediction for the test set \n",
    "\n",
    "2) What do you think happens if you change the 'trainable' flag in the embedding layer from 'False' to 'True'?   \n",
    "\n",
    "3) What do you need to change in the model if you want more filters of the same kernel size?    \n",
    "\n",
    "**Note/Question:** What would you need to change if you wanted to add CNN layers (at the same position) of different kernel sizes? That gets us to Keras Functional API... "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
