{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side Notes for Lesson 6\n",
    "\n",
    "Note: not reviewed and/or edited. This is to be seen as a live quick-and-dirty toy example.\n",
    "\n",
    "Today, we look at a very simple RNN (LSTM) model in Keras. Since the purpose of the notebook is to familiarize ourselves with the various inputs, outputs, dimensions, etc., we don't even bother to train the model - we simply define it, study the parameters/options and look at the model output for a specific example. \n",
    "\n",
    "And hopefully, everything makes sense...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras.layers import Lambda, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.backend import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the model with the Functional API formalism in Keras.\n",
    "\n",
    "This is very useful as it allows us to construct non-sequential models with multiple inputs, multiple outputs, branched models,  etc..\n",
    "\n",
    "Key aspects:\n",
    "\n",
    "* start with definition of inputs (input sequence and initial state(s))\n",
    "* define the actions of layers as:\n",
    "$$\\rm layer\\_activations = layer(layer\\_parameters) (previous\\_layer\\_activations)$$  \n",
    "* define the model by specifying input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(): \n",
    "    \n",
    "    # inputs - comprised of - for each example in batch:\n",
    "    #             - (four) x_t, each of dim 3, \n",
    "    #.            - the initial 2 x 3d state (h and c) \n",
    "    \n",
    "    in_x = tf.keras.layers.Input(shape=(4,3), name=\"in_id\")\n",
    "\n",
    "    in_state_h = tf.keras.layers.Input(shape=(3,), name=\"in_state_h\")\n",
    "    in_state_c = tf.keras.layers.Input(shape=(3,), name=\"in_state_c\")\n",
    "    \n",
    "    # define a very simple lstem layer, ACTING on the input\n",
    "    \n",
    "    lstm_output = tf.keras.layers.LSTM(3, return_sequences=True, return_state=True,\n",
    "                              kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.1),\n",
    "                              recurrent_initializer=tf.keras.initializers.RandomNormal(stddev=0.1))\\\n",
    "            (in_x, initial_state=[in_state_h, in_state_c])\n",
    "\n",
    "    model = tf.keras.models.Model([in_x, in_state_h, in_state_c], lstm_output)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "* is '84' correct?\n",
    "* what would change if we set return_sequences=False?\n",
    "* what would happen if we set return_state=False? \n",
    "\n",
    "\n",
    "### Model Output\n",
    " \n",
    "Now we create some very simple sample input, the fake 'embeddings' of four words and the initial $h$ and $c$ defining the initial state. We then let the model 'predict' (i.e, calculate based on the random initializations as we didn;'t train the model) the output of the model for the input.\n",
    "\n",
    "Do the results make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_input = np.array([[[1.1,2,3], [4,5,6], [7,8,9], [10,11,12]]])\n",
    "initial_h = np.array([[1.,4,3]]*1)\n",
    "initial_c = np.array([[1,2,6]]*1)\n",
    "\n",
    "out, state_h, state_c = myModel.predict([lstm_input,initial_h, initial_c],batch_size=4)\n",
    "\n",
    "\n",
    "print('output_vector', out)\n",
    "print('out_state_h ', state_h)\n",
    "print('out_state_c ', state_c)\n",
    "\n",
    "\n",
    "\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "* if we wondered whether we confused out_state_h and out_state_c, how could we tell?\n",
    "* if we change the initial states, do we get different results? (Of course we do, but still good to see.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "berkeley_21",
   "language": "python",
   "name": "berkeley_21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
