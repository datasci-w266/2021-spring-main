{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'top'></a>\n",
    "\n",
    "# Notebook Contents\n",
    "  * A. [Introduction to RNN](#introToRnn) \n",
    "    * 1. [Model Structure](#modelStructure)\n",
    "    * 2. [Multi-Layer Cells](#multiLayerCells)\n",
    "    * 3. [Batching and Truncated Backpropagation Through Time (BPTT)](#backProp)\n",
    "  * B. [Introducing the StringLookup and Dataset utilities](#kerasUtilities)  \n",
    "  * C. [Create model / setup Tensorboard / train](#createTensorboardTrain)\n",
    "  * D. [Using the model to infer language](#modelInference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'introToRnn'></a>\n",
    "# A. Introduction to Recurrent Neural Network Language Model\n",
    "\n",
    "In this part, we'll learn about building a recurrent neural network language model (RNNLM) using TensorFlow Keras. This class of models represented the cutting edge in language modeling about 5 years ago.  Even though nowadays transformers (like BERT) represent the state-of-the-art for overall accuracy, LSTMs tend to take much less time to train and so with a limited amount of training time and compute resources they can produce surpprisingly good results.  Analyzing and building them is also useful for understanding fundamental concepts of all neural network architectures (states, input / output dimensions, batching, setting up loss and metrics.)\n",
    "\n",
    "As a reference, you may want to review the following:\n",
    "\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) (Andrej Karpathy, 2015)\n",
    "- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) (Chris Olah, 2015)\n",
    "- [A Tensorflow / Keras tutorial on using RNNs for text generation](https://www.tensorflow.org/tutorials/text/text_generation) (updated Oct. 2020)\n",
    "\n",
    "The specific model we'll build is based on the following papers. You should skim these (particularly the first one), but you don't need to read them in detail:\n",
    "\n",
    "- [Recurrent neural network based language model](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf) (Mikolov, et al. 2010)\n",
    "- [Exploring the Limits of Language Modeling](http://arxiv.org/pdf/1602.02410.pdf) (Jozefowicz, et al. 2016)\n",
    "\n",
    "We'll build our model entirely in TensorFlow Keras, so you may want to review the [TensorFlow section of assignment 1](../a1/tensorflow/tensorflow.ipynb).\n",
    "\n",
    "Finally, you'll possibly want to consult the [TensorFlow Keras API reference](https://www.tensorflow.org/api_docs/python/tf/keras), and pay special attention to the types, dimensions and order of arguments for each function.  As we suggested you do in Assignment 1, you'll want to **draw the shape of any matrices you work with on a scrap paper** or you may have trouble keeping track of your forward path!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "\n",
    "Notebook I consists of 7 parts:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'modelStructure'></a>\n",
    "## RNNLM Model Structure\n",
    "\n",
    "![RNNLM](images/rnnlm_layers.png)\n",
    "\n",
    "Here's the basic spec for our model. We'll use the following notation:\n",
    "\n",
    "- $w^{(i)}$ for the $i^{th}$ word of the sequence (as an integer index)\n",
    "- $x^{(i)}$ for the vector representation of $w^{(i)}$\n",
    "- $h^{(i)}$ for the $i^{th}$ hidden state, with indices as in Section 5.8 of the async\n",
    "- $o^{(i)}$ for the $i^{th}$ output state, which may or may not be the same as the hidden state\n",
    "- $y^{(i)}$ for the $i^{th}$ target word, which for a language model is always equal to $w^{(i+1)}$\n",
    "\n",
    "Let $ h^{(-1)} = h^{init} $ be an initial state. For an input sequence of $n$ words and $i = 0, ..., n-1$, we have:\n",
    "\n",
    "- **Embedding layer:** $ x^{(i)} = W_{in}[w^{(i)}] $\n",
    "- **Recurrent layer:** $ (o^{(i)}, h^{(i)}) = \\text{CellFunc}(x^{(i)}, h^{(i-1)}) $\n",
    "- **Output layer:** $\\hat{P}(y^{(i)}) = \\hat{P}(w^{(i+1)}) = \\text{softmax}(o^{(i)}W_{out} + b_{out}) $\n",
    " \n",
    "$\\text{CellFunc}$ can be an arbitrary function representing our recurrent cell - it can be a simple RNN cell, or something more complicated like an LSTM, or even a stacked multi-layer cell. *Note that the cell has its own internal, trainable parameters.*\n",
    "\n",
    "It may be convenient to deal with the logits of the output layer, which are the un-normalized inputs to the softmax:\n",
    "\n",
    "$$ \\text{logits}^{(i)} = o^{(i)}W_{out} + b_{out} $$\n",
    "\n",
    "We'll use these as shorthand for important dimensions:\n",
    "- `V` : vocabulary size\n",
    "- `H` : hidden state size = embedding size = per-cell output size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'multiLayerCells'></a>\n",
    "### Multi-Layer Cells\n",
    "\n",
    "One popular technique for improving the performance of RNNs is to stack multiple layers. Conceptually, this is similar to an ordinary multi-layer network, such as those you implemented on Assignment 1.\n",
    "\n",
    "![RNNLM - multicell](images/rnnlm_multicell.png)\n",
    "\n",
    "**Recurrent layer 1** will take embeddings $ x^{(i)} $ as inputs and produce outputs $o^{(i)}_0$. We can feed these in to **Recurrent layer 2**, and get another set of outputs $o^{(i)}_1$, and so on. Note that because the input dimension of an RNN cell is typically the same as the output, all of these layers will have the same shape.\n",
    "\n",
    "In TensorFlow, a single RNN layer is composed of an [LSTM cell](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell), which represents one unit of time in our model (one word in our diagram above.  If you want to stack multiple layers at each time-step you can use [tf.keras.layers.StackedRNNCells](https://www.tensorflow.org/api_docs/python/tf/keras/layers/StackedRNNCells). The `StackedRNNCells` object provides a vertically-stacked cell, as shown by the dashed green lines above.\n",
    "\n",
    "Effectively the concept of time is an abstraction and ultimately at the end of training the model parameters of `cell` above are identical at each time position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'backProp'></a>\n",
    "## Batching and Truncated Backpropagation Through Time (BPTT)\n",
    "\n",
    "Batching for an RNN works the same as for any neural network: we'll run several copies of the RNN simultaneously, each with their own hidden state and outputs. Most TensorFlow functions are batch-aware, and expect `batch_size` as the first dimension.\n",
    "\n",
    "With RNNs, however, we also need to consider the sequence length. In theory, we model our RNN as operating on sequences of arbitary length, but in practice it's much more efficient to work with batches where all the sequences have the same (maximum) length. TensorFlow calls this dimension `max_time`.  _Note: since LSTMs model sequences, a lot of the nomenclature around them mentions \"time\".  Whenever you see a reference to \"time\" in documentation, just read it as \"word sequence position(s)\"._\n",
    "\n",
    "Put together, it looks like this, where our inputs $w$ and targets $y$ will both be 2D arrays of shape `[batch_size, max_time]`.\n",
    "\n",
    "![RNNLM - batching](images/rnnlm_batching.png)\n",
    "\n",
    "Note that along the batch dimension, sequences are independent. Along the time dimension, the output of one timestep is fed into the next. \n",
    "\n",
    "In the common case of processing sequences longer than `max_time`, we can chop the input up into smaller chunks, and carry the final hidden state from one batch as the input to the next. For example, given the input `[a b c d e f g h]` and `max_time = 4`, we would run twice:\n",
    "```\n",
    "h_init    -> RNN on [a b c d] -> h_final_0\n",
    "h_final_0 -> RNN on [e f g h] -> h_final_1\n",
    "```\n",
    "We can also do this with batches, taking care to construct our batches in such a way that each batch lines up with it's predecessor. For example, with inputs `[a b c d e f g h]` and `[s t u v w x y z]`, we would do:\n",
    "```\n",
    "h_init    -> RNN on [a b c d] -> h_final_0\n",
    "                    [s t u v]\n",
    "\n",
    "h_final_0 -> RNN on [e f g h] -> h_final_1\n",
    "                    [w x y z]\n",
    "```\n",
    "where our hidden states `h_init`, etc. have shape `[batch_size, state_size]`. (*Note that `state_size = H` for a simple RNN, but is larger for LSTMs or stacked cells.*)\n",
    "\n",
    "Training in this setting is known as *truncated backpropagation through time*, or truncated BPTT. We can backpropagate errors within a batch for up to `max_time` timesteps, but not any further past the batch boundary. In practice with `max_time` greater than 20 or so, this doesn't significantly hurt the performance of our language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'implementRnn'></a>\n",
    "## Choosing an optimizer\n",
    "\n",
    "For training steps, you can use any optimizer, but we recommend `tf.train.AdamOptimizer` with gradient clipping (`tf.clip_by_global_norm`).  Adam adjusts the learning rate on a per-variable basis, and also adds a \"momentum\" term that improves the speed of convergence. See [An overview of gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/) for more.\n",
    "\n",
    "For training with AdamOptimizer, you want to use the `learning_rate = 0.01` as defined under \"Training Parameters\" (next to batch size, num epochs, etc.). If you use `learning_rate = 0.1` with Adam, the model will likely overfit or training may be unstable. (However, 0.1 works well with Adagrad and vanilla SGD.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'runOnToyInput'></a>\n",
    "# (B) Let's implement a simple RNNLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "# NumPy and TensorFlow\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "assert(tf.__version__.startswith(\"2.\"))\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend\n",
    "assert(tf.__version__.startswith(\"2.\"))\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras import Model, layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "\n",
    "# Tensorboard\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary, tf_embed_viz\n",
    "\n",
    "# From sklearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'kerasUtilities'></a>\n",
    "[Return to Top](#top)\n",
    "# B. Keras utilities \n",
    "We'll introduce __StringLookup__ for vocabulary creation and __Dataset__ for training / label pre-processing.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/drewplant/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example of pre-standardized sentence:\n",
      "  ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n",
      "\n",
      "\n",
      "and after standardization:\n",
      "  ['<s>', 'the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of', \"atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.', '<s>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-b4d90557a7e0>:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  canonsentences = np.array([['<s>'] + [utils.canonicalize_word(word) for word in sentence] + ['<s>'] for sentence in sentences ])\n"
     ]
    }
   ],
   "source": [
    "# Load \"brown\" dataset\n",
    "from importlib import reload\n",
    "reload(utils)\n",
    "name = \"brown\"\n",
    "import nltk\n",
    "assert(nltk.download(name))\n",
    "corpus = nltk.corpus.__getattr__(name)\n",
    "sentences = list(corpus.sents())\n",
    "\n",
    "# Standardize words -- lower-case characters, convert numbers to a standard code, etc.\n",
    "# Also insert a '<s> character at the beginning and end of every sentence.'\n",
    "canonsentences = np.array([['<s>'] + [utils.canonicalize_word(word) for word in sentence] + ['<s>'] for sentence in sentences ])\n",
    "print('An example of pre-standardized sentence:\\n  {}'.format(sentences[0]))\n",
    "print('\\n\\nand after standardization:\\n  {}'.format(canonsentences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras PreProcessing:  StringLookup\n",
    "\n",
    "If you're using your own input data-set in order to train a custom embedding layer, you will need a way to convert words to ids for the embedding-layer lookup operation.  For this purpose, Keras provides a convenient utility to create an input vocabulary as well as a lookup dictionary, using the StringLookup object.  Refer to Keras documentation [here.](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/StringLookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of brown corpus is 57340 sentences\n",
      "Length of words in brown corpus is 1275872\n"
     ]
    }
   ],
   "source": [
    "# Size of corpus\n",
    "print('Length of brown corpus is {} sentences'.format(len(canonsentences)))\n",
    "\n",
    "# Convert to single dimension of words\n",
    "canonwords = [ word for sentence in canonsentences for word in sentence]\n",
    "print('Length of words in brown corpus is {}'.format(len(canonwords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted vocabulary length is 10000\n"
     ]
    }
   ],
   "source": [
    "# Create the string lookup object using the 10000 most-popular words\n",
    "words_to_ids = StringLookup(max_tokens = 10000)\n",
    "\n",
    "# Process the input corpus words, creating a vocabulary / id lookup:\n",
    "words_to_ids.adapt(canonwords)\n",
    "\n",
    "# Get vocabulary size\n",
    "V = len(words_to_ids.get_vocabulary())\n",
    "print('Extracted vocabulary length is {}'.format(V))\n",
    "\n",
    "# Also create an object to convert from ids back to words from the same vocabulary:\n",
    "ids_to_words = StringLookup(vocabulary=words_to_ids.get_vocabulary(), invert=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Dataset input utility\n",
    "\n",
    "To make life easier Keras offers a [Dataset](https://www.tensorflow.org/guide/data) interface to easily wrangle input and label data for feeding and training models (not just for an LSTM model but any Keras model.)\n",
    "\n",
    "Let's use this to create an input dataset for training and evaluating our RNN model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training / test sets of word ids \n",
    "corpus_ids = words_to_ids(canonwords).numpy()\n",
    "\n",
    "# Split into train (80%) dev (10%) test (10%)\n",
    "train_ids, dev_test_ids = train_test_split(corpus_ids, train_size=0.8, random_state=42, shuffle=False)\n",
    "\n",
    "dev_ids, test_ids = train_test_split(dev_test_ids, train_size=0.5, random_state=42, shuffle=False)\n",
    "\n",
    "x_ids_train = train_ids[:-1]\n",
    "y_ids_train = train_ids[1:]\n",
    "\n",
    "# inputs of length max_time words\n",
    "max_time = 25   # length of words per sequence\n",
    "buffer_size = 100\n",
    "batch_size = 100\n",
    "\n",
    "ids_labels_dataset = tf.data.Dataset.from_tensor_slices((x_ids_train, y_ids_train))\n",
    "# examples_per_epoch = len(corpus_ids)//(max_time+1)\n",
    "\n",
    "# Create a train sequence dimension for words.  \n",
    "sequences_train = ids_labels_dataset.batch(max_time, drop_remainder=True).shuffle(buffer_size).batch(\n",
    "    batch_size, drop_remainder=True)\n",
    "\n",
    "# Create a dataset for validating during fit\n",
    "x_dev = dev_ids[:-1]\n",
    "y_dev = dev_ids[1:]\n",
    "ids_labels_validation = tf.data.Dataset.from_tensor_slices((x_dev, y_dev))\n",
    "sequences_val = ids_labels_validation.batch(max_time, drop_remainder=True).shuffle(buffer_size).batch(\n",
    "    batch_size, drop_remainder=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'createTensorboardTrain'></a> \n",
    "[Top](#top)\n",
    "# C. Create the model.  Setup tensorboard.  Train the model.\n",
    "Let's build a new RNN model and fit to our larger Brown corpus dataset\n",
    "\n",
    "You can use [Tensorboard](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard) for visualizing your model's structure, as well as viewing training and validation accuracy as your training fit progresses.\n",
    "\n",
    "To create tensorboard data, simply create a callback and include the callback inside the call to fit() as shown below.  Then run the tensorboard command in a terminal window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LSTM layer provides two arguments:\n",
    "#   return_state (which returns lstm_state, lstm_last_time_state, cell_state)\n",
    "#   return_sequence (which ensures that the 'lstm_state' returned object is the vector output\n",
    "#   for all time positions in the sequence.)\n",
    "#\n",
    "#   Note that for the case (return_sequence = False, return_state = True) lstm_state and lstm_last_time_state\n",
    "#   are the same tensor.\n",
    "#\n",
    "# Here is a good article illustrating the two options:\n",
    "#    https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/\n",
    "\n",
    "\n",
    "# Let's build a model class to instantiate our model \n",
    "# ...and more closely control training / inference behavior.\n",
    "class MyModel(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_layers, rnn_units, hidden_activation, dropout_rate,\n",
    "                 hidden_initializer, batchnorm = True):\n",
    "        super().__init__(self)\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        # self.rnn = keras.layers.GRU(rnn_units, return_sequences = True, return_state=True, \n",
    "        #                              activation = hidden_activation, kernel_initializer = hidden_initializer, \n",
    "        #                              stateful=False)\n",
    "        self.n_layers = n_layers\n",
    "        self.rnn = []\n",
    "        self.norm = []\n",
    "        self.dropout = []\n",
    "        for i in range(n_layers):\n",
    "            self.rnn.append(tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True, \n",
    "                                                 activation = hidden_activation,\n",
    "                                                 kernel_initializer = hidden_initializer))\n",
    "            self.norm.append(tf.keras.layers.BatchNormalization())\n",
    "            self.dropout.append(tf.keras.layers.Dropout(dropout_rate))\n",
    "            \n",
    "        # self.rnn = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True,\n",
    "        #                                 activation = hidden_activation, \n",
    "        #                                 kernel_initializer = hidden_initializer)\n",
    "        \n",
    "        self.do_batchnorm = batchnorm\n",
    "    \n",
    "        # tf.keras.layers.GRU(rnn_units,\n",
    "        #                                return_sequences=True, \n",
    "        #                                return_state=True)\n",
    "        self.dense = keras.layers.Dense(vocab_size)\n",
    " \n",
    "    # You must set return_sequences=True when stacking LSTM layers so that the second LSTM layer has a three-dimensional sequence input.\n",
    "\n",
    "    def call(self, inputs, passin_states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "            \n",
    "        # In the following expression the return values are:  \n",
    "        #         x = the sequence of outputs from the layer, \n",
    "        #   state_h = the final state vector (at the last time-step)\n",
    "        #   state_c = the cell memory at the final step\n",
    "        states = []\n",
    "        for i in range(self.n_layers):\n",
    "            if passin_states is None:\n",
    "                statesi = self.rnn[i].get_initial_state(x)\n",
    "            else:\n",
    "                statesi = passin_states[i]\n",
    "            x, state_h, state_c = self.rnn[i](x, initial_state=statesi, training=training)\n",
    "            # x, state_h, state_c = self.rnn(x, initial_state=states, training=training)\n",
    "            x = self.dropout[i](x)\n",
    "            if self.do_batchnorm:\n",
    "                x = self.norm[i](x)\n",
    "            states.append((state_h, state_c))\n",
    "        \n",
    "        # Output layer outputs logits rather than softmax as we didn't specify any activation\n",
    "        x = self.dense(x, training=training)\n",
    "        \n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else: \n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = 10000\n",
    "\n",
    "# The embedding dimension\n",
    "# embedding_dim = 256\n",
    "embedding_dim = 50\n",
    "\n",
    "# Number of hidden layers\n",
    "n_layers = 2\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 100\n",
    "\n",
    "hidden_activation = 'relu'\n",
    "\n",
    "hidden_initializer = 'he_uniform'\n",
    "\n",
    "# Dropout\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Create model instance\n",
    "model = MyModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    n_layers = n_layers,\n",
    "    rnn_units=rnn_units,\n",
    "    hidden_activation = hidden_activation, \n",
    "    hidden_initializer = hidden_initializer,\n",
    "    dropout_rate = dropout_rate,\n",
    "    batchnorm = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 25, 10000) # (batch_size, sequence_length, vocab_size)\n",
      "(100, 25) (100, 25)\n",
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  500000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  60400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo multiple                  400       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch multiple                  400       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  1010000   \n",
      "=================================================================\n",
      "Total params: 1,651,600\n",
      "Trainable params: 1,651,200\n",
      "Non-trainable params: 400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get a feel for looking at training samples in our input Dataset\n",
    "for input_example_batch, target_example_batch in sequences_train.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "    print(input_example_batch.shape, target_example_batch.shape)\n",
    "\n",
    "# Print out a model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of example batch loss: ()\n",
      "Prediction shape:  (100, 25, 10000)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         9.210309\n",
      "20.0\n",
      "9999.687\n"
     ]
    }
   ],
   "source": [
    "# See the behavior of loss function, how to take mean loss over batch\n",
    "# We will use \"from_logits\" = True since our outputs are logits rather than softmax (ie, [batch,seq_len,V])\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# We can calculate an example loss using eager execution.\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print('Shape of example batch loss: {}'.format(example_batch_loss.numpy().shape))\n",
    "mean_loss = example_batch_loss.numpy().mean()\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", mean_loss)\n",
    "\n",
    "# We know that tensorflow uses natural log (base e) for crossentropy calculation \n",
    "checkbase = loss(np.array([[[0]]]), np.array([[[0., 20, 0.]]]))\n",
    "print(checkbase.numpy())\n",
    "# Confirm that exp(mean loss ~ V)  (why?)\n",
    "# If initialization is good, each q ~ 1 / V => p ln(V) = ln(V) -> exp(ln(V)) = V !!\n",
    "# ln(x) = ln(2^log2(x)) = log2(x) * ln(2) => \n",
    "print(tf.exp(mean_loss).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(optimizer='adam', loss=loss, metrics = ['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoints\n",
    "When training a model over the course of several hours or days it is important (vital!) to setup periodic checkpoints for your model, so if something bad happens (a power outage, timeout, loss of colab resources, etc.) you will not need to start over training from scratch.  This is the purpose of [checkpoints](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint).  \n",
    "\n",
    "The following cell shows an example of how to set this up and use it when fitting your model.  In this case we're check-pointing every epoch but you can also specify the frequency in a couple of other ways detailed in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "408/408 [==============================] - 124s 300ms/step - loss: 7.1994 - sparse_categorical_accuracy: 0.1541 - val_loss: 5.4874 - val_sparse_categorical_accuracy: 0.2262\n",
      "Epoch 2/20\n",
      "408/408 [==============================] - 124s 303ms/step - loss: 5.1634 - sparse_categorical_accuracy: 0.2383 - val_loss: 4.6527 - val_sparse_categorical_accuracy: 0.2712\n",
      "Epoch 3/20\n",
      "408/408 [==============================] - 124s 303ms/step - loss: 4.9937 - sparse_categorical_accuracy: 0.2480 - val_loss: 4.5921 - val_sparse_categorical_accuracy: 0.2736\n",
      "Epoch 4/20\n",
      "408/408 [==============================] - 141s 345ms/step - loss: 4.8830 - sparse_categorical_accuracy: 0.2546 - val_loss: 4.5490 - val_sparse_categorical_accuracy: 0.2775\n",
      "Epoch 5/20\n",
      "408/408 [==============================] - 132s 323ms/step - loss: 4.8014 - sparse_categorical_accuracy: 0.2594 - val_loss: 4.5416 - val_sparse_categorical_accuracy: 0.2775\n",
      "Epoch 6/20\n",
      "408/408 [==============================] - 129s 316ms/step - loss: 4.7365 - sparse_categorical_accuracy: 0.2625 - val_loss: 4.5319 - val_sparse_categorical_accuracy: 0.2786\n",
      "Epoch 7/20\n",
      "408/408 [==============================] - 143s 350ms/step - loss: 4.6843 - sparse_categorical_accuracy: 0.2648 - val_loss: 4.5428 - val_sparse_categorical_accuracy: 0.2783\n",
      "Epoch 8/20\n",
      "408/408 [==============================] - 141s 346ms/step - loss: 4.6381 - sparse_categorical_accuracy: 0.2669 - val_loss: 4.5555 - val_sparse_categorical_accuracy: 0.2796\n",
      "Epoch 9/20\n",
      "408/408 [==============================] - 139s 341ms/step - loss: 4.5992 - sparse_categorical_accuracy: 0.2687 - val_loss: 4.5726 - val_sparse_categorical_accuracy: 0.2783\n",
      "Epoch 10/20\n",
      "408/408 [==============================] - 138s 337ms/step - loss: 4.5637 - sparse_categorical_accuracy: 0.2701 - val_loss: 4.5870 - val_sparse_categorical_accuracy: 0.2790\n",
      "Epoch 11/20\n",
      "408/408 [==============================] - 138s 337ms/step - loss: 4.5328 - sparse_categorical_accuracy: 0.2713 - val_loss: 4.6015 - val_sparse_categorical_accuracy: 0.2793\n",
      "Epoch 12/20\n",
      "408/408 [==============================] - 133s 326ms/step - loss: 4.5036 - sparse_categorical_accuracy: 0.2730 - val_loss: 4.6159 - val_sparse_categorical_accuracy: 0.2797\n",
      "Epoch 13/20\n",
      "408/408 [==============================] - 133s 326ms/step - loss: 4.4795 - sparse_categorical_accuracy: 0.2738 - val_loss: 4.6325 - val_sparse_categorical_accuracy: 0.2795\n",
      "Epoch 14/20\n",
      "408/408 [==============================] - 132s 325ms/step - loss: 4.4548 - sparse_categorical_accuracy: 0.2743 - val_loss: 4.6435 - val_sparse_categorical_accuracy: 0.2798\n",
      "Epoch 15/20\n",
      "408/408 [==============================] - 133s 327ms/step - loss: 4.4341 - sparse_categorical_accuracy: 0.2758 - val_loss: 4.6632 - val_sparse_categorical_accuracy: 0.2796\n",
      "Epoch 16/20\n",
      "408/408 [==============================] - 133s 326ms/step - loss: 4.4149 - sparse_categorical_accuracy: 0.2769 - val_loss: 4.6793 - val_sparse_categorical_accuracy: 0.2805\n",
      "Epoch 17/20\n",
      "408/408 [==============================] - 136s 332ms/step - loss: 4.3974 - sparse_categorical_accuracy: 0.2772 - val_loss: 4.7011 - val_sparse_categorical_accuracy: 0.2806\n",
      "Epoch 18/20\n",
      "408/408 [==============================] - 133s 326ms/step - loss: 4.3793 - sparse_categorical_accuracy: 0.2777 - val_loss: 4.7121 - val_sparse_categorical_accuracy: 0.2801\n",
      "Epoch 19/20\n",
      "408/408 [==============================] - 133s 327ms/step - loss: 4.3640 - sparse_categorical_accuracy: 0.2786 - val_loss: 4.7295 - val_sparse_categorical_accuracy: 0.2800\n",
      "Epoch 20/20\n",
      "408/408 [==============================] - 141s 346ms/step - loss: 4.3472 - sparse_categorical_accuracy: 0.2789 - val_loss: 4.7472 - val_sparse_categorical_accuracy: 0.2791\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "EPOCHS = 20\n",
    "history = model.fit(sequences_train, \n",
    "                    validation_data = sequences_val, epochs=EPOCHS, \n",
    "                    callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#top)\n",
    "\n",
    "<a id = 'modelInference'></a>\n",
    "# D. Using the model to infer language\n",
    "\n",
    "Now that we have a model, let's see what it can do!  \n",
    "\n",
    "We'll write some code which handles the following:\n",
    "  * uses our trained model\n",
    "  * uses our words->ids lookup and another ids->words so we can read-back what words the model is prediction.\n",
    "  * passes in one word at a time for each inference step, also passing in the state vectors from the previous step to provide \"context\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Generate text\n",
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, ids_to_words, words_to_ids, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature=temperature\n",
    "    self.model = model\n",
    "    self.ids_to_words = ids_to_words\n",
    "    self.words_to_ids = words_to_ids\n",
    "\n",
    "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
    "    skip_ids = self.words_to_ids(['','[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices = skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(words_to_ids.get_vocabulary())]) \n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  # @tf.function\n",
    "  def generate_one_step(self, input_words, passin_states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    # input_words = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    # input_ids = self.words_to_ids(input_words).to_tensor()\n",
    "    input_words = tf.strings.split(input_words)\n",
    "    input_ids = self.words_to_ids(input_words.to_tensor())\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, word, next_word_logits] \n",
    "    predicted_logits, states =  self.model(inputs=input_ids, passin_states=passin_states, \n",
    "                                          return_state=True)\n",
    "    # Only use the prediction in the final time-position.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to words\n",
    "    predicted_words = self.ids_to_words(predicted_ids)\n",
    "\n",
    "    # Return the words and model state.\n",
    "    return predicted_words, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated language:\n",
      "hello, my name is reduced . <s> <s> he sat along text , cleaners , said and consumption it household , and presently to me , or in full on the period if he wanted to go according for a rate of ice '' . <s> <s> `` listen before all '' . <s> <s> `` i'm sure you haven't can't say some for a bit '' ? ? <s> <s> `` i asked what he had and putting your problems back to in the public '' . <s> <s> `` that wouldn't also wash so , so now do let them happen for \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 0.781890869140625\n"
     ]
    }
   ],
   "source": [
    "# Generate an inference instance\n",
    "one_step_model = OneStep(model, ids_to_words, words_to_ids)\n",
    "\n",
    "# \n",
    "start = time.time()\n",
    "states = None\n",
    "# next_word = tf.constant(['hello, my name is'])\n",
    "next_word = np.array(['hello, my name is'])\n",
    "# next_word = tf.constant([['hello, my name is'],['hello', 'my', 'name', 'is']])\n",
    "result = [next_word]\n",
    "\n",
    "for n in range(100):\n",
    "    next_word, states = one_step_model.generate_one_step(next_word, passin_states=states)\n",
    "    result.append(next_word)\n",
    "\n",
    "result = tf.strings.join(result, separator=' ')\n",
    "end = time.time()\n",
    "\n",
    "print('Generated language:')\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "\n",
    "print(f\"\\nRun time: {end - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'endOfNotebook'></a>\n",
    "_End of Notebook_  \n",
    "[Back to top](#top)  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
