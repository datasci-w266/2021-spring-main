{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'returnToTop'></a>\n",
    "\n",
    "# Table of contents\n",
    "  * A. [What's so important about NN model parameters?](#introToModelParams)  \n",
    "  * B. [Let's create our base model](#createOurModel)\n",
    "  * C. [Layer Activation](#layerActivation)  \n",
    "  * D. [Weight Initialization](#initializationFunctions)  \n",
    "  * E. [Optimizers](#optimizers) \n",
    "  * F. [Normalization](#batchNormalization)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'introToModelParams'></a>\n",
    "# A. What's so important about NN model parameters?\n",
    "\n",
    "Having appropriate model features can mean the difference being a model being able to learn effectively or not.  In the early 2000's neural net models suffered from the vanishing / exploding gradient problem.  Incremental innovations led to discoveries to overcome that problem, some of which we'll introduce here.  Most of these are considered de regueur today but at the time they were each revolutionary.\n",
    "\n",
    "In this lesson, we'll learn about some important model choices such as:\n",
    "  * layer activation choice\n",
    "  * initialization of weights\n",
    "  * optimizers\n",
    "  * normalization strategies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drewplant/.pyenv/versions/3.7.6/envs/w266/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"2.\"))\n",
    "from importlib import reload\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "# Keras libraries\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import layers, backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend\n",
    "assert(tf.__version__.startswith(\"2.\"))\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras import Model, layers\n",
    "\n",
    "# Tensorboard\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our model description\n",
    "\n",
    "We will develop a model with a word input and predicting the part of speech.  Our dataset will be a modified version of mobypos.txt from the Gutenberg project: https://archive.org/details/mobypartofspeech03203gut\n",
    "\n",
    "We've taken the original part of speech corpus (which had over 100 POS classes) and filtered it to only:  (1) 6 parts of speech  (2) words contained in the GloVe embeddings corpus.\n",
    "\n",
    "Our goal is to see if a model leveraging GloVe can accurately predict the part of speech given in our training Moby corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors from data/glove/glove.6B.zip\n",
      "Parsing file: data/glove/glove.6B.zip:glove.6B.100d.txt\n",
      "Found 400,000 words.\n",
      "Parsing vectors... Done! (W.shape = (400003, 100))\n"
     ]
    }
   ],
   "source": [
    "# Point to your existing GloVe download dataset from a3\n",
    "# Note:  This assumes you've already run the glove commands to download glove for a3!\n",
    "! if [[ ! -e data ]]; then ln -s ../../assignment/a3/data .; fi\n",
    "\n",
    "# Point to glove_helper utility in a3\n",
    "! if [[ ! -e glove_helper.py ]]; then     ln -s ../../assignment/a3/glove_helper.py .; fi\n",
    "\n",
    "# Point to w266_common in a3\n",
    "! if [[ ! -e w266_common ]]; then     ln -s ../../assignment/a3/w266_common .; fi\n",
    "\n",
    "import glove_helper; reload(glove_helper)\n",
    "embedding_dim = 100  # 50, 100, 200, 300 dim are available\n",
    "\n",
    "hands = glove_helper.Hands(ndim=embedding_dim)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our downloaded GloVe shape:  (400003, 100)\n"
     ]
    }
   ],
   "source": [
    "# We can load a pre-trained embedding matrix into Keras as follows (using GloVe)\n",
    "print('Our downloaded GloVe shape:  {}'.format(hands.W.shape))\n",
    "\n",
    "# Create a model, pre-load GloVe model into keras Embedding layer:\n",
    "num_tokens = hands.W.shape[0]\n",
    "embedding_matrix = hands.W\n",
    "\n",
    "# Load GloVe embedding matrix into Keras embedding layer\n",
    "embedPretrain = Embedding(num_tokens,\n",
    "                          embedding_dim,\n",
    "                          embeddings_initializer=initializers.Constant(\n",
    "                              embedding_matrix),\n",
    "                          trainable=False)\n",
    "\n",
    "# We will use this pre-trained embedding in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word    a bon march\n",
      "pos               v\n",
      "Name: 5, dtype: object\n",
      "\n",
      "There are 56227 word/POS samples in our dataset.\n"
     ]
    }
   ],
   "source": [
    "# Let's load our smaller Moby training dataset\n",
    "\n",
    "# posdf = pd.read_csv(\"mobysmall.txt\", sep=',', header = None, names = ['word', 'pos'])\n",
    "posdf = pd.read_csv(\"normalization/mobypos_singlepos.txt\", sep=',', header = None, names = ['word', 'pos'])\n",
    "\n",
    "# Let's look at an example word\n",
    "print(posdf.iloc[5])\n",
    "\n",
    "# We'll add a column that is the GloVe 'id' for the word (or 2 for <unk>)\n",
    "posdf['id'] = posdf.loc[:,'word'].apply(lambda x: hands.vocab.word_to_id.get(x, 2))\n",
    "\n",
    "# Further eliminate all training words not already in GloVe\n",
    "validdf = posdf.loc[posdf['id'] != 2, :]\n",
    "validdf.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# Count how many word samples we're left with\n",
    "print('\\nThere are {} word/POS samples in our dataset.'.format(len(validdf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12 unique parts of speech labels:  ['N' 'v' 'A' 'V' 'p' 't' 'i' 'P' 'D' 'C' 'r' 'h']\n",
      "       word pos      id\n",
      "0  aardvark   N   89193\n",
      "1  aardwolf   N  331196\n",
      "2        aa   N   13479\n",
      "3     abaca   N  166697\n",
      "4     aback   v   31658\n",
      "\n",
      "POS N has 29759 examples.\n",
      "POS v has 2631 examples.\n",
      "POS A has 12069 examples.\n",
      "POS V has 5370 examples.\n",
      "POS p has 2426 examples.\n",
      "POS t has 3028 examples.\n",
      "POS i has 677 examples.\n",
      "POS P has 84 examples.\n",
      "POS D has 58 examples.\n",
      "POS C has 40 examples.\n",
      "POS r has 65 examples.\n",
      "POS h has 20 examples.\n"
     ]
    }
   ],
   "source": [
    "# Look at part of speech (POS) labels\n",
    "\n",
    "allpos = validdf['pos'].unique()\n",
    "print('There are {} unique parts of speech labels:  {}'.format(len(allpos), allpos))\n",
    "\n",
    "# Key to pos as encoded by  Moby Part-of-Speech II author, Grady Ward:\n",
    "#     Noun                    N\n",
    "#     Plural                  p\n",
    "#     Noun Phrase             h\n",
    "#     Verb (usu participle)   V\n",
    "#     Verb (transitive)       t\n",
    "#     Verb (intransitive)     i\n",
    "#     Adjective               A\n",
    "#     Adverb                  v\n",
    "#     Conjunction             C\n",
    "#     Preposition             P\n",
    "#     Interjection            !\n",
    "#     Pronoun                 r\n",
    "#     Definite Article        D\n",
    "#     Indefinite Article      I\n",
    "#     Nominative              o\n",
    "\n",
    "# Create a numeric class label for each output pos string label\n",
    "pos2label = dict(zip(allpos, range(len(allpos))))\n",
    "label2pos = dict(zip(range(len(allpos)), allpos))\n",
    "\n",
    "# The first few rows of our dataframe\n",
    "print(validdf.head())\n",
    "print()\n",
    "\n",
    "# Print number of samples in each class\n",
    "for i in range(len(allpos)):\n",
    "    pos = label2pos[i]\n",
    "    print('POS {} has {} examples.'.format(pos, len(validdf.loc[validdf['pos'] == pos, :])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  word: unviable,  pos label: A\n",
      "  word: sawdust,  pos label: N\n",
      "  word: exclusivity,  pos label: N\n",
      "  word: double-hung,  pos label: A\n",
      "  word: coruscant,  pos label: A\n"
     ]
    }
   ],
   "source": [
    "# Our dataset is not very balanced as there are many more nouns (N) and Adjectives (A) than other POS.\n",
    "# ...Nonetheless we can still guage accuracy fairly well given that no single class has more than\n",
    "# ...56% so even clinging to simple baseline of the dominant (N) class would give only slightly better\n",
    "# ...than a random guess.\n",
    "\n",
    "# Our model input be fed with a single word:\n",
    "n_words = 1\n",
    "x_ids = validdf['id'].to_numpy()\n",
    "# Encode each pos as a number\n",
    "y_ids = np.array([pos2label[pos] for pos in validdf['pos']])\n",
    "\n",
    "# We need to create one-hot categorical variables for labels\n",
    "x_ids = x_ids.reshape(-1,1)\n",
    "y_ids = y_ids.reshape(-1,1)\n",
    "y_ids = tf.keras.utils.to_categorical(y_ids, num_classes = len(allpos))\n",
    "\n",
    "# Expand dims to insert dim = 1 y-class label at position 1\n",
    "y_ids = np.expand_dims(y_ids, axis = 1)\n",
    "\n",
    "# Create training, test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_ids, y_ids, test_size=0.33, \n",
    "                                                    random_state=42, shuffle = True)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# Print out the first few examples:\n",
    "for i in range(5):\n",
    "    print('  word: {},  pos label: {}'.format(hands.vocab.id_to_word[X_train[i][0]], label2pos[np.where(y_train[i][0]==1)[0][0]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'createOurModel'></a>\n",
    "# B. Let's create our base NN Model\n",
    "\n",
    "We've created a function which returns a model given a dict of input model params.  This will make it easier to try various modifications to the model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple keras model to predict POS\n",
    "# Model parameters\n",
    "modelParams = dict(\n",
    "    batch_size = batch_size, \n",
    "    learning_rate = 0.01,\n",
    "    n_words = 1,\n",
    "    num_layers = 10,\n",
    "    H = 40,\n",
    "    hidden_initializer = 'random_uniform',\n",
    "    hidden_activation = 'sigmoid',\n",
    "    batchnorm = False,\n",
    "    output_classes = len(allpos),\n",
    "    output_initializer = 'random_uniform',\n",
    "    output_activation = 'softmax',\n",
    "    output_loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "# Define a function to create our LSTM model\n",
    "def create_pos_classifier(**kwargs):\n",
    "    \n",
    "    # Read input keyword args\n",
    "    batch_size = kwargs.get('batch_size', 100) \n",
    "    learning_rate = kwargs.get('learning_rate', 0.01)\n",
    "    n_words = kwargs.get('n_words', 1)\n",
    "    num_layers = kwargs.get('num_layers', 1)\n",
    "    H = kwargs.get('H', 10)\n",
    "    hidden_initializer = kwargs.get('hidden_initializer', 'random_uniform')\n",
    "    hidden_activation = kwargs.get('hidden_activation', 'relu')\n",
    "    batchnorm = kwargs.get('batchnorm', False)\n",
    "    output_classes = kwargs.get('output_classes', 6)\n",
    "    output_initializer = kwargs.get('output_initializer', tf.keras.initializers.RandomUniform(minval=-1.0, maxval=1.0))\n",
    "    output_activation = kwargs.get('output_activation', 'softmax')\n",
    "    output_loss = kwargs.get('output_loss', tf.keras.losses.SparseCategoricalCrossentropy())\n",
    "    metrics = kwargs.get('metrics', ['accuracy'])\n",
    "    \n",
    "    # Create an input layer for our model\n",
    "    input_ = Input(shape = (n_words,), name=\"x\")\n",
    "    \n",
    "    # Use GloVe pre-trained embeddings\n",
    "    embedPretrain = Embedding(num_tokens, embedding_dim, \n",
    "                              embeddings_initializer=initializers.Constant(embedding_matrix),\n",
    "                              trainable=False)\n",
    "    x = embedPretrain(input_)\n",
    "    \n",
    "    # Create hidden layers\n",
    "    for i in range(num_layers):\n",
    "        x = Dense(units = H, activation = None, kernel_initializer = hidden_initializer)(x)\n",
    "        if (batchnorm):\n",
    "            x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation(hidden_activation)(x)\n",
    "    \n",
    "    # Create an output layer with softmax output type\n",
    "    outlayer = Dense(units = output_classes, activation = output_activation,\n",
    "                     kernel_initializer = output_initializer)\n",
    "    yhat = outlayer(x)\n",
    "    \n",
    "    # Use Adam optimizer with our programmed learning rate\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, name='Adam')\n",
    "    \n",
    "    # Build / compile the model\n",
    "    model = Model(inputs=input_, outputs=yhat, name=\"rnn_prediction_model\")\n",
    "    model.compile(loss = output_loss, optimizer = optimizer, metrics = metrics)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"rnn_prediction_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "x (InputLayer)               [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 1, 100)            40000300  \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1, 40)             4040      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 1, 40)             0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1, 40)             1640      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 1, 40)             0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1, 40)             1640      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 1, 40)             0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1, 40)             1640      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 1, 40)             0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1, 40)             1640      \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 1, 40)             0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1, 40)             1640      \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 1, 40)             0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1, 40)             1640      \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 1, 40)             0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1, 40)             1640      \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 1, 40)             0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1, 40)             1640      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 1, 40)             0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1, 40)             1640      \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 1, 40)             0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1, 12)             492       \n",
      "=================================================================\n",
      "Total params: 40,019,592\n",
      "Trainable params: 19,292\n",
      "Non-trainable params: 40,000,300\n",
      "_________________________________________________________________\n",
      "Train on 37672 samples, validate on 18555 samples\n",
      "Epoch 1/30\n",
      "37672/37672 [==============================] - 2s 57us/sample - loss: 1.4309 - accuracy: 0.5277 - val_loss: 1.4212 - val_accuracy: 0.5299\n",
      "Epoch 2/30\n",
      "37672/37672 [==============================] - 1s 24us/sample - loss: 1.4188 - accuracy: 0.5290 - val_loss: 1.4196 - val_accuracy: 0.5299\n",
      "Epoch 3/30\n",
      "37672/37672 [==============================] - 1s 23us/sample - loss: 1.4171 - accuracy: 0.5290 - val_loss: 1.4213 - val_accuracy: 0.5299\n",
      "Epoch 4/30\n",
      "37672/37672 [==============================] - 1s 23us/sample - loss: 1.4164 - accuracy: 0.5290 - val_loss: 1.4183 - val_accuracy: 0.5299\n",
      "Epoch 5/30\n",
      "37672/37672 [==============================] - 1s 26us/sample - loss: 1.4166 - accuracy: 0.5290 - val_loss: 1.4192 - val_accuracy: 0.5299\n",
      "Epoch 6/30\n",
      "37672/37672 [==============================] - 1s 28us/sample - loss: 1.4163 - accuracy: 0.5290 - val_loss: 1.4188 - val_accuracy: 0.5299\n",
      "Epoch 7/30\n",
      "37672/37672 [==============================] - 1s 25us/sample - loss: 1.4157 - accuracy: 0.5290 - val_loss: 1.4171 - val_accuracy: 0.5299\n",
      "Epoch 8/30\n",
      "37672/37672 [==============================] - 1s 27us/sample - loss: 1.4152 - accuracy: 0.5290 - val_loss: 1.4225 - val_accuracy: 0.5299\n",
      "Epoch 9/30\n",
      "37672/37672 [==============================] - 1s 26us/sample - loss: 1.4151 - accuracy: 0.5290 - val_loss: 1.4182 - val_accuracy: 0.5299\n",
      "Epoch 10/30\n",
      "37672/37672 [==============================] - 1s 28us/sample - loss: 1.4157 - accuracy: 0.5290 - val_loss: 1.4169 - val_accuracy: 0.5299\n",
      "Epoch 11/30\n",
      "37672/37672 [==============================] - 1s 25us/sample - loss: 1.4148 - accuracy: 0.5290 - val_loss: 1.4170 - val_accuracy: 0.5299\n",
      "Epoch 12/30\n",
      "37672/37672 [==============================] - 1s 23us/sample - loss: 1.4147 - accuracy: 0.5290 - val_loss: 1.4165 - val_accuracy: 0.5299\n",
      "Epoch 13/30\n",
      "37672/37672 [==============================] - 1s 23us/sample - loss: 1.4145 - accuracy: 0.5290 - val_loss: 1.4176 - val_accuracy: 0.5299\n",
      "Epoch 14/30\n",
      "37672/37672 [==============================] - 1s 23us/sample - loss: 1.4143 - accuracy: 0.5290 - val_loss: 1.4171 - val_accuracy: 0.5299\n",
      "Epoch 15/30\n",
      "37672/37672 [==============================] - 1s 23us/sample - loss: 1.4148 - accuracy: 0.5290 - val_loss: 1.4151 - val_accuracy: 0.5299\n",
      "Epoch 16/30\n",
      "37672/37672 [==============================] - 1s 23us/sample - loss: 1.4142 - accuracy: 0.5290 - val_loss: 1.4167 - val_accuracy: 0.5299\n",
      "Epoch 17/30\n",
      "37672/37672 [==============================] - 1s 23us/sample - loss: 1.4143 - accuracy: 0.5290 - val_loss: 1.4164 - val_accuracy: 0.5299\n",
      "Epoch 18/30\n",
      "37672/37672 [==============================] - 1s 28us/sample - loss: 1.4142 - accuracy: 0.5290 - val_loss: 1.4157 - val_accuracy: 0.5299\n",
      "Epoch 19/30\n",
      "37672/37672 [==============================] - 1s 24us/sample - loss: 1.4145 - accuracy: 0.5290 - val_loss: 1.4173 - val_accuracy: 0.5299\n",
      "Epoch 20/30\n",
      "37672/37672 [==============================] - 1s 23us/sample - loss: 1.4144 - accuracy: 0.5290 - val_loss: 1.4160 - val_accuracy: 0.5299\n",
      "Epoch 21/30\n",
      "37672/37672 [==============================] - 1s 24us/sample - loss: 1.4145 - accuracy: 0.5290 - val_loss: 1.4157 - val_accuracy: 0.5299\n",
      "Epoch 22/30\n",
      "37672/37672 [==============================] - 1s 24us/sample - loss: 1.4139 - accuracy: 0.5290 - val_loss: 1.4156 - val_accuracy: 0.5299\n",
      "Epoch 23/30\n",
      "37672/37672 [==============================] - 1s 23us/sample - loss: 1.4138 - accuracy: 0.5290 - val_loss: 1.4182 - val_accuracy: 0.5299\n",
      "Epoch 24/30\n",
      "37672/37672 [==============================] - 1s 29us/sample - loss: 1.4141 - accuracy: 0.5290 - val_loss: 1.4162 - val_accuracy: 0.5299\n",
      "Epoch 25/30\n",
      "37672/37672 [==============================] - 1s 28us/sample - loss: 1.4139 - accuracy: 0.5290 - val_loss: 1.4161 - val_accuracy: 0.5299\n",
      "Epoch 26/30\n",
      "37672/37672 [==============================] - 1s 25us/sample - loss: 1.4140 - accuracy: 0.5290 - val_loss: 1.4157 - val_accuracy: 0.5299\n",
      "Epoch 27/30\n",
      "37672/37672 [==============================] - 1s 26us/sample - loss: 1.4139 - accuracy: 0.5290 - val_loss: 1.4171 - val_accuracy: 0.5299\n",
      "Epoch 28/30\n",
      "37672/37672 [==============================] - 1s 25us/sample - loss: 1.4140 - accuracy: 0.5290 - val_loss: 1.4159 - val_accuracy: 0.5299\n",
      "Epoch 29/30\n",
      "37672/37672 [==============================] - 1s 23us/sample - loss: 1.4140 - accuracy: 0.5290 - val_loss: 1.4161 - val_accuracy: 0.5299\n",
      "Epoch 30/30\n",
      "37672/37672 [==============================] - 1s 22us/sample - loss: 1.4139 - accuracy: 0.5290 - val_loss: 1.4154 - val_accuracy: 0.5299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x152206550>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build model using our previous model parameter settings, show a summary and fit the model\n",
    "modelSimple = create_pos_classifier(**modelParams)\n",
    "modelSimple.summary()\n",
    "# Run a training session\n",
    "modelSimple.fit(X_train, y_train, batch_size = 100, validation_data = (X_test, y_test), epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xid: [100000]\n",
      "y_hat: [[[5.3318655e-01 4.3336723e-02 2.1067242e-01 9.6589103e-02 4.4125270e-02\n",
      "   5.3718239e-02 1.3913307e-02 1.2620878e-03 1.0136481e-03 7.7308016e-04\n",
      "   1.0379988e-03 3.7149608e-04]]]\n",
      "ytrue: [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Categorical cross-entropy loss: 1.557450771331787\n",
      "\n",
      "\n",
      "xid: [50133]\n",
      "y_hat: [[[5.3318655e-01 4.3336723e-02 2.1067242e-01 9.6589103e-02 4.4125270e-02\n",
      "   5.3718239e-02 1.3913307e-02 1.2620878e-03 1.0136481e-03 7.7308016e-04\n",
      "   1.0379988e-03 3.7149608e-04]]]\n",
      "ytrue: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Categorical cross-entropy loss: 0.6288837790489197\n",
      "\n",
      "\n",
      "xid: [36284]\n",
      "y_hat: [[[5.3318655e-01 4.3336723e-02 2.1067242e-01 9.6589103e-02 4.4125270e-02\n",
      "   5.3718239e-02 1.3913307e-02 1.2620878e-03 1.0136481e-03 7.7308016e-04\n",
      "   1.0379988e-03 3.7149608e-04]]]\n",
      "ytrue: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Categorical cross-entropy loss: 0.6288837790489197\n",
      "\n",
      "\n",
      "xid: [165577]\n",
      "y_hat: [[[5.3318655e-01 4.3336723e-02 2.1067242e-01 9.6589103e-02 4.4125270e-02\n",
      "   5.3718239e-02 1.3913307e-02 1.2620878e-03 1.0136481e-03 7.7308016e-04\n",
      "   1.0379988e-03 3.7149608e-04]]]\n",
      "ytrue: [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Categorical cross-entropy loss: 1.557450771331787\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can check out our model results and see how it compares with our labels.\n",
    "# You can also directly calculate CE loss for y_hat against y.\n",
    "for xin, ytrue in list(zip(X_train[:10], y_train[:4])):\n",
    "    # xin, ytrue = batch[0].numpy(), batch[1].numpy()\n",
    "    y_hat = modelSimple.predict([xin])\n",
    "    print('xid: {}\\ny_hat: {}\\nytrue: {}'.format(xin, y_hat, ytrue))\n",
    "    cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "    print('Categorical cross-entropy loss: {}\\n\\n'.format(cce(ytrue, y_hat).numpy()))\n",
    "\n",
    "# We could also do an error analysis to compare what words it still gets wrong.\n",
    "# Try this as an exercise to see where the model is weakest!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Top](#returnToTop) \n",
    "<a id = 'layerActivation'></a>\n",
    "# C. Layer Activations\n",
    "\n",
    "As you've learned in class, 'softmax' is used when you want to predict probabilites.  This is exactly the situation for our current example, where we are output probability for each of our 6 pos classes.  In fact, if you opted to output 'sigmoid' instead, your model would 'learn' that it can output near-0 loss by simply setting each output class to '1', thereby always hitting the correct class.  Of course it wouldn't actually be learning anything.\n",
    "\n",
    "The activation 'tanh' has an advantage over 'sigmoid' for being 0-valued at x = 0.  'relu' is also 0-valued at x = 0 and has the additional advantage of not saturating for x > 0 for helping maintain a non-zero differentiable loss function during backward propagation; you can try other choices for activation in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37672 samples, validate on 18555 samples\n",
      "Epoch 1/30\n",
      "37672/37672 [==============================] - 2s 55us/sample - loss: 1.4499 - accuracy: 0.5290 - val_loss: 1.4166 - val_accuracy: 0.5299\n",
      "Epoch 2/30\n",
      "37672/37672 [==============================] - 1s 22us/sample - loss: 1.4175 - accuracy: 0.5290 - val_loss: 1.4166 - val_accuracy: 0.5299\n",
      "Epoch 3/30\n",
      "37672/37672 [==============================] - 1s 29us/sample - loss: 1.4169 - accuracy: 0.5290 - val_loss: 1.4183 - val_accuracy: 0.5299\n",
      "Epoch 4/30\n",
      "37672/37672 [==============================] - 1s 23us/sample - loss: 1.4162 - accuracy: 0.5290 - val_loss: 1.4152 - val_accuracy: 0.5299\n",
      "Epoch 5/30\n",
      "37672/37672 [==============================] - 1s 23us/sample - loss: 1.4159 - accuracy: 0.5290 - val_loss: 1.4155 - val_accuracy: 0.5299\n",
      "Epoch 6/30\n",
      "37672/37672 [==============================] - 1s 23us/sample - loss: 1.4153 - accuracy: 0.5290 - val_loss: 1.4161 - val_accuracy: 0.5299\n",
      "Epoch 7/30\n",
      "37672/37672 [==============================] - 1s 26us/sample - loss: 1.4156 - accuracy: 0.5290 - val_loss: 1.4155 - val_accuracy: 0.5299\n",
      "Epoch 8/30\n",
      "37672/37672 [==============================] - 1s 23us/sample - loss: 1.4149 - accuracy: 0.5290 - val_loss: 1.4157 - val_accuracy: 0.5299\n",
      "Epoch 9/30\n",
      "37672/37672 [==============================] - 1s 23us/sample - loss: 1.4152 - accuracy: 0.5290 - val_loss: 1.4171 - val_accuracy: 0.5299\n",
      "Epoch 10/30\n",
      "37672/37672 [==============================] - 1s 23us/sample - loss: 1.4152 - accuracy: 0.5290 - val_loss: 1.4207 - val_accuracy: 0.5299\n",
      "Epoch 11/30\n",
      "37672/37672 [==============================] - 1s 23us/sample - loss: 1.4151 - accuracy: 0.5290 - val_loss: 1.4178 - val_accuracy: 0.5299\n",
      "Epoch 12/30\n",
      "37672/37672 [==============================] - 1s 27us/sample - loss: 1.4160 - accuracy: 0.5290 - val_loss: 1.4158 - val_accuracy: 0.5299\n",
      "Epoch 13/30\n",
      "37672/37672 [==============================] - 1s 27us/sample - loss: 1.4145 - accuracy: 0.5290 - val_loss: 1.4183 - val_accuracy: 0.5299\n",
      "Epoch 14/30\n",
      "37672/37672 [==============================] - 1s 23us/sample - loss: 1.4146 - accuracy: 0.5290 - val_loss: 1.4173 - val_accuracy: 0.5299\n",
      "Epoch 15/30\n",
      "37672/37672 [==============================] - 1s 22us/sample - loss: 1.4153 - accuracy: 0.5290 - val_loss: 1.4167 - val_accuracy: 0.5299\n",
      "Epoch 16/30\n",
      "37672/37672 [==============================] - 1s 23us/sample - loss: 1.4144 - accuracy: 0.5290 - val_loss: 1.4160 - val_accuracy: 0.5299\n",
      "Epoch 17/30\n",
      "37672/37672 [==============================] - 1s 21us/sample - loss: 1.4149 - accuracy: 0.5290 - val_loss: 1.4156 - val_accuracy: 0.5299\n",
      "Epoch 18/30\n",
      "37672/37672 [==============================] - 1s 22us/sample - loss: 1.4149 - accuracy: 0.5290 - val_loss: 1.4183 - val_accuracy: 0.5299\n",
      "Epoch 19/30\n",
      "37672/37672 [==============================] - 1s 23us/sample - loss: 1.4145 - accuracy: 0.5290 - val_loss: 1.4158 - val_accuracy: 0.5299\n",
      "Epoch 20/30\n",
      "37672/37672 [==============================] - 1s 22us/sample - loss: 1.4147 - accuracy: 0.5290 - val_loss: 1.4160 - val_accuracy: 0.5299\n",
      "Epoch 21/30\n",
      "37672/37672 [==============================] - 1s 22us/sample - loss: 1.4145 - accuracy: 0.5290 - val_loss: 1.4163 - val_accuracy: 0.5299\n",
      "Epoch 22/30\n",
      "37672/37672 [==============================] - 1s 22us/sample - loss: 1.4142 - accuracy: 0.5290 - val_loss: 1.4158 - val_accuracy: 0.5299\n",
      "Epoch 23/30\n",
      "37672/37672 [==============================] - 1s 22us/sample - loss: 1.4143 - accuracy: 0.5290 - val_loss: 1.4168 - val_accuracy: 0.5299\n",
      "Epoch 24/30\n",
      "37672/37672 [==============================] - 1s 24us/sample - loss: 1.4142 - accuracy: 0.5290 - val_loss: 1.4154 - val_accuracy: 0.5299\n",
      "Epoch 25/30\n",
      "37672/37672 [==============================] - 1s 27us/sample - loss: 1.4136 - accuracy: 0.5290 - val_loss: 1.4162 - val_accuracy: 0.5299\n",
      "Epoch 26/30\n",
      "37672/37672 [==============================] - 1s 28us/sample - loss: 1.4140 - accuracy: 0.5290 - val_loss: 1.4165 - val_accuracy: 0.5299\n",
      "Epoch 27/30\n",
      "37672/37672 [==============================] - 1s 28us/sample - loss: 1.4141 - accuracy: 0.5290 - val_loss: 1.4155 - val_accuracy: 0.5299\n",
      "Epoch 28/30\n",
      "37672/37672 [==============================] - 1s 23us/sample - loss: 1.4141 - accuracy: 0.5290 - val_loss: 1.4165 - val_accuracy: 0.5299\n",
      "Epoch 29/30\n",
      "37672/37672 [==============================] - 1s 22us/sample - loss: 1.4137 - accuracy: 0.5290 - val_loss: 1.4153 - val_accuracy: 0.5299\n",
      "Epoch 30/30\n",
      "37672/37672 [==============================] - 1s 23us/sample - loss: 1.4139 - accuracy: 0.5290 - val_loss: 1.4160 - val_accuracy: 0.5299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1528a6f10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelParams['hidden_activation'] = 'relu'  # You could also try 'tanh' or 'selu' \n",
    "# ...for Scaled Exponential Linear Unit\n",
    "modelHiddenAct = create_pos_classifier(**modelParams)\n",
    "modelHiddenAct.fit(X_train, y_train, batch_size = 100, validation_data = (X_test, y_test), epochs = 30)\n",
    "\n",
    "# If you want to see how 'sigmoid' for the output layer won't work, uncomment and try the below code:\n",
    "# modelParams['hidden_activation'] = 'relu' # reset hidden activation\n",
    "# modelParams['output_activation'] = 'sigmoid'  # This will produce bad results...  (why?)\n",
    "# modelOutputSigmoid = create_pos_classifier(**modelParams)\n",
    "# modelOutputSigmoid.fit(trainds, validation_data = testds, epochs = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Top](#returnToTop) \n",
    "<a id = 'initializationFunctions'></a>\n",
    "# D. Weight Initialization\n",
    "\n",
    "Weight initialization is another important tool for avoiding exploding, vanishing gradients problem.\n",
    "In the deep neural net early days, random uniform initialization of weights was common.  There appeared to be two problems:  (1) gradients could end up vanishing (or exploding) as they were calculated in moving from layer output to input and (2) signals could end up vanishing (or exploding) as they propagated from input to output in a forward direction through the layer.  \n",
    "\n",
    "A ground-breaking paper presented by Glorot and Bengio in 2010  (http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) described a way to initialize weights so as to avoid vanishing or exploding signals during inference and likewise to minimize vanishing/exploding gradients during backward propagation.  The goal is to keep input and output variance equal as well as output and input gradient variance.  Out of the paper was born 'Xavier' initialization (also known as 'Glorot uniform'.)\n",
    "\n",
    "Soon afterwards, the 'relu' activation function was seen to be an effective way to avoid moving the signal mean away from 0, as well as mitigating signal saturation.  At the same time, the 'He' initialization was presented as an improvement over 'Xavier' when using 'relu' activation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37672 samples, validate on 18555 samples\n",
      "Epoch 1/30\n",
      "37672/37672 [==============================] - 2s 58us/sample - loss: 0.9097 - accuracy: 0.7088 - val_loss: 0.7157 - val_accuracy: 0.7819\n",
      "Epoch 2/30\n",
      "37672/37672 [==============================] - 1s 22us/sample - loss: 0.7126 - accuracy: 0.7833 - val_loss: 0.6759 - val_accuracy: 0.7886\n",
      "Epoch 3/30\n",
      "37672/37672 [==============================] - 1s 21us/sample - loss: 0.6767 - accuracy: 0.7931 - val_loss: 0.7061 - val_accuracy: 0.7886\n",
      "Epoch 4/30\n",
      "37672/37672 [==============================] - 1s 21us/sample - loss: 0.6501 - accuracy: 0.8014 - val_loss: 0.6740 - val_accuracy: 0.7869\n",
      "Epoch 5/30\n",
      "37672/37672 [==============================] - 1s 27us/sample - loss: 0.6363 - accuracy: 0.8052 - val_loss: 0.6651 - val_accuracy: 0.7959\n",
      "Epoch 6/30\n",
      "37672/37672 [==============================] - 1s 24us/sample - loss: 0.6253 - accuracy: 0.8084 - val_loss: 0.6722 - val_accuracy: 0.7966\n",
      "Epoch 7/30\n",
      "37672/37672 [==============================] - 1s 23us/sample - loss: 0.6171 - accuracy: 0.8120 - val_loss: 0.6788 - val_accuracy: 0.7897\n",
      "Epoch 8/30\n",
      "37672/37672 [==============================] - 1s 24us/sample - loss: 0.6130 - accuracy: 0.8135 - val_loss: 0.6607 - val_accuracy: 0.7895\n",
      "Epoch 9/30\n",
      "37672/37672 [==============================] - 1s 25us/sample - loss: 0.6122 - accuracy: 0.8116 - val_loss: 0.6577 - val_accuracy: 0.7962\n",
      "Epoch 10/30\n",
      "37672/37672 [==============================] - 1s 22us/sample - loss: 0.5999 - accuracy: 0.8155 - val_loss: 0.6549 - val_accuracy: 0.7993\n",
      "Epoch 11/30\n",
      "37672/37672 [==============================] - 1s 23us/sample - loss: 0.5991 - accuracy: 0.8175 - val_loss: 0.6483 - val_accuracy: 0.8010\n",
      "Epoch 12/30\n",
      "37672/37672 [==============================] - 1s 22us/sample - loss: 0.5936 - accuracy: 0.8198 - val_loss: 0.6474 - val_accuracy: 0.8009\n",
      "Epoch 13/30\n",
      "37672/37672 [==============================] - 1s 23us/sample - loss: 0.5961 - accuracy: 0.8197 - val_loss: 0.6779 - val_accuracy: 0.7912\n",
      "Epoch 14/30\n",
      "37672/37672 [==============================] - 1s 27us/sample - loss: 0.5968 - accuracy: 0.8191 - val_loss: 0.6653 - val_accuracy: 0.7932\n",
      "Epoch 15/30\n",
      "37672/37672 [==============================] - 1s 24us/sample - loss: 0.5911 - accuracy: 0.8199 - val_loss: 0.6800 - val_accuracy: 0.7998\n",
      "Epoch 16/30\n",
      "37672/37672 [==============================] - 1s 29us/sample - loss: 0.5864 - accuracy: 0.8213 - val_loss: 0.7102 - val_accuracy: 0.7784\n",
      "Epoch 17/30\n",
      "37672/37672 [==============================] - 1s 22us/sample - loss: 0.5958 - accuracy: 0.8180 - val_loss: 0.6774 - val_accuracy: 0.7960\n",
      "Epoch 18/30\n",
      "37672/37672 [==============================] - 1s 22us/sample - loss: 0.5830 - accuracy: 0.8243 - val_loss: 0.6711 - val_accuracy: 0.7989\n",
      "Epoch 19/30\n",
      "37672/37672 [==============================] - 1s 22us/sample - loss: 0.5913 - accuracy: 0.8210 - val_loss: 0.7048 - val_accuracy: 0.7834\n",
      "Epoch 20/30\n",
      "37672/37672 [==============================] - 1s 22us/sample - loss: 0.6151 - accuracy: 0.8116 - val_loss: 0.6747 - val_accuracy: 0.7946\n",
      "Epoch 21/30\n",
      "37672/37672 [==============================] - 1s 22us/sample - loss: 0.6000 - accuracy: 0.8179 - val_loss: 0.6866 - val_accuracy: 0.7994\n",
      "Epoch 22/30\n",
      "37672/37672 [==============================] - 1s 22us/sample - loss: 0.6254 - accuracy: 0.8091 - val_loss: 0.6925 - val_accuracy: 0.7899\n",
      "Epoch 23/30\n",
      "37672/37672 [==============================] - 1s 23us/sample - loss: 0.6102 - accuracy: 0.8133 - val_loss: 0.6721 - val_accuracy: 0.7995\n",
      "Epoch 24/30\n",
      "37672/37672 [==============================] - 1s 23us/sample - loss: 0.5940 - accuracy: 0.8185 - val_loss: 0.6515 - val_accuracy: 0.7986\n",
      "Epoch 25/30\n",
      "37672/37672 [==============================] - 1s 27us/sample - loss: 0.5804 - accuracy: 0.8229 - val_loss: 0.6677 - val_accuracy: 0.7964\n",
      "Epoch 26/30\n",
      "37672/37672 [==============================] - 1s 23us/sample - loss: 0.6245 - accuracy: 0.8081 - val_loss: 0.6723 - val_accuracy: 0.7968\n",
      "Epoch 27/30\n",
      "37672/37672 [==============================] - 1s 22us/sample - loss: 0.5968 - accuracy: 0.8200 - val_loss: 0.6541 - val_accuracy: 0.7985\n",
      "Epoch 28/30\n",
      "37672/37672 [==============================] - 1s 23us/sample - loss: 0.6039 - accuracy: 0.8156 - val_loss: 0.6839 - val_accuracy: 0.7911\n",
      "Epoch 29/30\n",
      "37672/37672 [==============================] - 1s 22us/sample - loss: 0.6001 - accuracy: 0.8178 - val_loss: 0.6613 - val_accuracy: 0.8004\n",
      "Epoch 30/30\n",
      "37672/37672 [==============================] - 1s 23us/sample - loss: 0.6101 - accuracy: 0.8124 - val_loss: 0.6934 - val_accuracy: 0.7854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1502fe550>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelParams['hidden_activation'] = 'relu'  \n",
    "modelParams['hidden_initializer'] = 'he_uniform' # He uniform initialization works better for relu\n",
    "modelParams['output_initializer'] = 'he_uniform'\n",
    "modelHeInit = create_pos_classifier(**modelParams)\n",
    "modelHeInit.fit(X_train, y_train, batch_size = 100, validation_data = (X_test, y_test), epochs = 30)\n",
    "\n",
    "# You can also use LeCun initialization when using the SELU activation\n",
    "# modelParams['hidden_activation'] = 'selu'  \n",
    "# modelParams['output_initializer'] = 'lecun_normal' # LeCun initialization works better for selu\n",
    "# modelLeCunInit = create_pos_classifier(**modelParams)\n",
    "# modelLeCunInit.fit(trainds, validation_data = testds, epochs = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'optimizers'></a>\n",
    "## E. Optimizers\n",
    "\n",
    "For training steps, you can use any optimizer, but we recommend `tf.train.AdamOptimizer` with gradient clipping (`tf.clip_by_global_norm`).  Adam adjusts the learning rate on a per-variable basis, and also adds a \"momentum\" term that improves the speed of convergence. See [An overview of gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/) for more.\n",
    "\n",
    "For training with AdamOptimizer, a good value is `learning_rate = 0.01` as defined under \"Training Parameters\" (next to batch size, num epochs, etc.). If you use `learning_rate = 0.1` with Adam, the model will likely overfit or training may be unstable. (However, 0.1 works well with Adagrad and vanilla SGD.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to top](#top)\n",
    "<a id = 'batchNormalization'></a>\n",
    "# F. Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas having good initialization of weights helps avoid gradient exploding / vanishing at the beginning of training, a technique called *batch normalization* helps maintain zero-mean / unity-variance of layer inputs during training.  \n",
    "\n",
    "The technique involves computing a mean and standard deviation (averaged over the batch) for each input feature and using the mean and standard deviation vectors to normalize each feature.  Two additional learned parameters (beta and gamma) are used to further scale and shift the inputs to help maintain zero-offset.  \n",
    "\n",
    "![Batch Normalization equations](./normalization/batch_normalization_equations.png)\n",
    "\n",
    "The technique was presented in a 2015 paper by Ioffe and Szegedy (http://proceedings.mlr.press/v37/ioffe15.pdf).  \n",
    "\n",
    "Keras provides a Batch Normalization layer which can be placed after hidden layers (or even after the Input() layer) to take care of this scaling / shifting.\n",
    "\n",
    "Batch normalization tends to lengthen per-batch training computation time due to the requirement of computing batch means and batch variances.  The trade-off is that the quality of each training cycle tends to be better (by keeping layer inputs well-conditioned via scaling / shifting) which tends to have a net effect of improving the rate of accuracy improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"rnn_prediction_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "x (InputLayer)               [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 1, 100)            40000300  \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 1, 40)             4040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 1, 40)             160       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 1, 40)             0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 1, 40)             1640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 1, 40)             160       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 1, 40)             0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 1, 40)             1640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 1, 40)             160       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 1, 40)             0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 1, 40)             1640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 1, 40)             160       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1, 40)             0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 1, 40)             1640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 1, 40)             160       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 1, 40)             0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 1, 40)             1640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 1, 40)             160       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 1, 40)             0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 1, 40)             1640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 1, 40)             160       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 1, 40)             0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 1, 40)             1640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 1, 40)             160       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 1, 40)             0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 1, 40)             1640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 1, 40)             160       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 1, 40)             0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 1, 40)             1640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 1, 40)             160       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 1, 40)             0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 1, 12)             492       \n",
      "=================================================================\n",
      "Total params: 40,021,192\n",
      "Trainable params: 20,092\n",
      "Non-trainable params: 40,001,100\n",
      "_________________________________________________________________\n",
      "Train on 37672 samples, validate on 18555 samples\n",
      "Epoch 1/30\n",
      "37672/37672 [==============================] - 5s 134us/sample - loss: 0.8957 - accuracy: 0.7099 - val_loss: 0.9687 - val_accuracy: 0.7306\n",
      "Epoch 2/30\n",
      "37672/37672 [==============================] - 2s 42us/sample - loss: 0.6891 - accuracy: 0.7817 - val_loss: 0.7665 - val_accuracy: 0.7698\n",
      "Epoch 3/30\n",
      "37672/37672 [==============================] - 2s 40us/sample - loss: 0.6539 - accuracy: 0.7919 - val_loss: 0.6751 - val_accuracy: 0.7851\n",
      "Epoch 4/30\n",
      "37672/37672 [==============================] - 2s 48us/sample - loss: 0.6354 - accuracy: 0.7954 - val_loss: 0.8014 - val_accuracy: 0.7567\n",
      "Epoch 5/30\n",
      "37672/37672 [==============================] - 2s 46us/sample - loss: 0.6123 - accuracy: 0.8043 - val_loss: 0.6702 - val_accuracy: 0.7915\n",
      "Epoch 6/30\n",
      "37672/37672 [==============================] - 2s 40us/sample - loss: 0.5981 - accuracy: 0.8064 - val_loss: 0.6830 - val_accuracy: 0.7975\n",
      "Epoch 7/30\n",
      "37672/37672 [==============================] - 1s 36us/sample - loss: 0.5924 - accuracy: 0.8081 - val_loss: 0.6837 - val_accuracy: 0.7815\n",
      "Epoch 8/30\n",
      "37672/37672 [==============================] - 1s 37us/sample - loss: 0.5805 - accuracy: 0.8144 - val_loss: 0.6313 - val_accuracy: 0.8006\n",
      "Epoch 9/30\n",
      "37672/37672 [==============================] - 1s 35us/sample - loss: 0.5719 - accuracy: 0.8130 - val_loss: 0.6444 - val_accuracy: 0.7933\n",
      "Epoch 10/30\n",
      "37672/37672 [==============================] - 1s 37us/sample - loss: 0.5643 - accuracy: 0.8156 - val_loss: 0.6663 - val_accuracy: 0.7869\n",
      "Epoch 11/30\n",
      "37672/37672 [==============================] - 1s 40us/sample - loss: 0.5541 - accuracy: 0.8175 - val_loss: 0.6707 - val_accuracy: 0.7885\n",
      "Epoch 12/30\n",
      "37672/37672 [==============================] - 2s 46us/sample - loss: 0.5493 - accuracy: 0.8210 - val_loss: 0.6702 - val_accuracy: 0.7907\n",
      "Epoch 13/30\n",
      "37672/37672 [==============================] - 2s 49us/sample - loss: 0.5416 - accuracy: 0.8211 - val_loss: 0.6236 - val_accuracy: 0.8026\n",
      "Epoch 14/30\n",
      "37672/37672 [==============================] - 2s 45us/sample - loss: 0.5371 - accuracy: 0.8215 - val_loss: 0.6148 - val_accuracy: 0.8050\n",
      "Epoch 15/30\n",
      "37672/37672 [==============================] - 2s 41us/sample - loss: 0.5352 - accuracy: 0.8233 - val_loss: 0.6363 - val_accuracy: 0.7990\n",
      "Epoch 16/30\n",
      "37672/37672 [==============================] - 2s 43us/sample - loss: 0.5238 - accuracy: 0.8267 - val_loss: 0.6207 - val_accuracy: 0.8003\n",
      "Epoch 17/30\n",
      "37672/37672 [==============================] - 2s 41us/sample - loss: 0.5172 - accuracy: 0.8270 - val_loss: 0.6288 - val_accuracy: 0.8028\n",
      "Epoch 18/30\n",
      "37672/37672 [==============================] - 2s 41us/sample - loss: 0.5160 - accuracy: 0.8279 - val_loss: 0.6170 - val_accuracy: 0.8034\n",
      "Epoch 19/30\n",
      "37672/37672 [==============================] - 2s 41us/sample - loss: 0.5107 - accuracy: 0.8292 - val_loss: 0.6384 - val_accuracy: 0.7939\n",
      "Epoch 20/30\n",
      "37672/37672 [==============================] - 2s 40us/sample - loss: 0.5073 - accuracy: 0.8310 - val_loss: 0.6321 - val_accuracy: 0.8020\n",
      "Epoch 21/30\n",
      "37672/37672 [==============================] - 2s 41us/sample - loss: 0.5057 - accuracy: 0.8318 - val_loss: 0.6572 - val_accuracy: 0.7961\n",
      "Epoch 22/30\n",
      "37672/37672 [==============================] - 1s 40us/sample - loss: 0.4947 - accuracy: 0.8334 - val_loss: 0.6272 - val_accuracy: 0.8054\n",
      "Epoch 23/30\n",
      "37672/37672 [==============================] - 2s 41us/sample - loss: 0.4979 - accuracy: 0.8328 - val_loss: 0.6386 - val_accuracy: 0.7964\n",
      "Epoch 24/30\n",
      "37672/37672 [==============================] - 2s 42us/sample - loss: 0.4894 - accuracy: 0.8360 - val_loss: 0.6473 - val_accuracy: 0.8005\n",
      "Epoch 25/30\n",
      "37672/37672 [==============================] - 2s 40us/sample - loss: 0.4840 - accuracy: 0.8369 - val_loss: 0.6145 - val_accuracy: 0.8032\n",
      "Epoch 26/30\n",
      "37672/37672 [==============================] - 2s 42us/sample - loss: 0.4801 - accuracy: 0.8387 - val_loss: 0.6482 - val_accuracy: 0.8002\n",
      "Epoch 27/30\n",
      "37672/37672 [==============================] - 2s 40us/sample - loss: 0.4833 - accuracy: 0.8369 - val_loss: 0.6371 - val_accuracy: 0.8046\n",
      "Epoch 28/30\n",
      "37672/37672 [==============================] - 2s 41us/sample - loss: 0.4761 - accuracy: 0.8406 - val_loss: 0.6514 - val_accuracy: 0.7967\n",
      "Epoch 29/30\n",
      "37672/37672 [==============================] - 2s 40us/sample - loss: 0.4725 - accuracy: 0.8400 - val_loss: 0.6347 - val_accuracy: 0.8002\n",
      "Epoch 30/30\n",
      "37672/37672 [==============================] - 2s 40us/sample - loss: 0.4716 - accuracy: 0.8402 - val_loss: 0.6419 - val_accuracy: 0.8032\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x153fa5850>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modify model to enable Batch Normalization layer\n",
    "modelParams['batchnorm'] = True\n",
    "modelSimple = create_pos_classifier(**modelParams)\n",
    "modelSimple.summary()  # To see that batch normalization has been added\n",
    "# Run a training session\n",
    "modelSimple.fit(X_train, y_train, batch_size = 100, validation_data = (X_test, y_test), epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
