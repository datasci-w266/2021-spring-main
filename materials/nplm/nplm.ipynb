{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Probabilistic Language Model\n",
    "\n",
    "In this notebook, we'll implement the [Neural Probabilistic Language Model (Bengio et al. 2003)](http://machinelearning.wustl.edu/mlpapers/paper_files/BengioDVJ03.pdf). This model was one of the first applications of deep learning to NLP, predating `word2vec` by a whole decade! Many of the ideas in this model, however, are remarkably current - such as the skip-layer connection now popular in the form of \"residual networks.\"\n",
    "\n",
    "This model is a straightforward extension of n-gram language modeling: it uses a fixed context window, but instead of a table it uses a neural network to predict the next word. It'll also serve as a segue to Assignment 4, in which you'll implement a recurrent neural network language model (RNNLM).\n",
    "\n",
    "#### Note on training time\n",
    "The NPLM can take a while to train on a slower machine - we clocked it at 10-20 min on a 2-core Cloud Compute instance. \n",
    "\n",
    "If you're using a cloud compute instance, you can add more CPUs without having to re-do setup. With your instance turned off, go to https://console.cloud.google.com/compute/instances, click your instance, and go to \"Edit\". Under machine type, select \"Custom\" and pick 4-8 CPUs and 2 GB of RAM. Make sure you shut down when you're done, and use the Edit menu again to scale back the size to something less expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, json, time, shutil\n",
    "import itertools, collections\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"2.\"))\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary, tf_embed_viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NPLM Model Architecture\n",
    "\n",
    "Recall that our n-gram mode of order $k+1$ was:\n",
    "\n",
    "$$ P(w_i | w_{i-1}, w_{i-2}, ..., w_0) \\approx P(w_i | w_{i-1}, ..., w_{i-k}) $$\n",
    "\n",
    "Where we estimated the probabilities by smoothed maximum likelihood.\n",
    "\n",
    "For the NPLM, we'll replace that estimate with a neural network predictor that directly learns a mapping from contexts $(w_{i-1}, ..., w_{i-k})$ to a distribution over words $w_i$:\n",
    "\n",
    "$$ P(w_i | w_{i-1}, ..., w_{i-k}) = f(w_i, (w_{i-1}, ..., w_{i-k})) $$\n",
    "\n",
    "Here's what that network will look like:\n",
    "![NPLM architecture](nplm.png)\n",
    "\n",
    "Broadly, there are three parts:\n",
    "1. **Embedding layer**: map words into vector space\n",
    "2. **Hidden layer**: compress and apply nonlinearity\n",
    "3. **Output layer**: predict next word using softmax\n",
    "\n",
    "The model also has *skip connections* between the embedding layer and the output layer. This just means that the output layer takes as input the concatenated embeddings in addition to the hidden layer output. This was considered an unusual pattern, but has recently become popular again in the form of [Residual Networks](http://www.kaiminghe.com/icml16tutorial/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf) and [Highway Networks](https://arxiv.org/abs/1505.00387).\n",
    "\n",
    "With modern computers and a couple tricks, we should be able to get a decent model to run in just a few minutes - a far cry from the three weeks it took in 2003!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing our Model\n",
    "\n",
    "To implement the NPLM in TensorFlow, we need to define a Tensor for each model component. To make a clear distinction between TF and non-TF code, we'll use variable names that end in an underscore for Tensor objects. We'll also construct the model so it can accept batch inputs, as this will greatly speed up training.\n",
    "\n",
    "Hyperparameters:\n",
    "- `V` : vocabulary size\n",
    "- `M` : embedding size\n",
    "- `N` : context window size\n",
    "- `H` : hidden units\n",
    "\n",
    "Inputs:\n",
    "- `ids_` : (batch_size, N), integer indices for context words\n",
    "- `y_` : (batch_size,), integer indices for target word\n",
    "\n",
    "Model parameters:\n",
    "- `C_` : (V,M), input-side word embeddings\n",
    "- `W1_` : (NxM, H)\n",
    "- `b1_` : (H,)\n",
    "- `W2_` : (H, V)\n",
    "- `W3_` : (NxM, V), matrix for skip-layer connection\n",
    "- `b3_` : (V,)\n",
    "\n",
    "Intermediate states:\n",
    "- `x_` : (batch_size, NxM), concatenated embeddings\n",
    "- `h_` : (batch_size, H), hidden state $= \\tanh(xW_1 + b_1)$\n",
    "- `logit_` : (batch_size, V), $= hW_2 + xW_3 + b_3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution() #bad form - strictly for compatibility. NOT the way to do native tf2.x\n",
    "tf.compat.v1.reset_default_graph()\n",
    "tf.compat.v1.set_random_seed(42)\n",
    "\n",
    "##\n",
    "# Hyperparameters\n",
    "V = 10000\n",
    "M = 30\n",
    "N = 3\n",
    "H = 50\n",
    "\n",
    "# Inputs\n",
    "# Using \"None\" in place of batch size allows \n",
    "# it to be dynamically computed later.\n",
    "with tf.compat.v1.name_scope(\"Inputs\"):\n",
    "    ids_ = tf.compat.v1.placeholder(tf.int32, shape=[None, N], name=\"ids\")\n",
    "    y_ = tf.compat.v1.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "    \n",
    "with tf.compat.v1.name_scope(\"Embedding_Layer\"):\n",
    "    C_ = tf.Variable(tf.random.uniform([V, M], -1.0, 1.0), name=\"C\")\n",
    "    # embedding_lookup gives shape (batch_size, N, M)\n",
    "    x_ = tf.reshape(tf.nn.embedding_lookup(params=C_, ids=ids_), \n",
    "                    [-1, N*M], name=\"x\")\n",
    "    \n",
    "with tf.compat.v1.name_scope(\"Hidden_Layer\"):\n",
    "    W1_ = tf.Variable(tf.random.normal([N*M,H]), name=\"W1\")\n",
    "    b1_ = tf.Variable(tf.zeros([H,], dtype=tf.float32), name=\"b1\")\n",
    "    h_ = tf.tanh(tf.matmul(x_, W1_) + b1_, name=\"h\")\n",
    "    \n",
    "with tf.compat.v1.name_scope(\"Output_Layer\"):\n",
    "    W2_ = tf.Variable(tf.random.normal([H,V]), name=\"W2\")\n",
    "    W3_ = tf.Variable(tf.random.normal([N*M,V]), name=\"W3\")\n",
    "    b3_ = tf.Variable(tf.zeros([V,], dtype=tf.float32), name=\"b3\")\n",
    "    # Concat [h x] and [W2 W3]\n",
    "    hx_ = tf.concat([h_, x_], 1, name=\"hx\")\n",
    "    W23_ = tf.concat([W2_, W3_], 0, name=\"W23\")\n",
    "    logits_ = tf.add(tf.matmul(hx_, W23_), b3_, name=\"logits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll add in our usual cross-entropy loss. Recall from async that this is *very* slow for a large vocabulary, and even for a small vocabulary it represents the bulk of the computation time. To speed up training we'll use a sampled softmax loss, as in [Jozefowicz et al. 2016](https://arxiv.org/abs/1602.02410):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.compat.v1.name_scope(\"Cost_Function\"):\n",
    "    # Sampled softmax loss, for training\n",
    "    per_example_train_loss_ = tf.nn.sampled_softmax_loss(weights=tf.transpose(a=W23_), biases=b3_,\n",
    "                                             labels=tf.expand_dims(y_, 1), inputs=hx_,\n",
    "                                             num_sampled=100, num_classes=V,\n",
    "                                             name=\"per_example_sampled_softmax_loss\")\n",
    "    train_loss_ = tf.reduce_mean(input_tensor=per_example_train_loss_, name=\"sampled_softmax_loss\")\n",
    "    \n",
    "    # Full softmax loss, for scoring\n",
    "    per_example_loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_, logits=logits_, name=\"per_example_loss\")\n",
    "    loss_ = tf.reduce_mean(input_tensor=per_example_loss_, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And add training ops. We'll use AdaGrad instead of vanilla SGD, as this tends to converge faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mhbutler/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.name_scope(\"Training\"):\n",
    "    alpha_ = tf.compat.v1.placeholder(tf.float32, name=\"learning_rate\")\n",
    "    optimizer_ = tf.compat.v1.train.AdagradOptimizer(alpha_)\n",
    "    # train_step_ = optimizer_.minimize(loss_)\n",
    "    train_step_ = optimizer_.minimize(train_loss_)\n",
    "    \n",
    "# Initializer step\n",
    "init_ = tf.compat.v1.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll add a few ops to do prediction:\n",
    "- `pred_proba_` : (batch_size, V), $ = P(w_i | w_{i-1}, ...)$ for all words $i$\n",
    "- `pred_max` : (batch_size,), id of most likely next word\n",
    "- `pred_random` : (batch_size,), id of a randomly-sampled next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.compat.v1.name_scope(\"Prediction\"):\n",
    "    pred_proba_ = tf.nn.softmax(logits_, name=\"pred_proba\")\n",
    "    pred_max_ = tf.argmax(input=logits_, axis=1, name=\"pred_max\")\n",
    "    pred_random_ = tf.random.categorical(logits=logits_, num_samples=1, name=\"pred_random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use TensorBoard to view this graph, even before we run the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_writer = tf.compat.v1.summary.FileWriter(\"tf_graph\", \n",
    "                                       tf.compat.v1.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a separate terminal, run:\n",
    "```\n",
    "tensorboard --logdir=\"~/w266/materials/nplm/tf_graph\" --port 6006\n",
    "```\n",
    "and go to http://localhost:6006/\n",
    "\n",
    "It should look something like this:\n",
    "![NPLM graph](nplm-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Corpus\n",
    "\n",
    "As in the original paper, we'll train on the Brown corpus. We'll pre-process the inputs as in earlier assignments, lowercasing and canonicalizing digits and adding `<s>` and `</s>` markers to sentence boundaries. See [embeddings.ipynb](../embeddings/embeddings.ipynb) for more details.\n",
    "\n",
    "We'll also restrict our vocabulary to the top 10,000 words. \n",
    "\n",
    "_**Exercise:**_ why do we need a fixed-size vocabulary for our neural model? And why does it help to restrict its size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/mhbutler/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: 10,000 types\n",
      "Loaded 57,340 sentences (1.16119e+06 tokens)\n",
      "Training set: 45,872 sentences (979,646 tokens)\n",
      "Test set: 11,468 sentences (181,546 tokens)\n"
     ]
    }
   ],
   "source": [
    "corpus_name = \"brown\"\n",
    "V = 10000\n",
    "\n",
    "vocab, train_ids, test_ids = utils.load_corpus(corpus_name, split=0.8, V=V, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is designed to accept batches of data, so we need to do a little re-formatting. We want our input batches to look like the following, where the first $N$ columns are the inputs and the last is the target word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_{i-3}</th>\n",
       "      <th>w_{i-2}</th>\n",
       "      <th>w_{i-1}</th>\n",
       "      <th>target: w_i</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5613</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>5613</td>\n",
       "      <td>655</td>\n",
       "      <td>2288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5613</td>\n",
       "      <td>655</td>\n",
       "      <td>2288</td>\n",
       "      <td>1640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols = [\"w_{i-%d}\" % d for d in range(N,0,-1)] + [\"target: w_i\"]\n",
    "M = np.array([[0,3,5613,655], [3,5613,655,2288], [5613,655,2288,1640]])\n",
    "utils.pretty_print_matrix(M, cols=cols, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll format our entire corpus like this, and then we can just sample blocks from it to get our training minibatches. The code for this is in `utils.py`, but you can view it in-notebook by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.build_windows??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_{i-3}</th>\n",
       "      <th>w_{i-2}</th>\n",
       "      <th>w_{i-1}</th>\n",
       "      <th>target: w_i</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5405</td>\n",
       "      <td>652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>5405</td>\n",
       "      <td>652</td>\n",
       "      <td>2287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5405</td>\n",
       "      <td>652</td>\n",
       "      <td>2287</td>\n",
       "      <td>1628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>652</td>\n",
       "      <td>2287</td>\n",
       "      <td>1628</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2287</td>\n",
       "      <td>1628</td>\n",
       "      <td>65</td>\n",
       "      <td>1837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_windows = utils.build_windows(train_ids, N)\n",
    "test_windows = utils.build_windows(test_ids, N)\n",
    "\n",
    "# Check that we got what we want\n",
    "# Just look at the first few IDs for this sample\n",
    "utils.pretty_print_matrix(utils.build_windows(train_ids[:(N+5)], N, shuffle=False), cols=cols, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training time!\n",
    "\n",
    "With our data in array form, we can train our model much like any machine learning model. The code below should look familiar to the `train_nn` function from Assignment 1. We'll factor out a few operations into helpers, so that the basic structure is clearer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Helper functions for training\n",
    "def train_batch(session, batch, alpha):\n",
    "    # Feed last column as targets\n",
    "    feed_dict = {ids_:batch[:,:-1],\n",
    "                 y_:batch[:,-1],\n",
    "                 alpha_:alpha}\n",
    "    c, _ = session.run([train_loss_, train_step_],\n",
    "                       feed_dict=feed_dict)\n",
    "    return c\n",
    "\n",
    "## We'll use this to generate the batches.\n",
    "utils.batch_generator??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a single epoch should take around 6-7 minutes on a 2-core Cloud Compute instance, or around 30 seconds on a GTX 980 GPU. You should get good results after just 2-3 epochs (train loss around 3.5).\n",
    "\n",
    "Rememer that the cost printed is the average *training* loss. Since we're using the sampled softmax, this will be an underestimate of the true loss. We'll need to do a separate run over the data to compute perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[epoch 1] seen 0 minibatches\n",
      "[epoch 1] seen 1000 minibatches\n",
      "[epoch 1] seen 2000 minibatches\n",
      "[epoch 1] seen 3000 minibatches\n",
      "[epoch 1] seen 4000 minibatches\n",
      "[epoch 1] seen 5000 minibatches\n",
      "[epoch 1] seen 6000 minibatches\n",
      "[epoch 1] seen 7000 minibatches\n",
      "[epoch 1] seen 8000 minibatches\n",
      "[epoch 1] seen 9000 minibatches\n",
      "[epoch 1] seen 10000 minibatches\n",
      "[epoch 1] Completed 10255 minibatches in 0:02:17\n",
      "[epoch 1] Average cost: 5.708\n",
      "\n",
      "[epoch 2] seen 0 minibatches\n",
      "[epoch 2] seen 1000 minibatches\n",
      "[epoch 2] seen 2000 minibatches\n",
      "[epoch 2] seen 3000 minibatches\n",
      "[epoch 2] seen 4000 minibatches\n",
      "[epoch 2] seen 5000 minibatches\n",
      "[epoch 2] seen 6000 minibatches\n",
      "[epoch 2] seen 7000 minibatches\n",
      "[epoch 2] seen 8000 minibatches\n",
      "[epoch 2] seen 9000 minibatches\n",
      "[epoch 2] seen 10000 minibatches\n",
      "[epoch 2] Completed 10255 minibatches in 0:02:14\n",
      "[epoch 2] Average cost: 4.064\n",
      "\n",
      "[epoch 3] seen 0 minibatches\n",
      "[epoch 3] seen 1000 minibatches\n",
      "[epoch 3] seen 2000 minibatches\n",
      "[epoch 3] seen 3000 minibatches\n",
      "[epoch 3] seen 4000 minibatches\n",
      "[epoch 3] seen 5000 minibatches\n",
      "[epoch 3] seen 6000 minibatches\n",
      "[epoch 3] seen 7000 minibatches\n",
      "[epoch 3] seen 8000 minibatches\n",
      "[epoch 3] seen 9000 minibatches\n",
      "[epoch 3] seen 10000 minibatches\n",
      "[epoch 3] Completed 10255 minibatches in 0:02:17\n",
      "[epoch 3] Average cost: 3.754\n"
     ]
    }
   ],
   "source": [
    "# One epoch = one pass through the training data\n",
    "num_epochs = 3\n",
    "batch_size = 100\n",
    "alpha = 0.5  # learning rate\n",
    "print_every = 1000\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "session = tf.compat.v1.Session()\n",
    "session.run(init_)\n",
    "\n",
    "t0 = time.time()\n",
    "#for epoch in xrange(1,num_epochs+1):\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    t0_epoch = time.time()\n",
    "    epoch_cost = 0.0\n",
    "    total_batches = 0\n",
    "    print(\"\")\n",
    "    for i, batch in enumerate(utils.batch_generator(train_windows, batch_size)):\n",
    "        if (i % print_every == 0):\n",
    "            print('[epoch %d] seen %d minibatches' % (epoch,i))\n",
    "        \n",
    "        epoch_cost += train_batch(session, batch, alpha)\n",
    "        total_batches = i + 1\n",
    "\n",
    "    avg_cost = epoch_cost / total_batches\n",
    "\n",
    "    print('[epoch %d] Completed %d minibatches in %s' % (epoch, i, utils.pretty_timedelta(since=t0_epoch)))\n",
    "    print('[epoch %d] Average cost: %.03f' % (epoch, avg_cost,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring\n",
    "\n",
    "We'll score our model the same as the n-gram model, by computing perplexity over the dev set. Recall that perplexity is just the exponentiated average cross-entropy loss:\n",
    "\n",
    "$$ \\text{Perplexity} = \\left( \\prod_i \\frac{1}{Q(x_i)} \\right)^{1/N} = \\left( \\prod_i 2^{- \\log_2 Q(x_i)} \\right)^{1/N} = 2^{\\left(\\frac{1}{N} \\sum_i -\\log_2 Q(x_i)\\right)} = 2^{\\tilde{CE}(P,Q)}$$\n",
    "\n",
    "In practice TF uses the natural log, so the loss will be scaled by a factor of $\\ln 2$ - but the base cancels and the perplexity scores will be the same.\n",
    "\n",
    "Note that below we use `loss_`, which is the cross-entropy loss with the full softmax. Because this is so much slower than the sampled softmax, on a slower machine the scoring step might take as long or longer than training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_batch(session, batch):\n",
    "    feed_dict = {ids_:batch[:,:-1],\n",
    "                 y_:batch[:,-1]}\n",
    "    return session.run(loss_, feed_dict=feed_dict)\n",
    "\n",
    "def score_dataset(data):\n",
    "    total_cost = 0.0\n",
    "    total_batches = 0\n",
    "    for batch in utils.batch_generator(data, 1000):\n",
    "        total_cost += score_batch(session, batch)\n",
    "        total_batches += 1\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set perplexity: 265.227\n",
      "Test set perplexity: 304.082\n"
     ]
    }
   ],
   "source": [
    "print(\"Train set perplexity: %.03f\" % np.exp(score_dataset(train_windows)))\n",
    "print(\"Test set perplexity: %.03f\" % np.exp(score_dataset(test_windows)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty good! Note that these numbers aren't directly comparable to the literature, since we made the task easier by lowercasing everything, canonicalizing digits, and treating a fairly large number of words as an `<unk>` token.\n",
    "\n",
    "We can remove some of this handicap by looking at our perplexity on non-`<unk>` target words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered test set perplexity: 384.198\n"
     ]
    }
   ],
   "source": [
    "filtered_test_windows = test_windows[test_windows[:,-1] != vocab.UNK_ID]\n",
    "print(\"Filtered test set perplexity: %.03f\" % np.exp(score_dataset(filtered_test_windows)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "\n",
    "We can sample sentences from the model much as we did with n-gram models. We'll use the `pred_random_` op that we defined before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> <s> even language is provided something in more <unk> to the <unk> <unk> , let up the on indeed , we are <unk> <unk> , no one come expensive in a\n",
      "[33 tokens; log P(seq): 4.23, per-token: 0.14]\n",
      "\n",
      "<s> <s> <s> part was a `` maintain massachusetts , <unk> and me , any locked fuller theatrical of slave orchestra . <s>\n",
      "[23 tokens; log P(seq): 5.58, per-token: 0.27]\n",
      "\n",
      "<s> <s> <s> <unk> design man that a year <unk> , you . <s>\n",
      "[14 tokens; log P(seq): 4.65, per-token: 0.39]\n",
      "\n",
      "<s> <s> <s> <unk> <unk> , the <unk> of of shapes she did the s. <unk> <unk> massachusetts , for the projects `` another coverage , <unk> encourage the beach and <unk> <unk>\n",
      "[33 tokens; log P(seq): 4.34, per-token: 0.14]\n",
      "\n",
      "<s> <s> <s> it , and <unk> <unk> <unk> <unk> , <unk> won't have to appears on the fair virus , born , putting the smooth support for furnished with , it has\n",
      "[33 tokens; log P(seq): 4.42, per-token: 0.14]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_next(session, seq):\n",
    "    feed_dict={ids_:np.array([seq[-N:]])}\n",
    "    next_id = session.run(pred_random_, feed_dict=feed_dict)\n",
    "    return next_id[0][0]\n",
    "\n",
    "def score_seq(session, seq):\n",
    "    # Some gymnastics to generate windows for scoring\n",
    "    windows = [seq[i:i+N+1] for i in range(len(seq)-(N+1))]\n",
    "    return score_batch(session, np.array(windows))\n",
    "\n",
    "max_length = 30\n",
    "num_sentences = 5\n",
    "\n",
    "for _ in range(num_sentences):\n",
    "    seq = [vocab.word_to_id[\"<s>\"]]*N  # init N+1-gram model\n",
    "    for i in range(max_length):\n",
    "        seq.append(predict_next(session, seq))\n",
    "        if seq[-1] == vocab.word_to_id[\"<s>\"]: break\n",
    "    print(\" \".join(vocab.ids_to_words(seq)))\n",
    "    score = score_seq(session, seq)\n",
    "    print (\"[%d tokens; log P(seq): %.02f, per-token: %.02f]\" % (len(seq), score, score/(len(seq)-2)))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
