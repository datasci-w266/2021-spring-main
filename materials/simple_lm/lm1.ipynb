{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling I\n",
    "\n",
    "In this week's section, we'll explore basic techniques for language modeling. \n",
    "\n",
    "We'll first dig into word distributions to illustrate the sparsity problem and the difficulty of prediction.\n",
    "\n",
    "Then, we'll look at a simple N-gram language model and generate some sample sentences. In the homework, you'll extend this by implementing smoothing techniques that improve performance, and perform some more detailed analysis of the model.\n",
    "\n",
    "**Note:** if you're viewing this in-browser, jump over to the NBViewer link. This will get around GitHub's JavaScript block and properly render the plots:  \n",
    "http://nbviewer.jupyter.org/github/datasci-w266/2021-spring-main/blob/master/materials/simple_lm/lm1.ipynb\n",
    "\n",
    "We'll use [Bokeh](https://bokeh.pydata.org/en/latest/docs/user_guide/quickstart.html) for plotting some data in this notebook. It's similar to `matplotlib`, but renders JavaScript-based plots that support more interactive control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\": \"kLr4fYcqcSpbuI95brIH3vnnYCquzzSxHPU6XGQCIkQRGJwhg0StNbj1eegrHs12\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\": \"xIGPmVtaOm+z0BqfSOMn4lOR6ciex448GIKG4eE61LsAvmGj48XcMQZtKcE/UXZe\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\": \"Dc9u1wF/0zApGIWoBbH77iWEHtdmkuYWG839Uzmv8y8yBLXebjO9ZnERsde5Ln/P\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\": \"cT9JaBz7GiRXdENrJLZNSC6eMNF3nh3fa5fTF51Svp+ukxPdwcU5kGXGPBgDCa2j\"};\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\": \"kLr4fYcqcSpbuI95brIH3vnnYCquzzSxHPU6XGQCIkQRGJwhg0StNbj1eegrHs12\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\": \"xIGPmVtaOm+z0BqfSOMn4lOR6ciex448GIKG4eE61LsAvmGj48XcMQZtKcE/UXZe\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\": \"Dc9u1wF/0zApGIWoBbH77iWEHtdmkuYWG839Uzmv8y8yBLXebjO9ZnERsde5Ln/P\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\": \"cT9JaBz7GiRXdENrJLZNSC6eMNF3nh3fa5fTF51Svp+ukxPdwcU5kGXGPBgDCa2j\"};\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard python helper libraries.\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os, sys, time\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "# Numerical manipulation libraries.\n",
    "import numpy as np\n",
    "from scipy import stats, optimize\n",
    "\n",
    "# NLTK is the Natural Language Toolkit, and contains several language datasets\n",
    "# as well as implementations of many popular NLP algorithms.\n",
    "# HINT: You should look at what is available here when thinking about your project!\n",
    "import nltk\n",
    "\n",
    "# Helper libraries (see the corresponding py files in this notebook's directory).\n",
    "from w266_common import utils, vocabulary\n",
    "import segment\n",
    "\n",
    "utils.require_package(\"tqdm\")  # for nice progress bars\n",
    "from tqdm import tqdm as ProgressBar\n",
    "\n",
    "# Bokeh for plotting.\n",
    "utils.require_package(\"bokeh\")\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool\n",
    "bp.output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Statistics\n",
    "\n",
    "NLTK includes a number of corpora that we can experiment with for this exercise. Different types of text can have very different N-gram distributions, and some are more difficult to model than others.\n",
    "\n",
    "Let's start with the [Brown corpus](http://www.essex.ac.uk/linguistics/external/clmt/w3c/corpus_ling/content/corpora/list/private/brown/brown.html), the first major computer-readable linguistic corpus. It consists of around 1 million words of American English, sampled from 15 different text categories ranging from news text to academic articles to popular fiction.\n",
    "\n",
    "If you haven't yet run `nltk.download()`, the cell below will download the Brown corpus for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/mhbutler/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by looking at the all the words in the corpus, and looking at some basic statistics.\n",
    "\n",
    "We've built a helper class, `Vocabulary`, that ingests a list of words, counts their frequencies, and assigns each one a numerical ID that will be useful later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1161192it [00:06, 188470.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 48,174\n",
      "Most common unigrams:\n",
      "\"the\": 69,971\n",
      "\",\": 58,334\n",
      "\".\": 49,346\n",
      "\"of\": 36,412\n",
      "\"and\": 28,853\n",
      "\"to\": 26,158\n",
      "\"a\": 23,195\n",
      "\"in\": 21,337\n",
      "\"that\": 10,594\n",
      "\"is\": 10,109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = nltk.corpus.brown\n",
    "\n",
    "# \"canonicalize_word\" performs a few tweaks to the token stream of\n",
    "# the corpus.  For example, it replaces digits with DG allowing numbers\n",
    "# to aggregate together when we count them below.\n",
    "# You can read the details in utils.py if you're really curious.\n",
    "token_feed = (utils.canonicalize_word(w) for w in corpus.words())\n",
    "\n",
    "# Collect counts of tokens and assign wordids.\n",
    "vocab = vocabulary.Vocabulary(token_feed, progressbar=ProgressBar)\n",
    "print(\"Vocabulary size: {:,}\".format(vocab.size))\n",
    "\n",
    "# Print out some (debugging) statistics to make sure everything went\n",
    "# as we expected.  (Unsurprisingly, you should see \"the\" as the most popular word.)\n",
    "print(\"Most common unigrams:\")\n",
    "for word, count in vocab.unigram_counts.most_common(10):\n",
    "    print(\"\\\"{:s}\\\": {:,}\".format(word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Vocabulary.unigram_counts` is a dictionary (actually, `collections.Counter`) of the unigram frequencies $c(w)$. Let's look at a plot of the top frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"59721777-a637-4bc6-bc7b-12c7196ad8fb\" data-root-id=\"1003\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"6f5aa23d-01ce-429c-8ee0-a19f3b625089\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1012\"}],\"center\":[{\"id\":\"1014\"},{\"id\":\"1018\"}],\"left\":[{\"id\":\"1015\"}],\"plot_height\":400,\"plot_width\":800,\"renderers\":[{\"id\":\"1025\"}],\"title\":{\"id\":\"1028\"},\"toolbar\":{\"id\":\"1019\"},\"x_range\":{\"id\":\"1004\"},\"x_scale\":{\"id\":\"1008\"},\"y_range\":{\"id\":\"1006\"},\"y_scale\":{\"id\":\"1010\"}},\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"fill_color\":{\"value\":\"firebrick\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.8},\"x\":{\"field\":\"x\"}},\"id\":\"1024\",\"type\":\"VBar\"},{\"attributes\":{\"factors\":[\"the\",\",\",\".\",\"of\",\"and\",\"to\",\"a\",\"in\",\"that\",\"is\",\"was\",\"he\",\"for\",\"``\",\"''\",\"it\",\"with\",\"as\",\"his\",\"on\"]},\"id\":\"1004\",\"type\":\"FactorRange\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.8},\"x\":{\"field\":\"x\"}},\"id\":\"1023\",\"type\":\"VBar\"},{\"attributes\":{\"data\":{\"top\":[69971,58334,49346,36412,28853,26158,23195,21337,10594,10109,9815,9548,9489,8837,8789,8760,7289,7253,6996,6741],\"x\":[\"the\",\",\",\".\",\"of\",\"and\",\"to\",\"a\",\"in\",\"that\",\"is\",\"was\",\"he\",\"for\",\"``\",\"''\",\"it\",\"with\",\"as\",\"his\",\"on\"]},\"selected\":{\"id\":\"1033\"},\"selection_policy\":{\"id\":\"1034\"}},\"id\":\"1021\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"callback\":null,\"mode\":\"vline\",\"tooltips\":[[\"word\",\"@x\"],[\"count\",\"@top\"]]},\"id\":\"1002\",\"type\":\"HoverTool\"},{\"attributes\":{},\"id\":\"1008\",\"type\":\"CategoricalScale\"},{\"attributes\":{\"source\":{\"id\":\"1021\"}},\"id\":\"1026\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1013\",\"type\":\"CategoricalTicker\"},{\"attributes\":{},\"id\":\"1010\",\"type\":\"LinearScale\"},{\"attributes\":{\"axis_label\":\"Count(w)\",\"formatter\":{\"id\":\"1029\"},\"ticker\":{\"id\":\"1016\"}},\"id\":\"1015\",\"type\":\"LinearAxis\"},{\"attributes\":{\"start\":0},\"id\":\"1006\",\"type\":\"DataRange1d\"},{\"attributes\":{\"data_source\":{\"id\":\"1021\"},\"glyph\":{\"id\":\"1022\"},\"hover_glyph\":{\"id\":\"1024\"},\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1023\"},\"selection_glyph\":null,\"view\":{\"id\":\"1026\"}},\"id\":\"1025\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"1016\",\"type\":\"BasicTicker\"},{\"attributes\":{\"formatter\":{\"id\":\"1031\"},\"ticker\":{\"id\":\"1013\"}},\"id\":\"1012\",\"type\":\"CategoricalAxis\"},{\"attributes\":{\"axis\":{\"id\":\"1015\"},\"dimension\":1,\"ticker\":null},\"id\":\"1018\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1033\",\"type\":\"Selection\"},{\"attributes\":{\"axis\":{\"id\":\"1012\"},\"ticker\":null},\"id\":\"1014\",\"type\":\"Grid\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1002\"}]},\"id\":\"1019\",\"type\":\"Toolbar\"},{\"attributes\":{\"text\":\"\"},\"id\":\"1028\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"1031\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{},\"id\":\"1029\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.8},\"x\":{\"field\":\"x\"}},\"id\":\"1022\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"1034\",\"type\":\"UnionRenderers\"}],\"root_ids\":[\"1003\"]},\"title\":\"Bokeh Application\",\"version\":\"2.1.1\"}};\n",
       "  var render_items = [{\"docid\":\"6f5aa23d-01ce-429c-8ee0-a19f3b625089\",\"root_ids\":[\"1003\"],\"roots\":{\"1003\":\"59721777-a637-4bc6-bc7b-12c7196ad8fb\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1003"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "words, counts = zip(*vocab.unigram_counts.most_common(20))\n",
    "\n",
    "hover = HoverTool(tooltips=[(\"word\", \"@x\"), (\"count\", \"@top\")], mode=\"vline\")\n",
    "fig = bp.figure(x_range=words, plot_width=800, plot_height=400, tools=[hover])\n",
    "fig.vbar(x=words, width=0.8, top=counts, hover_fill_color=\"firebrick\")\n",
    "fig.y_range.start = 0\n",
    "fig.yaxis.axis_label = \"Count(w)\"\n",
    "bp.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that it falls off very quickly! It also flattens out a lot after the initial dip.  Recall that word frequencies tend to follow **Zipf's law**, in that they are rougly proportional to $\\frac{1}{\\mathrm{rank}(w)}$, where rank = 1 for the most common word, 2 for the second-most, etc. We can test this directly with a numerical fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power law exponent: β = -1.32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"889d4795-51bf-40c4-8572-86904221a410\" data-root-id=\"1083\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"70cc99ec-1ba7-4f83-a97b-d7cadb2818be\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1092\"}],\"center\":[{\"id\":\"1094\"},{\"id\":\"1098\"}],\"left\":[{\"id\":\"1095\"}],\"plot_height\":400,\"plot_width\":800,\"renderers\":[{\"id\":\"1117\"},{\"id\":\"1124\"}],\"title\":{\"id\":\"1135\"},\"toolbar\":{\"id\":\"1106\"},\"x_range\":{\"id\":\"1084\"},\"x_scale\":{\"id\":\"1088\"},\"y_range\":{\"id\":\"1086\"},\"y_scale\":{\"id\":\"1090\"}},\"id\":\"1083\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"axis\":{\"id\":\"1095\"},\"dimension\":1,\"ticker\":null},\"id\":\"1098\",\"type\":\"Grid\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"Orange\",\"line_width\":2,\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1123\",\"type\":\"Line\"},{\"attributes\":{\"formatter\":{\"id\":\"1138\"},\"ticker\":{\"id\":\"1093\"}},\"id\":\"1092\",\"type\":\"CategoricalAxis\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1105\",\"type\":\"BoxAnnotation\"},{\"attributes\":{},\"id\":\"1143\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1144\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"text\":\"\"},\"id\":\"1135\",\"type\":\"Title\"},{\"attributes\":{\"factors\":[\"the\",\",\",\".\",\"of\",\"and\",\"to\",\"a\",\"in\",\"that\",\"is\",\"was\",\"he\",\"for\",\"``\",\"''\",\"it\",\"with\",\"as\",\"his\",\"on\",\"be\",\";\",\"at\",\"by\",\"i\",\"this\",\"had\",\"?\",\"not\",\"are\",\"but\",\"from\",\"or\",\"have\",\"an\",\"they\",\"which\",\"--\",\"one\",\"you\",\"were\",\"her\",\"all\",\"she\",\"there\",\"would\",\"their\",\"we\",\"him\",\"been\",\")\",\"has\",\"(\",\"when\",\"who\",\"will\",\"more\",\"if\",\"no\",\"out\",\"DG\",\"so\",\"said\",\"DGDGDGDG\",\"DGDG\",\"what\",\"up\",\"its\",\"about\",\":\",\"into\",\"than\",\"them\",\"can\",\"only\",\"other\",\"new\",\"some\",\"could\",\"time\",\"!\",\"these\",\"two\",\"may\",\"then\",\"do\",\"first\",\"any\",\"my\",\"now\",\"such\",\"like\",\"our\",\"over\",\"man\",\"me\",\"even\",\"most\",\"made\",\"also\"]},\"id\":\"1084\",\"type\":\"FactorRange\"},{\"attributes\":{},\"id\":\"1136\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"1088\",\"type\":\"CategoricalScale\"},{\"attributes\":{\"callback\":null,\"mode\":\"vline\",\"renderers\":[{\"id\":\"1117\"}],\"tooltips\":[[\"word\",\"@x\"],[\"count\",\"@top\"]]},\"id\":\"1119\",\"type\":\"HoverTool\"},{\"attributes\":{},\"id\":\"1090\",\"type\":\"LinearScale\"},{\"attributes\":{\"end\":83965.2,\"start\":0},\"id\":\"1086\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"1099\",\"type\":\"PanTool\"},{\"attributes\":{\"axis\":{\"id\":\"1092\"},\"grid_line_alpha\":0.5,\"ticker\":null},\"id\":\"1094\",\"type\":\"Grid\"},{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.8},\"x\":{\"field\":\"x\"}},\"id\":\"1114\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"1093\",\"type\":\"CategoricalTicker\"},{\"attributes\":{},\"id\":\"1138\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{\"axis_label\":\"Count(w)\",\"formatter\":{\"id\":\"1136\"},\"ticker\":{\"id\":\"1096\"}},\"id\":\"1095\",\"type\":\"LinearAxis\"},{\"attributes\":{\"source\":{\"id\":\"1121\"}},\"id\":\"1125\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1096\",\"type\":\"BasicTicker\"},{\"attributes\":{\"data_source\":{\"id\":\"1121\"},\"glyph\":{\"id\":\"1122\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1123\"},\"selection_glyph\":null,\"view\":{\"id\":\"1125\"}},\"id\":\"1124\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.8},\"x\":{\"field\":\"x\"}},\"id\":\"1115\",\"type\":\"VBar\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1099\"},{\"id\":\"1100\"},{\"id\":\"1101\"},{\"id\":\"1102\"},{\"id\":\"1103\"},{\"id\":\"1104\"},{\"id\":\"1119\"}]},\"id\":\"1106\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"1141\",\"type\":\"Selection\"},{\"attributes\":{\"source\":{\"id\":\"1113\"}},\"id\":\"1118\",\"type\":\"CDSView\"},{\"attributes\":{\"data\":{\"top\":{\"__ndarray__\":\"AAAAADAV8UAAAAAAwHvsQAAAAABAGOhAAAAAAIDH4UAAAAAAQC3cQAAAAACAi9lAAAAAAMCm1kAAAAAAQNbUQAAAAAAAscRAAAAAAIC+w0AAAAAAgCvDQAAAAAAApsJAAAAAAICIwkAAAAAAgELBQAAAAACAKsFAAAAAAAAcwUAAAAAAAHm8QAAAAAAAVbxAAAAAAABUu0AAAAAAAFW6QAAAAAAA6bhAAAAAAAC+tUAAAAAAAPy0QAAAAAAAurRAAAAAAAAstEAAAAAAABm0QAAAAAAADbRAAAAAAABVskAAAAAAAAKyQAAAAAAAKrFAAAAAAAAdsUAAAAAAABKxQAAAAAAAbrBAAAAAAADMrkAAAAAAADitQAAAAAAASKxAAAAAAADSq0AAAAAAANCqQAAAAAAAuKlAAAAAAACsqUAAAAAAAKipQAAAAAAAuKdAAAAAAAByp0AAAAAAAFimQAAAAAAAUKVAAAAAAAA0pUAAAAAAANqkQAAAAAAAuKRAAAAAAAB2pEAAAAAAAFCjQAAAAAAARKNAAAAAAAAKo0AAAAAAAAajQAAAAAAANqJAAAAAAACYoUAAAAAAAIqhQAAAAAAATqFAAAAAAAAsoUAAAAAAALagQAAAAAAAYqBAAAAAAABYn0AAAAAAAASfQAAAAAAApJ5AAAAAAAAgnkAAAAAAAOydQAAAAAAA0J1AAAAAAACInUAAAAAAAAidQAAAAAAAXJxAAAAAAAAMnEAAAAAAAPybQAAAAAAA+JtAAAAAAADwm0AAAAAAALCbQAAAAAAAUJtAAAAAAACYmkAAAAAAAIyZQAAAAAAASJlAAAAAAAAEmUAAAAAAAPiYQAAAAAAA8JhAAAAAAACUmEAAAAAAABCWQAAAAAAA6JVAAAAAAACQlUAAAAAAAEyVQAAAAAAARJVAAAAAAAAAlUAAAAAAAJiUQAAAAAAAiJRAAAAAAABclEAAAAAAADCUQAAAAAAAkJNAAAAAAABQk0AAAAAAANySQAAAAAAAdJJAAAAAAABIkkAAAAAAABySQAAAAAAAlJFAAAAAAAC0kEA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[100]},\"x\":[\"the\",\",\",\".\",\"of\",\"and\",\"to\",\"a\",\"in\",\"that\",\"is\",\"was\",\"he\",\"for\",\"``\",\"''\",\"it\",\"with\",\"as\",\"his\",\"on\",\"be\",\";\",\"at\",\"by\",\"i\",\"this\",\"had\",\"?\",\"not\",\"are\",\"but\",\"from\",\"or\",\"have\",\"an\",\"they\",\"which\",\"--\",\"one\",\"you\",\"were\",\"her\",\"all\",\"she\",\"there\",\"would\",\"their\",\"we\",\"him\",\"been\",\")\",\"has\",\"(\",\"when\",\"who\",\"will\",\"more\",\"if\",\"no\",\"out\",\"DG\",\"so\",\"said\",\"DGDGDGDG\",\"DGDG\",\"what\",\"up\",\"its\",\"about\",\":\",\"into\",\"than\",\"them\",\"can\",\"only\",\"other\",\"new\",\"some\",\"could\",\"time\",\"!\",\"these\",\"two\",\"may\",\"then\",\"do\",\"first\",\"any\",\"my\",\"now\",\"such\",\"like\",\"our\",\"over\",\"man\",\"me\",\"even\",\"most\",\"made\",\"also\"]},\"selected\":{\"id\":\"1141\"},\"selection_policy\":{\"id\":\"1142\"}},\"id\":\"1113\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1100\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"1142\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"fill_color\":{\"value\":\"firebrick\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.8},\"x\":{\"field\":\"x\"}},\"id\":\"1116\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"1104\",\"type\":\"HelpTool\"},{\"attributes\":{\"data_source\":{\"id\":\"1113\"},\"glyph\":{\"id\":\"1114\"},\"hover_glyph\":{\"id\":\"1116\"},\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1115\"},\"selection_glyph\":null,\"view\":{\"id\":\"1118\"}},\"id\":\"1117\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"overlay\":{\"id\":\"1105\"}},\"id\":\"1101\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"1102\",\"type\":\"SaveTool\"},{\"attributes\":{\"data\":{\"x\":[\"the\",\",\",\".\",\"of\",\"and\",\"to\",\"a\",\"in\",\"that\",\"is\",\"was\",\"he\",\"for\",\"``\",\"''\",\"it\",\"with\",\"as\",\"his\",\"on\",\"be\",\";\",\"at\",\"by\",\"i\",\"this\",\"had\",\"?\",\"not\",\"are\",\"but\",\"from\",\"or\",\"have\",\"an\",\"they\",\"which\",\"--\",\"one\",\"you\",\"were\",\"her\",\"all\",\"she\",\"there\",\"would\",\"their\",\"we\",\"him\",\"been\",\")\",\"has\",\"(\",\"when\",\"who\",\"will\",\"more\",\"if\",\"no\",\"out\",\"DG\",\"so\",\"said\",\"DGDGDGDG\",\"DGDG\",\"what\",\"up\",\"its\",\"about\",\":\",\"into\",\"than\",\"them\",\"can\",\"only\",\"other\",\"new\",\"some\",\"could\",\"time\",\"!\",\"these\",\"two\",\"may\",\"then\",\"do\",\"first\",\"any\",\"my\",\"now\",\"such\",\"like\",\"our\",\"over\",\"man\",\"me\",\"even\",\"most\",\"made\",\"also\"],\"y\":{\"__ndarray__\":\"AAAAAJSxE0EAAAAAAHv/QAAAAACgaPJAAAAAACAp6UAAAAAAQLriQAAAAABAbd1AAAAAAAD/10AAAAAAABzUQAAAAAAANdFAAAAAAIDvzUAAAAAAgGPKQAAAAAAAhcdAAAAAAAAoxUAAAAAAAC7DQAAAAACAgcFAAAAAAIASwEAAAAAAAKu9QAAAAAAAgrtAAAAAAACbuUAAAAAAAO23QAAAAAAAbrZAAAAAAAAXtUAAAAAAAOOzQAAAAAAAzLJAAAAAAADPsUAAAAAAAOmwQAAAAAAAFrBAAAAAAACorkAAAAAAAEStQAAAAAAA/KtAAAAAAADMqkAAAAAAALKpQAAAAAAAqqhAAAAAAAC2p0AAAAAAANKmQAAAAAAA/KVAAAAAAAA0pUAAAAAAAHikQAAAAAAAxqNAAAAAAAAgo0AAAAAAAIKiQAAAAAAA7qFAAAAAAABgoUAAAAAAANygQAAAAAAAXqBAAAAAAADIn0AAAAAAAOSeQAAAAAAADJ5AAAAAAAA8nUAAAAAAAHicQAAAAAAAvJtAAAAAAAAIm0AAAAAAAFyaQAAAAAAAuJlAAAAAAAAYmUAAAAAAAICYQAAAAAAA8JdAAAAAAABkl0AAAAAAAOCWQAAAAAAAXJZAAAAAAADglUAAAAAAAGyVQAAAAAAA+JRAAAAAAACIlEAAAAAAACCUQAAAAAAAuJNAAAAAAABUk0AAAAAAAPSSQAAAAAAAmJJAAAAAAAA8kkAAAAAAAOiRQAAAAAAAlJFAAAAAAABAkUAAAAAAAPSQQAAAAAAApJBAAAAAAABckEAAAAAAABSQQAAAAAAAoI9AAAAAAAAYj0AAAAAAAJCOQAAAAAAAEI5AAAAAAACYjUAAAAAAACCNQAAAAAAAqIxAAAAAAAA4jEAAAAAAAMiLQAAAAAAAWItAAAAAAADwikAAAAAAAIiKQAAAAAAAKIpAAAAAAADIiUAAAAAAAGiJQAAAAAAACIlAAAAAAACwiEAAAAAAAFiIQAAAAAAACIhAAAAAAACwh0AAAAAAAGCHQAAAAAAAEIdAAAAAAADAhkA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[100]}},\"selected\":{\"id\":\"1143\"},\"selection_policy\":{\"id\":\"1144\"}},\"id\":\"1121\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1103\",\"type\":\"ResetTool\"},{\"attributes\":{\"line_color\":\"Orange\",\"line_width\":2,\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1122\",\"type\":\"Line\"}],\"root_ids\":[\"1083\"]},\"title\":\"Bokeh Application\",\"version\":\"2.1.1\"}};\n",
       "  var render_items = [{\"docid\":\"70cc99ec-1ba7-4f83-a97b-d7cadb2818be\",\"root_ids\":[\"1083\"],\"roots\":{\"1083\":\"889d4795-51bf-40c4-8572-86904221a410\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1083"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This next line splits the pairs of <word, count> in the vocabulary into two lists:\n",
    "# 1.  a list of words (types)\n",
    "# 2.  a list of counts (per type)\n",
    "# with the property that the ith word in the list has its corresponding count in the ith counts.\n",
    "words, counts = zip(*vocab.unigram_counts.most_common(vocab.size))\n",
    "counts = np.array(counts, dtype=float)  # Avoid integer math.\n",
    "rank = 1 + np.arange(len(counts))  # rank is an array of [1, 2, 3, 4, ..., num_types]\n",
    "N = np.sum(counts)  # N = total # of tokens seen.\n",
    "p = counts / N  # p is an array the length of `words`.  #_times_word_seen / total_#_words\n",
    "\n",
    "# Fit a power law curve to the histogram above.\n",
    "# Optimize least-squares in log space.\n",
    "# See http://nlp.stanford.edu/IR-book/html/htmledition/zipfs-law-modeling-the-distribution-of-terms-1.html\n",
    "# fit_func = lambda c: (np.log(c[0]*p) - np.log(c[0] * rank**c[1]))\n",
    "fit_func = lambda c: (np.log(p) - np.log(c[0] * rank**c[1]))\n",
    "(a,b), _ = optimize.leastsq(fit_func, np.array([p[0], -1.0]))\n",
    "print(u\"Power law exponent: \\u03B2 = {:.02f}\".format(b))\n",
    "p_pred = (a * rank**b) / sum(a * rank**b)  # predict probabilities\n",
    "c_pred = N * p_pred  # predict counts\n",
    "\n",
    "\n",
    "# Plot counts, with fit curve.\n",
    "nplot = 100\n",
    "fig = bp.figure(x_range=words[:nplot], plot_width=800, plot_height=400)\n",
    "bars = fig.vbar(x=words[:nplot], width=0.8, top=counts[:nplot], hover_fill_color=\"firebrick\")\n",
    "fig.add_tools(HoverTool(tooltips=[(\"word\", \"@x\"), (\"count\", \"@top\")], renderers=[bars], mode=\"vline\"))\n",
    "fig.line(x=words[:nplot], y=np.round(c_pred)[:nplot], color=\"Orange\", line_width=2)\n",
    "fig.y_range.start = 0\n",
    "fig.y_range.end = 1.2*max(counts)\n",
    "fig.yaxis.axis_label = \"Count(w)\"\n",
    "fig.xgrid.grid_line_alpha = 0.5\n",
    "bp.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why should we care about the form of this distribution? Power-law distributions like [Zipf's law](https://en.wikipedia.org/wiki/Zipf%E2%80%93Mandelbrot_law) have a **long tail**, which means that a large fraction of the tokens (words on the page) belong to types (words in the dictionary) that appear quite rarely. \n",
    "\n",
    "We can look at this directly by plotting the cumulative distribution function, *as a function of the count* $c(w)$:\n",
    "\n",
    "$$ f(k) = \\sum_{i \\in \\text{words}}^{|\\text{corpus}|} \\mathbf{1}\\{c(w_i) <= k\\} = \\sum_{j \\in \\text{types}}^{|V|} c(w_j)\\ \\mathbf{1}\\{c(w_j) <= k\\} $$\n",
    "\n",
    "This asks: for a given word on the page, how likely is $c(w) <= k$?\n",
    "\n",
    "The Brown corpus has about 1 million words (tokens), about 7% of which are the token \"the\" and 6% of which are commas. So, we'll plot our data on a log scale so that we can see what's going on for $k= 1, ..., 100$, as well as the higher counts ($c(\\text{the}) = 6.9 \\cdot 10^4$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"67c64e5f-4498-4bbf-8c78-266f2546be20\" data-root-id=\"1211\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"e52bacaf-fa3a-4eee-a6b1-490f5d5fab60\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1222\"}],\"center\":[{\"id\":\"1225\"},{\"id\":\"1229\"}],\"left\":[{\"id\":\"1226\"}],\"plot_height\":400,\"plot_width\":800,\"renderers\":[{\"id\":\"1247\"},{\"id\":\"1252\"}],\"title\":{\"id\":\"1212\"},\"toolbar\":{\"id\":\"1237\"},\"x_range\":{\"id\":\"1214\"},\"x_scale\":{\"id\":\"1218\"},\"y_range\":{\"id\":\"1216\"},\"y_scale\":{\"id\":\"1220\"}},\"id\":\"1211\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"ticker\":null},\"id\":\"1278\",\"type\":\"LogTickFormatter\"},{\"attributes\":{},\"id\":\"1231\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"text\":\"Cumulative Word Counts\"},\"id\":\"1212\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"1218\",\"type\":\"LogScale\"},{\"attributes\":{\"start\":0},\"id\":\"1216\",\"type\":\"DataRange1d\"},{\"attributes\":{\"data_source\":{\"id\":\"1244\"},\"glyph\":{\"id\":\"1245\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1246\"},\"selection_glyph\":null,\"view\":{\"id\":\"1248\"}},\"id\":\"1247\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"callback\":null,\"mode\":\"vline\",\"renderers\":[{\"id\":\"1247\"}],\"tooltips\":[[\"count\",\"@x\"],[\"CDF\",\"@y\"]]},\"id\":\"1254\",\"type\":\"HoverTool\"},{\"attributes\":{},\"id\":\"1214\",\"type\":\"DataRange1d\"},{\"attributes\":{\"axis_label\":\"Count (log-scale)\",\"formatter\":{\"id\":\"1278\"},\"ticker\":{\"id\":\"1223\"}},\"id\":\"1222\",\"type\":\"LogAxis\"},{\"attributes\":{},\"id\":\"1220\",\"type\":\"LinearScale\"},{\"attributes\":{\"axis\":{\"id\":\"1226\"},\"dimension\":1,\"ticker\":null},\"id\":\"1229\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1235\",\"type\":\"HelpTool\"},{\"attributes\":{},\"id\":\"1282\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1281\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1284\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"num_minor_ticks\":10},\"id\":\"1223\",\"type\":\"LogTicker\"},{\"attributes\":{\"source\":{\"id\":\"1244\"}},\"id\":\"1248\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1276\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"overlay\":{\"id\":\"1236\"}},\"id\":\"1232\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"line_color\":\"#1f77b4\",\"line_width\":2,\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1245\",\"type\":\"Line\"},{\"attributes\":{\"data\":{\"x\":{\"__ndarray__\":\"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\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[559]},\"y\":{\"__ndarray__\":\"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\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[559]}},\"selected\":{\"id\":\"1281\"},\"selection_policy\":{\"id\":\"1282\"}},\"id\":\"1244\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1230\",\"type\":\"PanTool\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1230\"},{\"id\":\"1231\"},{\"id\":\"1232\"},{\"id\":\"1233\"},{\"id\":\"1234\"},{\"id\":\"1235\"},{\"id\":\"1254\"}]},\"id\":\"1237\",\"type\":\"Toolbar\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#1f77b4\",\"line_width\":2,\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1246\",\"type\":\"Line\"},{\"attributes\":{\"fill_color\":{\"value\":\"white\"},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1250\",\"type\":\"Circle\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"white\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1251\",\"type\":\"Circle\"},{\"attributes\":{\"data\":{\"x\":{\"__ndarray__\":\"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\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[559]},\"y\":{\"__ndarray__\":\"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\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[559]}},\"selected\":{\"id\":\"1283\"},\"selection_policy\":{\"id\":\"1284\"}},\"id\":\"1249\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1227\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1283\",\"type\":\"Selection\"},{\"attributes\":{\"data_source\":{\"id\":\"1249\"},\"glyph\":{\"id\":\"1250\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1251\"},\"selection_glyph\":null,\"view\":{\"id\":\"1253\"}},\"id\":\"1252\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1236\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"axis_label\":\"p(Count(w) < c)\",\"formatter\":{\"id\":\"1276\"},\"ticker\":{\"id\":\"1227\"}},\"id\":\"1226\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1234\",\"type\":\"ResetTool\"},{\"attributes\":{\"axis\":{\"id\":\"1222\"},\"ticker\":null},\"id\":\"1225\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1233\",\"type\":\"SaveTool\"},{\"attributes\":{\"source\":{\"id\":\"1249\"}},\"id\":\"1253\",\"type\":\"CDSView\"}],\"root_ids\":[\"1211\"]},\"title\":\"Bokeh Application\",\"version\":\"2.1.1\"}};\n",
       "  var render_items = [{\"docid\":\"e52bacaf-fa3a-4eee-a6b1-490f5d5fab60\",\"root_ids\":[\"1211\"],\"roots\":{\"1211\":\"67c64e5f-4498-4bbf-8c78-266f2546be20\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1211"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We'll use the histogram function with variable bins in order to get a stair-step plot.\n",
    "b_shift = 0.5  # So counts don't fall on bin boundaries.\n",
    "# Weights give us distribution by token; remove this to get distribution by type.\n",
    "h, bins = np.histogram(counts, weights=counts, bins=b_shift+np.concatenate([[0], np.unique(counts)]))\n",
    "\n",
    "fig = bp.figure(plot_width=800, plot_height=400, x_axis_type=\"log\", title=\"Cumulative Word Counts\")\n",
    "l = fig.line(x=bins[1:]-b_shift, y=np.cumsum(1.0*h)/np.sum(h), line_width=2)\n",
    "fig.circle(x=bins[1:]-b_shift, y=np.cumsum(1.0*h)/np.sum(h), fill_color=\"white\", size=4)\n",
    "fig.add_tools(HoverTool(tooltips=[(\"count\", \"@x\"), (\"CDF\", \"@y\")], renderers=[l], mode=\"vline\"))\n",
    "fig.y_range.start = 0\n",
    "fig.yaxis.axis_label = \"p(Count(w) < c)\"\n",
    "fig.xaxis.axis_label = \"Count (log-scale)\"\n",
    "bp.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** the cell above plots the distribution based on *tokens*, or words on the page. Modify the histogram code to make a similar plot, but based on *types*, or distinct words in the vocabulary. If we pick a random word in the dictionary, how many times are we likely to see it in a 1M word corpus?\n",
    "\n",
    "**Exercise (more involved):** compute the same function for the bigram and trigram distributions, and overlay it on a version of the plot above. (This should be similar to a graph from the async.) What does this tell you about the sparsity problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling\n",
    "\n",
    "With a basic idea of what our corpus looks like, we can now embark on the task of modeling it. Recall from the async that language modeling is the task of predicting the next word, given the preceding history:\n",
    "\n",
    "$$ P(w_i | w_{i-1}, ..., w_0)$$\n",
    "\n",
    "## Perplexity\n",
    "\n",
    "How hard is this task? Let's use the unigram distribution as a starting point for our model. We saw that the distribution is very uneven - some words (types) are very common, while others are very rare. If the corpus (or a sample thereof) is our test set, we're going to see our training labels according to the distribution $y \\sim p(w)$. A smarter model might take some features such as immediately previous words and do a better job estimating the next word (let's call such features $x$ in general). We can summarize how uncertain our model is by looking at the entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram entropy: 10.066 bits\n"
     ]
    }
   ],
   "source": [
    "print(\"Unigram entropy: {:.03f} bits\".format(stats.entropy(p, base=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bits are still a little unintuitive, but we can do better. Recall from Assignment 1 that the entropy of a _uniform_ distribution over $n$ elements is $ \\log_2 n $. So given some distribution with entropy $H(P) = k$ bits we can say that distribution $P$ is _as uncertain as_ a uniform distribution over $2^k$ elements.\n",
    "\n",
    "If we apply this to our unigram distribution, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct unigrams: 48171\n",
      "As uncertain as:   1071.78\n"
     ]
    }
   ],
   "source": [
    "print(\"Distinct unigrams: {:d}\".format(len(p)))\n",
    "print(\"As uncertain as:   {:.02f}\".format(2**stats.entropy(p, base=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in the (real) machine learning setting, we can't really measure the entropy of the underlying distribution $P(y\\ |\\ x) = P(w_i | w_{i-1}, ..., w_0)$. But, we can approximate the cross-entropy between the true distribution (for which we have samples $y_i \\sim P(y\\ |\\ x_i)$) and our _predicted_ distribution $\\hat{P}(y\\ |\\ x_i)$:\n",
    "\n",
    "$$ \\text{CE}(P, \\hat{P}) = - \\sum_{y,x} P(y\\ |\\ x) \\log_2 \\hat{P}(y\\ |\\ x) \\approx - \\frac{1}{N} \\sum_{i}^N \\log_2 \\hat{P}(y_i\\ |\\ x_i)$$\n",
    "\n",
    "This has the same units (bits), and we can exponentiate it in the same way to give us a measure of \"how confused\" the model is. This quantity is the **perplexity** of the model. A perplexity of $k$ tells us that the model is as uncertain as if it had to choose from $k$ elements with equal probability.\n",
    "\n",
    "So suppose we used the unigram model as our language model. We'll cheat for now and just use $\\hat{P}_{\\text{unigram}}(w) = \\tilde{p}(w) $ (where $\\tilde{p}$ denotes an estimate from a finite sample). Then our (training set) perplexity is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram language model\n",
      "Expected full-corpus perplexity: 1071.78\n"
     ]
    }
   ],
   "source": [
    "# scipy.stats.entropy with two arguments\n",
    "def cross_entropy(p, q):\n",
    "    return -1*np.sum(p * np.log2(q))\n",
    "\n",
    "print(\"Unigram language model\")\n",
    "print(\"Expected full-corpus perplexity: {:.02f}\".format(2**cross_entropy(p, q=p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to sanity-check it, we can do our usual machine learning loss calculation: look at all the examples (words), and take the average loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1161192/1161192 [00:04<00:00, 280412.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity (full-corpus): 1071.78\n"
     ]
    }
   ],
   "source": [
    "word_to_prob = {w:p[i] for i,w in enumerate(words)}\n",
    "probs = [word_to_prob[utils.canonicalize_word(w)] \n",
    "         for w in ProgressBar(corpus.words())]\n",
    "perplexity = 2**(-1*np.sum(np.log2(probs))/len(probs))\n",
    "print(\"Perplexity (full-corpus): {:.02f}\".format(perplexity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the two cells below, we test out our simple model by taking a piece of text with all the spaces removed, enumerating all possible ways of splitting it (efficiently using dynamic programming - more on that later in the course), scoring each, and then returning the highest scoring sequence.\n",
    "\n",
    "You can see we can apply even this **really** simple model to a neat application[1]!\n",
    "\n",
    "[1] Peter Norvig, a director of research at Google implemented this model here:  http://norvig.com/ngrams/ch14.pdf and extends it pretty far.  By the end, he has managed to decrypt WWI encryption using only this simple language model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'there', 'how', 'are', 'you']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment.segment('hellotherehowareyou', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dance', 'a', 'jig']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment.segment('danceajig', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'cat', 'in', 'the', 'hat']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment.segment('thecatinthehat', vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram Language Models\n",
    "\n",
    "The unigram model isn't a very good one - it doesn't model any previous context at all. On the other hand, we can't model _all_ of the preceding words, because that history could get prohibitively long and extremely sparse. As a compromise, we'll make a _Markov assumption_ and limit ourselves to a finite history of $n$ words.\n",
    "\n",
    "For now, we'll build a trigram model, which considers the two preceding words:\n",
    "\n",
    "$$ P(w_i\\ |\\ w_{i-1}, ..., w_0) \\approx P(w_i\\ |\\ w_{i-1}, w_{i-2}) $$\n",
    "\n",
    "**Exercise:** how many possible trigrams are there?\n",
    "\n",
    "We'll need to store a table of the probabilities, indexed by triples $(w_i, w_{i-1}, w_{i-2})$. If we store every possible combination, it can get quite large. Assuming 8 bytes for each entry, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:         48,174 words\n",
      "Unigrams need:        376.36 kB\n",
      "Bigrams  need:      17705.80 MB\n",
      "Trigrams need:  852959083.65 MB\n",
      "Available:           3055.28 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "print(\"Vocab size:     {:10,} words\".format(vocab.size))\n",
    "print(\"Unigrams need:  {:12.2f} kB\".format(8 * (vocab.size ** 1) / (2**10)))\n",
    "print(\"Bigrams  need:  {:12.2f} MB\".format(8 * (vocab.size ** 2) / (2**20)))\n",
    "print(\"Trigrams need:  {:12.2f} MB\".format(8 * (vocab.size ** 3) / (2**20)))\n",
    "print(\"Available:      {:12.2f} MB\".format(psutil.virtual_memory().available / (2**20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that's no good. We can store all possible unigrams, but after that we're toast.\n",
    "\n",
    "Thankfully, we don't have to store everything. Recall that most words only appear a handful of times, and that there are plenty of words in the English language that we don't see in our corpus at all. For bigrams and trigrams, the table will be quite sparse. We need only store the entries that we actually observe; the rest we can take to be zero, or estimate their values on the fly through smoothing or backoff.\n",
    "\n",
    "**Observation:** When building language models, be sure to only keep non-zero counts in your datastructures and assume anything missing is 0.\n",
    "\n",
    "**Exercise:** for a corpus of 1 million words and a vocabulary of 10000, what is the maximum number of bigrams that can be actually _observed_? How about trigrams?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing our Model\n",
    "\n",
    "We'll represent our model with a nested map `context => word => probability`, where word is $w_i$ and for our trigram model, the context is the two preceding words $(w_{i-2}, w_{i-1})$.\n",
    "\n",
    "First, we'll go through the corpus and compute raw trigram counts $c(abc)$, which we'll then normalize into probabilities:\n",
    "\n",
    "$$  P_{abc} = P(w_i = c\\ |\\ w_{i-1} = b, w_{i-2} = a) = \\frac{\\mathrm{c(abc)}}{\\sum_{c'}\\mathrm{c(abc')}} = \\frac{C_{abc}}{\\sum_{c'} C_{abc'}} $$\n",
    "\n",
    "Here's the code for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def normalize_counter(c):\n",
    "    \"\"\"Given a dictionary of <item, counts>, return <item, fraction>.\"\"\"\n",
    "    total = sum(c.values())\n",
    "    return {w:float(c[w])/total for w in c}\n",
    "\n",
    "class SimpleTrigramLM(object):\n",
    "    def __init__(self, words):\n",
    "        \"\"\"Build our simple trigram model.\"\"\"\n",
    "        # Raw trigram counts over the corpus. \n",
    "        # c(w | w_1 w_2) = self.counts[(w_2,w_1)][w]\n",
    "        self.counts = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "    \n",
    "        # Iterate through the word stream once.\n",
    "        w_1, w_2 = None, None\n",
    "        for word in words:\n",
    "            if w_1 is not None and w_2 is not None:\n",
    "                # Increment trigram count.\n",
    "                self.counts[(w_2,w_1)][word] += 1\n",
    "            # Shift context along the stream of words.\n",
    "            w_2 = w_1\n",
    "            w_1 = word\n",
    "            \n",
    "        # Normalize so that for each context we have a valid probability\n",
    "        # distribution (i.e. adds up to 1.0) of possible next tokens.\n",
    "        self.probas = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "        for context, ctr in self.counts.items():\n",
    "            self.probas[context] = normalize_counter(ctr)\n",
    "            \n",
    "    def next_word_proba(self, word, seq):\n",
    "        \"\"\"Compute p(word | seq)\"\"\"\n",
    "        context = tuple(seq[-2:])  # last two words\n",
    "        return self.probas[context].get(word, 0.0)\n",
    "    \n",
    "    def predict_next(self, seq):\n",
    "        \"\"\"Sample a word from the conditional distribution.\"\"\"\n",
    "        context = tuple(seq[-2:])  # last two words\n",
    "        pc = self.probas[context]  # conditional distribution\n",
    "        words, probs = zip(*pc.items())  # convert to list\n",
    "        return np.random.choice(words, p=probs)\n",
    "    \n",
    "    def score_seq(self, seq, verbose=False):\n",
    "        \"\"\"Compute log probability (base 2) of the given sequence.\"\"\"\n",
    "        score = 0.0\n",
    "        count = 0\n",
    "        # Start at third word, since we need a full context.\n",
    "        for i in range(2, len(seq)):\n",
    "            if (seq[i] == \"<s>\" or seq[i] == \"</s>\"):\n",
    "                continue  # Don't count special tokens in score.\n",
    "            s = np.log2(self.next_word_proba(seq[i], seq[i-2:i]))\n",
    "            score += s\n",
    "            count += 1\n",
    "            # DEBUG\n",
    "            if verbose:\n",
    "                print(\"log P({:s} | {:s}) = {.03f}\".format(seq[i], \" \".join(seq[i-2:i]), s))\n",
    "        return score, count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Let's train our model. We'll do a proper train-test split this time, so we can evaluate on unseen data. This means that we also have to fix our vocabulary based on the *training* set - which means that some unseen words in the test set will get replaced by `<unk>`. The `Vocabulary` helper class will take care of this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 57,340 sentences (1.16119e+06 tokens)\n",
      "Training set: 45,872 sentences (929,539 tokens)\n",
      "Test set: 11,468 sentences (231,653 tokens)\n"
     ]
    }
   ],
   "source": [
    "train_sents, test_sents = utils.get_train_test_sents(corpus, split=0.8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 929539/929539 [00:02<00:00, 337300.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set vocabulary: 43707 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = vocabulary.Vocabulary(utils.canonicalize_word(w) for w in ProgressBar(utils.flatten(train_sents)))\n",
    "print(\"Train set vocabulary: %d words\" % vocab.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note on Preprocessing\n",
    "\n",
    "We didn't do this for the unigram LM, but when modeling sentences it's helpful to add special beginning-of-sentence (`<s>`) and end-of-sentence (`</s>`) tokens. \n",
    "\n",
    "This lets the model estimate the probability of a word appearing at the beginning of a sentence, and lets it model the end of a sentence properly, since periods or other punctuation aren't always an accurate guide (e.g. `\"Dr.\"` or `\"Yahoo!\"`).\n",
    "\n",
    "Our padded sentences will look like this:\n",
    "```\n",
    "<s> <s> the cat sat in the hat . </s>\n",
    "```\n",
    "**Exercise:** why do we add two `<s>` tokens at the beginning? (*Hint: this is specific to our trigram model.*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1067155/1067155 [00:00<00:00, 1255987.09it/s]\n",
      "100%|██████████| 266057/266057 [00:00<00:00, 1114881.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building trigram LM...\n",
      "done in 3.05 s\n"
     ]
    }
   ],
   "source": [
    "def sents_to_tokens(sents):\n",
    "    \"\"\"Returns an flattened list of the words in the sentences, with padding for a trigram model.\"\"\"\n",
    "    padded_sentences = ([\"<s>\", \"<s>\"] + s + [\"</s>\"] for s in sents)\n",
    "    # This will canonicalize words, and replace anything not in vocab with <unk>\n",
    "    return np.array([utils.canonicalize_word(w, wordset=vocab.wordset) \n",
    "                     for w in ProgressBar(utils.flatten(padded_sentences))], dtype=object)\n",
    "\n",
    "train_tokens = sents_to_tokens(train_sents)\n",
    "test_tokens = sents_to_tokens(test_sents)\n",
    "\n",
    "t0 = time.time()\n",
    "print(\"Building trigram LM...\",)\n",
    "lm = SimpleTrigramLM(train_tokens)\n",
    "print(\"done in %.02f s\" % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Sentences\n",
    "\n",
    "Before we run scoring, let's look at some generated sentences from our model. We'll generate them sequentially, one token at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> the average reader of outdoor articles ; ; </s>\n",
      "[8 tokens; log P(seq): -22.17]\n",
      "\n",
      "<s> <s> this observation about the food consumed . </s>\n",
      "[7 tokens; log P(seq): -28.61]\n",
      "\n",
      "<s> <s> when she went on : </s>\n",
      "[5 tokens; log P(seq): -24.49]\n",
      "\n",
      "<s> <s> we have in mind , he discovered that munich's early mornings even in germany in russia , not sinking .\n",
      "[20 tokens; log P(seq): -61.26]\n",
      "\n",
      "<s> <s> despite their own amateur graces . </s>\n",
      "[6 tokens; log P(seq): -23.50]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_length = 20\n",
    "num_sentences = 5\n",
    "\n",
    "for _ in range(num_sentences):\n",
    "    seq = [\"<s>\", \"<s>\"]\n",
    "    for i in range(max_length):\n",
    "        seq.append(lm.predict_next(seq))\n",
    "        # Stop at end-of-sentence\n",
    "        if seq[-1] == \"</s>\": break\n",
    "    print(\" \".join(seq))\n",
    "    print(\"[{1:d} tokens; log P(seq): {0:.02f}]\".format(*lm.score_seq(seq)))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring our model\n",
    "\n",
    "We'll score our model by running the `score_seq` function, which computes \n",
    "\n",
    "$$ \\text{CE}_{\\text{total}}(y, \\hat{y}) = \\sum_{i=2}^N \\log_2 \\hat{p}(w_i\\ |\\ w_{i-1}, w_{i-2}) $$\n",
    "\n",
    "This is the cross-entropy loss, which is equal to $-1$ times the log-likelihood of the data under our model. As usual, we'll exponentiate to get the perplexity score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train perplexity: 7.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-5e1b63fcb65f>:51: RuntimeWarning: divide by zero encountered in log2\n",
      "  s = np.log2(self.next_word_proba(seq[i], seq[i-2:i]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity: inf\n"
     ]
    }
   ],
   "source": [
    "log_p_data, num_real_tokens = lm.score_seq(train_tokens)\n",
    "print(\"Train perplexity: {:.02f}\".format(2**(-1*log_p_data/num_real_tokens)))\n",
    "\n",
    "log_p_data, num_real_tokens = lm.score_seq(test_tokens)\n",
    "print(\"Test perplexity: {:.02f}\".format(2**(-1*log_p_data/num_real_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's going on here? Our model gets an absurdly low perplexity on the training data, but a perplexity of _infinity_ on the test data.\n",
    "\n",
    "**Answer:** the n-gram model badly overfits without any smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing and Handling the Unknown\n",
    "\n",
    "Our simple model doesn't have any real mechanism for handling unknown words - if we feed something unseen to `score_seq`, it will assign it a probability of zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-5e1b63fcb65f>:51: RuntimeWarning: divide by zero encountered in log2\n",
      "  s = np.log2(self.next_word_proba(seq[i], seq[i-2:i]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.score_seq([\"<s>\", \"i\", \"love\", \"w266\", \"</s>\"])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the cause of the infinite perplexity, above: $\\log_2 0 = -\\infty $. \n",
    "\n",
    "**Exercise:** besides unknown words, when else would the un-smoothed trigram model predict $p(w\\ |\\ w_{i-1}, w_{i-2}) = 0$ ?\n",
    "\n",
    "Is assuming zero probabilities realistic? Let's look back at our unigram distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% words seen only once: 1.80%\n"
     ]
    }
   ],
   "source": [
    "print(\"% words seen only once: {:.02%}\".format(sum(counts * (counts == 1)) / sum(counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 1 in 50 words were seen only once in our corpus, so we might expect a comparable fraction of words in a new sample to also be previously-unseen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% <unk> in test set: 1.83%\n"
     ]
    }
   ],
   "source": [
    "print(\"% <unk> in test set: {:.02%}\".format(np.sum(np.array(test_tokens) == \"<unk>\") / len(test_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to use our language model in the wild, we'll need to implement some kind of smoothing to hedge our bets whenever these come up. This will be a major focus of Assignment 3, in which you'll build on the `SimpleTrigramLM` by implementing Laplace (add-k) and Kneser-Ney smoothing."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
